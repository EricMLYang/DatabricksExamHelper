#!/usr/bin/env python3
"""
Mock Exam Grader - æ¨¡æ“¬å·æ‰¹æ”¹å·¥å…·

æ­¤è…³æœ¬è®€å–æ¨¡æ“¬å·ç­”æ¡ˆæª”ï¼ˆYAML æ ¼å¼ï¼‰ï¼Œç”¢å‡ºéŒ¯é¡Œå ±å‘Šèˆ‡çµ±è¨ˆåˆ†æã€‚

åŠŸèƒ½ï¼š
- æ‰¹æ”¹æ¨¡æ“¬å·ï¼Œè¨ˆç®—å¾—åˆ†ç‡
- ç”¢å‡ºéŒ¯é¡Œæ¸…å–®èˆ‡å¼±é»åˆ†æ
- æ”¯æ´ Markdown æ ¼å¼è¼¸å‡º

ä½¿ç”¨ç¯„ä¾‹ï¼š
    python mock_exam_grader.py --input my_answers.yaml --user eric --output result.md
    python mock_exam_grader.py --input exam.yaml --user alice --output ./reports/alice_exam1.md

YAML è¼¸å…¥æ ¼å¼ï¼š
    exam:
      name: "Mock Exam 01"
      total_questions: 40
      passing_score: 70

    answers:
      - question_id: Q-MOCK01-001
        user_answer: A
        correct_answer: B
        topics: ["Delta-Lake", "VACUUM"]
        difficulty: L2-Intermediate

      - question_id: Q-MOCK01-002
        user_answer: C
        correct_answer: C
        topics: ["Streaming", "Triggers"]
        difficulty: L1-Basic
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
from collections import Counter

try:
    import yaml
except ImportError:
    print("Error: PyYAML is not installed. Please run: pip install pyyaml")
    sys.exit(1)


@dataclass
class Question:
    """é¡Œç›®è³‡æ–™çµæ§‹"""
    question_id: str
    user_answer: str
    correct_answer: str
    topics: List[str]
    difficulty: str

    def is_correct(self) -> bool:
        """åˆ¤æ–·ç­”æ¡ˆæ˜¯å¦æ­£ç¢º"""
        return self.user_answer.upper() == self.correct_answer.upper()


@dataclass
class ExamResult:
    """è€ƒè©¦çµæœè³‡æ–™çµæ§‹"""
    exam_name: str
    user_name: str
    total_questions: int
    correct_count: int
    wrong_count: int
    passing_score: float
    user_score: float
    passed: bool
    wrong_questions: List[Question]
    weak_topics: List[tuple[str, int]]  # (topic, error_count)
    difficulty_stats: Dict[str, Dict[str, int]]  # difficulty: {correct, wrong}
    timestamp: str


class MockExamGrader:
    """æ¨¡æ“¬å·æ‰¹æ”¹å™¨"""

    def __init__(self, input_file: Path, user_name: str):
        """
        åˆå§‹åŒ–æ‰¹æ”¹å™¨

        Args:
            input_file: YAML ç­”æ¡ˆæª”è·¯å¾‘
            user_name: ä½¿ç”¨è€…åç¨±
        """
        self.input_file = input_file
        self.user_name = user_name
        self.questions: List[Question] = []
        self.exam_info: Dict[str, Any] = {}

    def load_answers(self) -> None:
        """
        è¼‰å…¥ YAML ç­”æ¡ˆæª”

        Raises:
            FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨
            yaml.YAMLError: YAML æ ¼å¼éŒ¯èª¤
            ValueError: è³‡æ–™æ ¼å¼ä¸ç¬¦åˆè¦ç¯„
        """
        if not self.input_file.exists():
            raise FileNotFoundError(f"ç­”æ¡ˆæª”ä¸å­˜åœ¨: {self.input_file}")

        try:
            with open(self.input_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise yaml.YAMLError(f"YAML æ ¼å¼éŒ¯èª¤: {e}")

        # é©—è­‰è³‡æ–™çµæ§‹
        if not isinstance(data, dict):
            raise ValueError("YAML æ ¹ç¯€é»å¿…é ˆæ˜¯ dict")

        if 'exam' not in data:
            raise ValueError("ç¼ºå°‘å¿…è¦æ¬„ä½: exam")

        if 'answers' not in data:
            raise ValueError("ç¼ºå°‘å¿…è¦æ¬„ä½: answers")

        self.exam_info = data['exam']

        # é©—è­‰ exam æ¬„ä½
        required_exam_fields = ['name', 'total_questions', 'passing_score']
        for field in required_exam_fields:
            if field not in self.exam_info:
                raise ValueError(f"exam ç¼ºå°‘å¿…è¦æ¬„ä½: {field}")

        # è§£æç­”æ¡ˆ
        for idx, answer in enumerate(data['answers']):
            try:
                question = Question(
                    question_id=answer['question_id'],
                    user_answer=answer['user_answer'],
                    correct_answer=answer['correct_answer'],
                    topics=answer.get('topics', []),
                    difficulty=answer.get('difficulty', 'L2-Intermediate')
                )
                self.questions.append(question)
            except KeyError as e:
                raise ValueError(f"ç­”æ¡ˆ #{idx + 1} ç¼ºå°‘å¿…è¦æ¬„ä½: {e}")

    def grade(self) -> ExamResult:
        """
        æ‰¹æ”¹è€ƒå·

        Returns:
            ExamResult: è€ƒè©¦çµæœ
        """
        correct_count = sum(1 for q in self.questions if q.is_correct())
        wrong_count = len(self.questions) - correct_count

        # è¨ˆç®—å¾—åˆ†
        if len(self.questions) > 0:
            user_score = (correct_count / len(self.questions)) * 100
        else:
            user_score = 0

        passing_score = self.exam_info.get('passing_score', 70)
        passed = user_score >= passing_score

        # çµ±è¨ˆéŒ¯é¡Œ
        wrong_questions = [q for q in self.questions if not q.is_correct()]

        # çµ±è¨ˆå¼±é»ä¸»é¡Œ
        topic_errors = Counter()
        for q in wrong_questions:
            for topic in q.topics:
                topic_errors[topic] += 1

        weak_topics = topic_errors.most_common()

        # çµ±è¨ˆé›£åº¦åˆ†ä½ˆ
        difficulty_stats: Dict[str, Dict[str, int]] = {}
        for q in self.questions:
            if q.difficulty not in difficulty_stats:
                difficulty_stats[q.difficulty] = {'correct': 0, 'wrong': 0}

            if q.is_correct():
                difficulty_stats[q.difficulty]['correct'] += 1
            else:
                difficulty_stats[q.difficulty]['wrong'] += 1

        return ExamResult(
            exam_name=self.exam_info['name'],
            user_name=self.user_name,
            total_questions=len(self.questions),
            correct_count=correct_count,
            wrong_count=wrong_count,
            passing_score=passing_score,
            user_score=user_score,
            passed=passed,
            wrong_questions=wrong_questions,
            weak_topics=weak_topics,
            difficulty_stats=difficulty_stats,
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        )


class MarkdownReportGenerator:
    """Markdown å ±å‘Šç”Ÿæˆå™¨"""

    @staticmethod
    def generate(result: ExamResult) -> str:
        """
        ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š

        Args:
            result: è€ƒè©¦çµæœ

        Returns:
            str: Markdown æ ¼å¼å ±å‘Šå…§å®¹
        """
        lines = []

        # æ¨™é¡Œ
        lines.append(f"# æ¨¡æ“¬å·æ‰¹æ”¹å ±å‘Š\n")
        lines.append(f"**è€ƒè©¦åç¨±:** {result.exam_name}")
        lines.append(f"**è€ƒç”Ÿ:** {result.user_name}")
        lines.append(f"**æ‰¹æ”¹æ™‚é–“:** {result.timestamp}\n")
        lines.append("---\n")

        # ç¸½é«”çµ±è¨ˆ
        lines.append("## ğŸ“Š ç¸½é«”çµ±è¨ˆ\n")
        lines.append(f"| é …ç›® | æ•¸å€¼ |")
        lines.append(f"|------|------|")
        lines.append(f"| ç¸½é¡Œæ•¸ | {result.total_questions} |")
        lines.append(f"| ç­”å° | {result.correct_count} âœ… |")
        lines.append(f"| ç­”éŒ¯ | {result.wrong_count} âŒ |")
        lines.append(f"| æ­£ç¢ºç‡ | {result.user_score:.1f}% |")
        lines.append(f"| åŠæ ¼æ¨™æº– | {result.passing_score}% |")

        # çµæœåˆ¤å®š
        if result.passed:
            lines.append(f"| **çµæœ** | **ğŸ‰ é€šé** |")
        else:
            needed = result.passing_score - result.user_score
            lines.append(f"| **çµæœ** | **âŒ æœªé€šé** (è·é›¢åŠæ ¼é‚„å·® {needed:.1f}%) |")

        lines.append("")
        lines.append("---\n")

        # é›£åº¦åˆ†ä½ˆçµ±è¨ˆ
        lines.append("## ğŸ“ˆ å„é›£åº¦è¡¨ç¾\n")
        lines.append("| é›£åº¦ | ç­”å° | ç­”éŒ¯ | æ­£ç¢ºç‡ |")
        lines.append("|------|------|------|--------|")

        for difficulty in ['L1-Basic', 'L2-Intermediate', 'L3-Advanced']:
            if difficulty in result.difficulty_stats:
                stats = result.difficulty_stats[difficulty]
                correct = stats['correct']
                wrong = stats['wrong']
                total = correct + wrong
                accuracy = (correct / total * 100) if total > 0 else 0
                lines.append(f"| {difficulty} | {correct} | {wrong} | {accuracy:.1f}% |")

        lines.append("")
        lines.append("---\n")

        # å¼±é»ä¸»é¡Œåˆ†æ
        if result.weak_topics:
            lines.append("## âš ï¸ å¼±é»ä¸»é¡Œåˆ†æ\n")
            lines.append("ä»¥ä¸‹æ˜¯æ‚¨ç­”éŒ¯æœ€å¤šçš„ä¸»é¡Œï¼Œå»ºè­°å„ªå…ˆè¤‡ç¿’ï¼š\n")
            lines.append("| æ’å | ä¸»é¡Œ | éŒ¯èª¤æ¬¡æ•¸ |")
            lines.append("|------|------|----------|")

            for rank, (topic, count) in enumerate(result.weak_topics[:10], 1):
                lines.append(f"| {rank} | `{topic}` | {count} |")

            lines.append("")
            lines.append("---\n")

        # éŒ¯é¡Œæ¸…å–®
        if result.wrong_questions:
            lines.append("## âŒ éŒ¯é¡Œæ¸…å–®\n")
            lines.append(f"å…± {len(result.wrong_questions)} é¡Œéœ€è¦è¤‡ç¿’ï¼š\n")

            for idx, q in enumerate(result.wrong_questions, 1):
                lines.append(f"### {idx}. {q.question_id}\n")
                lines.append(f"- **æ‚¨çš„ç­”æ¡ˆ:** {q.user_answer}")
                lines.append(f"- **æ­£ç¢ºç­”æ¡ˆ:** {q.correct_answer}")
                lines.append(f"- **è€ƒé»:** {', '.join([f'`{t}`' for t in q.topics])}")
                lines.append(f"- **é›£åº¦:** {q.difficulty}")
                lines.append("")
        else:
            lines.append("## ğŸ‰ æ­å–œï¼\n")
            lines.append("å…¨éƒ¨ç­”å°ï¼Œæ²’æœ‰éŒ¯é¡Œï¼\n")

        lines.append("---\n")

        # å»ºè­°
        lines.append("## ğŸ’¡ è¤‡ç¿’å»ºè­°\n")

        if result.user_score >= 90:
            lines.append("ğŸŒŸ **å„ªç§€ï¼** æ‚¨å°é€™äº›è€ƒé»æŒæ¡å¾—éå¸¸å¥½ã€‚å»ºè­°ï¼š")
            lines.append("- è¤‡ç¿’éŒ¯é¡Œï¼ˆè‹¥æœ‰ï¼‰")
            lines.append("- å˜—è©¦æ›´é«˜é›£åº¦çš„è®Šå½¢é¡Œ")
            lines.append("- å¹«åŠ©å…¶ä»–åŒå­¸ç†è§£é€™äº›è€ƒé»")
        elif result.user_score >= result.passing_score:
            lines.append("âœ… **é€šéï¼** æ‚¨å·²é”åˆ°åŠæ ¼æ¨™æº–ã€‚å»ºè­°ï¼š")
            lines.append("- é‡é»è¤‡ç¿’å¼±é»ä¸»é¡Œ")
            lines.append("- å®ŒæˆéŒ¯é¡Œçš„è®Šå½¢é¡Œè¨“ç·´")
            lines.append("- å°‡éŒ¯é¡Œè§£æåŠ å…¥å€‹äººç­†è¨˜")
        else:
            lines.append("ğŸ“š **éœ€åŠ å¼·ï¼** å»ºè­°ï¼š")
            lines.append(f"1. å„ªå…ˆè¤‡ç¿’å¼±é»ä¸»é¡Œï¼ˆè¦‹ä¸Šæ–¹ã€Œå¼±é»ä¸»é¡Œåˆ†æã€ï¼‰")
            lines.append(f"2. é€é¡Œåˆ†æéŒ¯é¡Œï¼Œä½¿ç”¨ `explain-why-not` æŠ€èƒ½æ·±åº¦æ‹†è§£")
            lines.append(f"3. å®ŒæˆéŒ¯é¡Œçš„è®Šå½¢é¡Œè¨“ç·´ï¼Œç¢ºä¿çœŸæ­£ç†è§£")
            lines.append(f"4. 7 å¤©å¾Œé‡æ–°åšä¸€æ¬¡é€™ä»½æ¨¡æ“¬å·ï¼Œæª¢è¦–é€²æ­¥å¹…åº¦")

        lines.append("")
        lines.append("---\n")
        lines.append(f"*å ±å‘Šç”Ÿæˆæ–¼ {result.timestamp}*\n")

        return "\n".join(lines)


def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(
        description="æ¨¡æ“¬å·æ‰¹æ”¹å·¥å…· - è®€å– YAML ç­”æ¡ˆæª”ï¼Œç”¢å‡ºéŒ¯é¡Œå ±å‘Š",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # æ‰¹æ”¹æ¨¡æ“¬å·ä¸¦è¼¸å‡ºåˆ°æª”æ¡ˆ
  python mock_exam_grader.py --input my_answers.yaml --user eric --output result.md

  # æ‰¹æ”¹æ¨¡æ“¬å·ä¸¦é¡¯ç¤ºåœ¨è¢å¹•ä¸Š
  python mock_exam_grader.py --input exam.yaml --user alice

  # æŒ‡å®šè¼¸å‡ºç›®éŒ„
  python mock_exam_grader.py --input exam.yaml --user bob --output ./reports/bob_exam1.md
        """
    )

    parser.add_argument(
        '--input',
        type=str,
        required=True,
        help='YAML ç­”æ¡ˆæª”è·¯å¾‘ (å¿…å¡«)'
    )

    parser.add_argument(
        '--user',
        type=str,
        required=True,
        help='ä½¿ç”¨è€…åç¨± (å¿…å¡«)'
    )

    parser.add_argument(
        '--output',
        type=str,
        default=None,
        help='è¼¸å‡ºå ±å‘Šæª”æ¡ˆè·¯å¾‘ (é¸å¡«ï¼Œé è¨­è¼¸å‡ºåˆ°è¢å¹•)'
    )

    args = parser.parse_args()

    try:
        # åˆå§‹åŒ–æ‰¹æ”¹å™¨
        input_path = Path(args.input)
        grader = MockExamGrader(input_path, args.user)

        # è¼‰å…¥ç­”æ¡ˆ
        print(f"ğŸ“‚ è¼‰å…¥ç­”æ¡ˆæª”: {input_path}")
        grader.load_answers()
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(grader.questions)} é¡Œ")

        # æ‰¹æ”¹
        print(f"â³ æ‰¹æ”¹ä¸­...")
        result = grader.grade()
        print(f"âœ… æ‰¹æ”¹å®Œæˆï¼")
        print(f"ğŸ“Š å¾—åˆ†: {result.user_score:.1f}% ({result.correct_count}/{result.total_questions})")

        # ç”Ÿæˆå ±å‘Š
        report = MarkdownReportGenerator.generate(result)

        # è¼¸å‡ºå ±å‘Š
        if args.output:
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)

            print(f"ğŸ’¾ å ±å‘Šå·²å„²å­˜è‡³: {output_path}")
        else:
            print("\n" + "=" * 60)
            print(report)
            print("=" * 60)

    except FileNotFoundError as e:
        print(f"âŒ éŒ¯èª¤: {e}", file=sys.stderr)
        sys.exit(1)

    except yaml.YAMLError as e:
        print(f"âŒ YAML æ ¼å¼éŒ¯èª¤: {e}", file=sys.stderr)
        print("\nè«‹ç¢ºèª YAML æª”æ¡ˆæ ¼å¼æ­£ç¢ºã€‚åƒè€ƒæ ¼å¼ï¼š", file=sys.stderr)
        print("""
exam:
  name: "Mock Exam 01"
  total_questions: 40
  passing_score: 70

answers:
  - question_id: Q-MOCK01-001
    user_answer: A
    correct_answer: B
    topics: ["Delta-Lake", "VACUUM"]
    difficulty: L2-Intermediate
        """, file=sys.stderr)
        sys.exit(1)

    except ValueError as e:
        print(f"âŒ è³‡æ–™æ ¼å¼éŒ¯èª¤: {e}", file=sys.stderr)
        sys.exit(1)

    except Exception as e:
        print(f"âŒ æœªé æœŸçš„éŒ¯èª¤: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
