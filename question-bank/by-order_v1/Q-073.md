# Question #73

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-073`

### ä¾†æº
**ä¾†æº:** Community Contributed (Real Exam-style)

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹
A Structured Streaming job deployed to production has been resulting in higher than expected cloud storage costs. At present, during normal execution, each microbatch of data is processed in less than 3s; at least 12 times per minute, a microbatch is processed that contains 0 records. The streaming write was configured using the default trigger settings. The production job is currently scheduled alongside many other Databricks jobs in a workspace with instance pools provisioned to reduce start-up time for jobs with batch execution. 

Holding all other variables constant and assuming records need to be processed in less than 10 minutes, which adjustment will meet the requirement?

### é¸é …
- **A.** Set the trigger interval to 3 seconds; the default trigger interval is consuming too many records per batch, resulting in spill to disk that can increase volume costs.
- **B.** Increase the number of shuffle partitions to maximize parallelism, since the trigger interval cannot be modified without modifying the checkpoint directory.
- **C.** Set the trigger interval to 10 minutes; each batch calls APIs in the source storage account, so decreasing trigger frequency to maximum allowable threshold should minimize this cost.
- **D.** Set the trigger interval to 500 milliseconds; setting a small but non-zero trigger interval ensures that the source is not queried too frequently.
- **E.** Use the trigger once option and configure a Databricks job to execute the query every 10 minutes; this approach minimizes costs for both compute and storage.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `Spark-Performance`, `Cluster-Management`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Performance-Misconception`, `Execution-Behavior`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** `Streaming`

---

## ç­”æ¡ˆèˆ‡è§£æé€£çµ

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`

### è§£ææª”æ¡ˆ
**è©³ç´°è§£æ:** ä¸‹æ–¹æ•´åˆè§£æ

---

## ç›¸é—œè³‡æº

### å®˜æ–¹æ–‡ä»¶
- [Trigger types in Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers)
- [Cost optimization for streaming workloads](https://docs.databricks.com/optimizations/cost.html)

---

## è§£é¡Œè§£æï¼ˆæ•´åˆï¼‰

### ç‚ºä»€éº¼ C æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æˆæœ¬å•é¡Œæ ¸å¿ƒï¼š**
- é è¨­ trigger ç‚º 500msï¼Œå°è‡´æ¯ç§’ 2 æ¬¡æª¢æŸ¥ â†’ æ¯åˆ†é˜è‡³å°‘ 120 æ¬¡ API å‘¼å«
- é¡Œç›®æŒ‡å‡ºæ¯åˆ†é˜æœ‰ **12 æ¬¡ç©ºæ‰¹æ¬¡ï¼ˆ0 ç­†è¨˜éŒ„ï¼‰**ï¼Œé¡¯ç¤ºéåº¦é »ç¹çš„æŸ¥è©¢
- æ¯æ¬¡ microbatch éƒ½æœƒå‘¼å«é›²ç«¯å„²å­˜çš„ API æª¢æŸ¥æ–°è³‡æ–™ï¼Œå³ä½¿æ²’æœ‰æ–°è³‡æ–™ä¹Ÿæœƒç”¢ç”Ÿæˆæœ¬

**ç‚ºä½•è¨­å®š 10 åˆ†é˜æ˜¯æœ€ä½³è§£ï¼š**
- é¡Œç›®å®¹è¨±å»¶é²ï¼šã€Œrecords need to be processed in less than 10 minutesã€
- å°‡ trigger interval è¨­ç‚º 10 åˆ†é˜ â†’ å¾æ¯åˆ†é˜ 120 æ¬¡é™ç‚ºæ¯ 10 åˆ†é˜ 1 æ¬¡
- **å¤§å¹…æ¸›å°‘ API å‘¼å«æ¬¡æ•¸ â†’ ç›´æ¥é™ä½å„²å­˜å¸³æˆ¶çš„äº¤æ˜“æˆæœ¬**
- ä»ä¿æŒä¸²æµæ¶æ§‹ï¼Œç„¡éœ€é‡æ–°è¨­è¨ˆç‚ºæ‰¹æ¬¡ä½œæ¥­

**å¯¦å‹™æ‡‰ç”¨ï¼š**
```python
df.writeStream \
  .trigger(processingTime='10 minutes') \  # è¨­å®š 10 åˆ†é˜è§¸ç™¼ä¸€æ¬¡
  .format("delta") \
  .option("checkpointLocation", "/path/checkpoint") \
  .start("/output")
```

### éŒ¯èª¤é¸é …æ’é™¤

**é¸é … A - è¨­å®š 3 ç§’è§¸ç™¼ï¼š**
- âŒ èª¤è§£å•é¡Œï¼šæˆæœ¬å•é¡Œä¾†è‡ª**éæ–¼é »ç¹çš„ API å‘¼å«**ï¼Œéè¨˜éŒ„æ•¸éå¤šå°è‡´ spill
- âŒ 3 ç§’ä»æœƒå°è‡´æ¯åˆ†é˜ 20 æ¬¡æª¢æŸ¥ï¼Œç„¡æ³•è§£æ±ºé«˜é »æŸ¥è©¢å•é¡Œ
- âŒ é¡Œç›®æ˜ç¢ºèªªæ˜æ¯å€‹ microbatch åªéœ€ 3 ç§’è™•ç†å®Œæˆï¼Œç„¡ spill å•é¡Œ

**é¸é … B - å¢åŠ  shuffle partitionsï¼š**
- âŒ å®Œå…¨ç„¡é—œï¼šshuffle partitions å½±éŸ¿è³‡æ–™é‡åˆ†é…ï¼Œèˆ‡ API å‘¼å«é »ç‡ç„¡é—œ
- âŒ éŒ¯èª¤å‰æï¼štrigger interval å¯ä¿®æ”¹ä¸”ä¸éœ€è®Šæ›´ checkpoint directory

**é¸é … D - è¨­å®š 500ms è§¸ç™¼ï¼š**
- âŒ é€™å°±æ˜¯é è¨­å€¼ï¼é¡Œç›®èªªã€Œdefault trigger settingsã€å·²å°è‡´æˆæœ¬å•é¡Œ
- âŒ åè€Œä½¿å•é¡Œæ›´åš´é‡ï¼š500ms = æ¯ç§’ 2 æ¬¡ = æ¯åˆ†é˜ 120 æ¬¡æª¢æŸ¥

**é¸é … E - Trigger Once + Job æ’ç¨‹ï¼š**
- âš ï¸ **çˆ­è­°é¸é …**ï¼ˆç¤¾ç¾¤ 58% æ”¯æŒ Eï¼Œ42% æ”¯æŒ Cï¼‰
- âœ… å„ªé»ï¼šç¢ºå¯¦èƒ½é™ä½æˆæœ¬ä¸”ç¬¦åˆ 10 åˆ†é˜è¦æ±‚
- âŒ ç‚ºä½•ä¸å¦‚ Cï¼š
  - éœ€è¦æ”¹è®Šæ¶æ§‹ï¼ˆå¾ä¸²æµæ”¹ç‚ºæ’ç¨‹æ‰¹æ¬¡ï¼‰
  - é¡Œç›®å•ã€Œå“ªå€‹èª¿æ•´ã€ï¼Œæš—ç¤ºåœ¨ç¾æœ‰æ¶æ§‹ä¸Šå„ªåŒ–
  - C æ›´ç›´æ¥é‡å°å•é¡Œæ ¹æºï¼ˆæ¸›å°‘ API å‘¼å«é »ç‡ï¼‰
  - Instance pools å·²é…ç½®ç”¨æ–¼æ‰¹æ¬¡ä½œæ¥­ï¼Œæš—ç¤ºç¾æœ‰ä¸²æµæ‡‰ç¶­æŒä¸²æµæ¶æ§‹

### è¨˜æ†¶å£è¨£
- **ã€Œç©ºæ‰¹å¤š â†’ trigger æ…¢ï¼›API è²´ â†’ é™é »ç‡ã€**ï¼šç©ºæ‰¹æ¬¡å¤šä»£è¡¨æª¢æŸ¥éæ–¼é »ç¹ï¼Œå»¶é•· trigger interval ç›´æ¥æ¸›å°‘ API æˆæœ¬ã€‚

### è§£é¡Œæ­¥é©Ÿ
1. **è­˜åˆ¥æˆæœ¬ä¾†æº**ï¼šæ¯åˆ†é˜ 12 æ¬¡ç©ºæ‰¹æ¬¡ â†’ éåº¦é »ç¹çš„ API å‘¼å«
2. **æª¢æŸ¥å»¶é²å®¹å¿**ï¼š10 åˆ†é˜å…§è™•ç†å³å¯ â†’ æœ‰å¾ˆå¤§èª¿æ•´ç©ºé–“
3. **é¸æ“‡æœ€ç›´æ¥æ–¹æ¡ˆ**ï¼šå»¶é•· trigger interval è‡³æœ€å¤§å…è¨±å€¼ï¼ˆ10 åˆ†é˜ï¼‰
4. **é©—è­‰å…¶ä»–é¸é …**ï¼šæ’é™¤ç„¡é—œï¼ˆshuffleï¼‰ã€åæ•ˆæœï¼ˆ500msï¼‰ã€æ¶æ§‹è®Šæ›´ï¼ˆtrigger onceï¼‰

### å¸¸è¦‹é™·é˜±è­¦ç¤º
- âš ï¸ **æŠŠ 500ms ç•¶æˆè§£æ±ºæ–¹æ¡ˆ**ï¼šé€™æ˜¯é è¨­å€¼ï¼Œå°±æ˜¯é€ æˆå•é¡Œçš„åŸå› 
- âš ï¸ **æ··æ·† spill æˆæœ¬èˆ‡ API æˆæœ¬**ï¼šé¡Œç›®æ˜ç¢ºæŒ‡å‡ºæ˜¯ã€Œstorage costsã€è€Œéè¨ˆç®—æˆæœ¬
- âš ï¸ **éåº¦å„ªåŒ–**ï¼šE é›–å¯è¡Œä½†æ”¹è®Šæ¶æ§‹ï¼ŒC åœ¨ç¾æœ‰æ¶æ§‹ä¸Šèª¿æ•´æ›´ç°¡æ½”

### C vs E æ·±åº¦æ¯”è¼ƒ

| é¢å‘ | C (å»¶é•· Trigger Interval) | E (Trigger Once + Job) |
|------|--------------------------|------------------------|
| æˆæœ¬é™ä½ | âœ… æ¸›å°‘ API å‘¼å« | âœ… æ¸›å°‘ API å‘¼å« + çœ compute |
| æ¶æ§‹è®Šæ›´ | âœ… æœ€å°ï¼ˆåƒ…èª¿åƒæ•¸ï¼‰ | âŒ å¾ä¸²æµæ”¹æ‰¹æ¬¡ |
| ç¬¦åˆé¡Œæ„ | âœ…ã€Œadjustmentã€æš—ç¤ºå¾®èª¿ | âš ï¸ éœ€é‡æ–°è¨­è¨ˆ |
| å¯¦æ–½è¤‡é›œåº¦ | âœ… ä¸€è¡Œç¨‹å¼ç¢¼ | âš ï¸ éœ€é…ç½® Job æ’ç¨‹ |
| é¢¨éšª | âœ… ä½ | âš ï¸ ä¸­ï¼ˆcheckpoint è™•ç†ï¼‰ |

**çµè«–ï¼š** C æ˜¯åœ¨ç¾æœ‰ä¸²æµæ¶æ§‹ä¸‹æœ€ç›´æ¥ã€é¢¨éšªæœ€ä½çš„æˆæœ¬å„ªåŒ–æ–¹æ¡ˆã€‚

---

## ğŸ·ï¸ æ¨™ç±¤èˆ‡åˆ†é¡
**Topics:** `Streaming`, `Spark-Performance`, `Cluster-Management`
**Traps:** `Performance-Misconception`, `Execution-Behavior`
**Difficulty:** `L2-Intermediate`
