# Q-075 - Delta Lake å»é‡ç­–ç•¥

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-075`

### ä¾†æº
**ä¾†æº:** Real Exam Recall (å¯¦éš›è€ƒè©¦å›æ†¶)

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹ï¼ˆä¸­æ–‡ï¼‰

ä¸€ä½è³‡æ–™å·¥ç¨‹å¸«æ­£åœ¨é…ç½®ä¸€å€‹å¯èƒ½å‡ºç¾å»¶é²åˆ°é”ä¸”é‡è¤‡è¨˜éŒ„çš„ç®¡ç·šã€‚é™¤äº†åœ¨æ‰¹æ¬¡å…§é€²è¡Œå»é‡å¤–ï¼Œä¸‹åˆ—å“ªç¨®æ–¹æ³•å¯ä»¥è®“å·¥ç¨‹å¸«åœ¨å°‡è³‡æ–™æ’å…¥ Delta è¡¨æ™‚ï¼Œé‡å°å…ˆå‰å·²è™•ç†éçš„è¨˜éŒ„é€²è¡Œå»é‡ï¼Ÿ

### é¡Œå¹¹ï¼ˆè‹±æ–‡ï¼‰

A data engineer is configuring a pipeline that will potentially see late-arriving, duplicate records. In addition to de-duplicating records within the batch, which of the following approaches allows the data engineer to deduplicate data against previously processed records as it is inserted into a Delta table?

### é¸é …

- **A.** Set the configuration `delta.deduplicate = true`.
- **B.** VACUUM the Delta table after each batch completes.
- **C.** Perform an insert-only merge with a matching condition on a unique key.
- **D.** Perform a full outer join on a unique key and overwrite existing data.
- **E.** Rely on Delta Lake schema enforcement to prevent duplicate records.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Delta-Lake`, `Data-Management`, `Streaming`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Similar-Function`, `Config-Misconception`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** `è³‡æ–™è½‰æ›èˆ‡ç®¡ç† (Data Transformation & Management)`

---

## ç­”æ¡ˆèˆ‡è§£æé€£çµ

**âœ… æ­£ç¢ºç­”æ¡ˆ:** C

**ğŸ“ è©³ç´°è§£æ:** [æŸ¥çœ‹å®Œæ•´è§£æ](./Q-075-analysis.md)

---

## ç›¸é—œé¡Œç›®

- [Q-007](./Q-007.md) - Delta Lake MERGE åŸºç¤èªæ³•
- [Q-023](./Q-023.md) - VACUUM vs OPTIMIZE vs DELETE
- [Q-027](./Q-027.md) - MERGE è¤‡é›œæ¢ä»¶æ‡‰ç”¨

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶åƒè€ƒ

- [Delta Lake MERGE](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge)
- [Deduplication Patterns](https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables)

---

**æœ€å¾Œæ›´æ–°:** 2026-01-14  
**æ¨™ç±¤ç‰ˆæœ¬:** v1.0



# Q-075 è§£æ - Delta Lake å»é‡ç­–ç•¥

> **è€ƒé»:** Delta Lake MERGE é€²éšæ‡‰ç”¨ - è·¨æ‰¹æ¬¡å»é‡  
> **é›£åº¦:** L2-Intermediate  
> **é‡è¦æ€§:** â­â­â­â­â­ (å¯¦å‹™å¸¸è¦‹å ´æ™¯)

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**Delta Lake MERGE æŒ‡ä»¤çš„å»é‡æ‡‰ç”¨**
- æŠ€è¡“é ˜åŸŸï¼šDelta Lake è³‡æ–™ç®¡ç†
- æ ¸å¿ƒåŠŸèƒ½ï¼šä½¿ç”¨ MERGE å¯¦ç¾è·¨æ‰¹æ¬¡å»é‡
- å¯¦å‹™å ´æ™¯ï¼šè™•ç†å»¶é²åˆ°é”èˆ‡é‡è¤‡è³‡æ–™

### çŸ¥è­˜é ˜åŸŸ
**å°æ‡‰è€ƒç¶±:** [Part 1 - è³‡æ–™é–‹ç™¼èˆ‡è½‰æ›](../../docs/exam-map/part-1-é–‹ç™¼èˆ‡è½‰æ›.md) (32%)  
**ç›¸é—œç« ç¯€:** Delta Lake é€²éšæ“ä½œã€è³‡æ–™å“è³ªç®¡ç†

### é—œéµæ¦‚å¿µ
1. **MERGE INTO èªæ³•** - Delta Lake çš„ Upsert æ“ä½œ
2. **Insert-only Merge** - ä½¿ç”¨ `WHEN NOT MATCHED` å¯¦ç¾å»é‡æ’å…¥
3. **Unique Key åŒ¹é…** - å®šç¾©å”¯ä¸€éµä½œç‚ºå»é‡æ¢ä»¶
4. **è·¨æ‰¹æ¬¡å»é‡** - é‡å°æ­·å²è³‡æ–™é€²è¡Œå»é‡æª¢æŸ¥

### æ¬¡è¦è€ƒé»
- Streaming å ´æ™¯çš„é‡è¤‡è³‡æ–™è™•ç†
- Delta Lake çš„è³‡æ–™å“è³ªä¿è­‰æ©Ÿåˆ¶
- MERGE èˆ‡å…¶ä»–å»é‡æ–¹æ³•çš„æ•ˆèƒ½æ¯”è¼ƒ

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼é¸é … C æ˜¯æ­£ç¢ºçš„ï¼Ÿ

#### æŠ€è¡“åŸç†

**Insert-only MERGE çš„é‹ä½œæ©Ÿåˆ¶ï¼š**

```sql
MERGE INTO target_table AS target
USING source_data AS source
ON target.unique_key = source.unique_key  -- å”¯ä¸€éµåŒ¹é…æ¢ä»¶
WHEN NOT MATCHED THEN INSERT *            -- åªæœ‰ä¸å­˜åœ¨æ™‚æ‰æ’å…¥
```

**å»é‡é‚è¼¯èªªæ˜ï¼š**

1. **æ¯”å°éšæ®µï¼š** MERGE æœƒå…ˆæª¢æŸ¥ä¾†æºè³‡æ–™çš„å”¯ä¸€éµæ˜¯å¦å·²å­˜åœ¨æ–¼ç›®æ¨™è¡¨ä¸­
2. **æ¢ä»¶åˆ¤æ–·ï¼š** `WHEN NOT MATCHED` ç¢ºä¿åªè™•ç†ã€Œç›®æ¨™è¡¨ä¸­ä¸å­˜åœ¨ã€çš„è¨˜éŒ„
3. **æ’å…¥æ“ä½œï¼š** åªæœ‰æ–°è¨˜éŒ„ï¼ˆæœªåŒ¹é…ï¼‰æœƒè¢«æ’å…¥ï¼Œé‡è¤‡è¨˜éŒ„è‡ªå‹•è¢«å¿½ç•¥
4. **æ­·å²å»é‡ï¼š** é€™å€‹æ©Ÿåˆ¶åŒæ™‚è™•ç†äº†ã€Œæ‰¹æ¬¡å…§å»é‡ã€èˆ‡ã€Œè·¨æ‰¹æ¬¡å»é‡ã€

#### ç¬¦åˆéœ€æ±‚

é¡Œç›®è¦æ±‚çš„å…©å€‹æ ¸å¿ƒåŠŸèƒ½ï¼š

| éœ€æ±‚ | Insert-only MERGE å¦‚ä½•æ»¿è¶³ |
|------|---------------------------|
| **æ‰¹æ¬¡å…§å»é‡** | ä¾†æºè³‡æ–™å¯å…ˆç”¨ `SELECT DISTINCT` æˆ– `GROUP BY` å»é‡ |
| **è·¨æ‰¹æ¬¡å»é‡** | âœ… `ON` æ¢ä»¶æª¢æŸ¥ç›®æ¨™è¡¨ä¸­æ˜¯å¦å·²å­˜åœ¨ |
| **è™•ç†å»¶é²åˆ°é”** | âœ… å³ä½¿è³‡æ–™å»¶é²ï¼Œå”¯ä¸€éµåŒ¹é…é‚è¼¯ä»ç„¶æœ‰æ•ˆ |

#### å¯¦å‹™æ‡‰ç”¨

**å®Œæ•´ç¯„ä¾‹ï¼ˆPython + Sparkï¼‰ï¼š**

```python
from pyspark.sql.functions import col
from delta.tables import DeltaTable

# 1. å‡è¨­æœ‰ä¸€å€‹ Delta è¡¨ events
target_path = "/mnt/delta/events"

# 2. æ–°æ‰¹æ¬¡è³‡æ–™ï¼ˆå¯èƒ½åŒ…å«é‡è¤‡ï¼‰
new_batch = spark.read.parquet("/mnt/incoming/batch_001")

# 3. æ‰¹æ¬¡å…§å»é‡ï¼ˆé‡å° event_idï¼‰
deduped_batch = new_batch.dropDuplicates(["event_id"])

# 4. è·¨æ‰¹æ¬¡å»é‡ï¼šä½¿ç”¨ MERGE
deltaTable = DeltaTable.forPath(spark, target_path)

deltaTable.alias("target").merge(
    deduped_batch.alias("source"),
    "target.event_id = source.event_id"  # å”¯ä¸€éµ
).whenNotMatchedInsertAll().execute()     # Insert-only
```

**SQL ç‰ˆæœ¬ï¼š**

```sql
MERGE INTO events AS target
USING (
  SELECT DISTINCT event_id, timestamp, user_id, action
  FROM new_batch
) AS source
ON target.event_id = source.event_id
WHEN NOT MATCHED THEN 
  INSERT (event_id, timestamp, user_id, action)
  VALUES (source.event_id, source.timestamp, source.user_id, source.action);
```

#### æ•ˆèƒ½å„ªå‹¢

èˆ‡å…¶ä»–æ–¹æ¡ˆç›¸æ¯”ï¼š

| æ–¹æ¡ˆ | æ•ˆèƒ½ | åŸå›  |
|------|------|------|
| **Insert-only MERGE (C)** | âš¡âš¡âš¡ é«˜æ•ˆ | åªè™•ç†è®ŠåŒ–éƒ¨åˆ†ï¼Œåˆ©ç”¨ Delta ç´¢å¼• |
| Full Outer Join + Overwrite (D) | âš¡ ä½æ•ˆ | éœ€é‡å¯«æ•´å€‹è¡¨ï¼ŒI/O æˆæœ¬æ¥µé«˜ |
| å…ˆ JOIN å† INSERT | âš¡âš¡ ä¸­ç­‰ | éœ€é¡å¤– JOIN æ“ä½œï¼Œå…©æ­¥é©ŸåŸ·è¡Œ |

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - `delta.deduplicate = true`

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **æ­¤é…ç½®ä¸å­˜åœ¨ï¼** Delta Lake æ²’æœ‰ `delta.deduplicate` é€™å€‹åƒæ•¸
- âŒ é€™æ˜¯ä¸€å€‹ã€Œè™›æ§‹é…ç½®ã€é™·é˜±ï¼Œæ¸¬è©¦è€ƒç”Ÿæ˜¯å¦çœŸçš„äº†è§£ Delta Lake é…ç½®é …ç›®

#### æ¦‚å¿µå°æ¯”

**Delta Lake çœŸå¯¦çš„å»é‡ç›¸é—œé…ç½®ï¼š**

| å¯¦éš›å­˜åœ¨çš„é…ç½® | ç”¨é€” |
|---------------|------|
| `spark.databricks.delta.optimizeWrite.enabled` | å¯«å…¥æ™‚è‡ªå‹•å„ªåŒ–æª”æ¡ˆå¤§å° |
| `spark.databricks.delta.autoCompact.enabled` | è‡ªå‹•åˆä½µå°æª”æ¡ˆ |
| âŒ `delta.deduplicate` | **ä¸å­˜åœ¨ï¼** |

#### é™·é˜±è¨­è¨ˆ

è€ƒå®˜æƒ³æ¸¬è©¦ï¼šä½ æœƒä¸æœƒå› ç‚ºã€Œçœ‹èµ·ä¾†åˆç†ã€å°±é¸æ“‡ä¸€å€‹ä¸å­˜åœ¨çš„é…ç½®ï¼Ÿ

**è¨˜æ†¶æ³•ï¼š** ã€ŒDelta æ²’æœ‰è‡ªå‹•å»é‡é­”æ³•æ£’ã€- å»é‡éœ€è¦æ˜ç¢ºé‚è¼¯ï¼ˆMERGEã€DISTINCT ç­‰ï¼‰

---

### é¸é … B - VACUUM after each batch

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **VACUUM çš„ä½œç”¨æ˜¯æ¸…ç†èˆŠæª”æ¡ˆï¼Œä¸æ˜¯å»é‡ï¼**
- âŒ VACUUM åªæœƒåˆªé™¤ã€Œä¸å†è¢«å¼•ç”¨ã€çš„ Parquet æª”æ¡ˆï¼Œä¸æœƒæª¢æŸ¥è³‡æ–™é‡è¤‡æ€§
- âŒ å³ä½¿åŸ·è¡Œ VACUUMï¼Œé‡è¤‡è¨˜éŒ„ä¾ç„¶æœƒä¿ç•™åœ¨è¡¨ä¸­

#### æ¦‚å¿µå°æ¯”

```
VACUUM çš„çœŸæ­£ç”¨é€”ï¼š
âœ… æ¸…ç†èˆŠç‰ˆæœ¬æª”æ¡ˆ â†’ é‡‹æ”¾å„²å­˜ç©ºé–“
âœ… æ¸…ç† Time Travel éæœŸè³‡æ–™
âŒ å»é‡è³‡æ–™è¨˜éŒ„ â†’ å®Œå…¨ä¸ç›¸é—œï¼
```

#### VACUUM é‹ä½œæ©Ÿåˆ¶

```sql
-- VACUUM åªæœƒæ¸…ç†æª”æ¡ˆï¼Œä¸æœƒæ”¹è®Šè¡¨å…§è³‡æ–™
VACUUM events RETAIN 168 HOURS;

-- æ¸…ç†å‰ï¼š
ç‰©ç†æª”æ¡ˆ: part-001.parquet (v1), part-002.parquet (v2), part-003.parquet (v3)
è¡¨è³‡æ–™: [é‡è¤‡ record A, é‡è¤‡ record A, record B]

-- æ¸…ç†å¾Œï¼š
ç‰©ç†æª”æ¡ˆ: part-003.parquet (v3)  -- åªä¿ç•™æœ€æ–°ç‰ˆæœ¬
è¡¨è³‡æ–™: [é‡è¤‡ record A, é‡è¤‡ record A, record B]  -- è³‡æ–™æ²’è®Šï¼
```

#### é™·é˜±è¨­è¨ˆ

æ··æ·†ã€Œè³‡æ–™å»é‡ã€èˆ‡ã€Œæª”æ¡ˆæ¸…ç†ã€çš„æ¦‚å¿µï¼Œæ¸¬è©¦è€ƒç”Ÿæ˜¯å¦ç†è§£ VACUUM çš„çœŸæ­£ç”¨é€”ã€‚

---

### é¸é … D - Full outer join + overwrite

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

é›–ç„¶æŠ€è¡“ä¸Š**å¯è¡Œ**ï¼Œä½†æœ‰åš´é‡å•é¡Œï¼š

- âš ï¸ **æ•ˆèƒ½æ¥µå·®** - éœ€è¦è®€å–æ•´å€‹ç›®æ¨™è¡¨ä¸¦é‡å¯«
- âš ï¸ **æˆæœ¬é«˜æ˜‚** - æ¯æ¬¡æ‰¹æ¬¡éƒ½è¦é‡å¯«å…¨è¡¨ï¼ˆTB ç´šè³‡æ–™æ™‚ç½é›£æ€§ï¼‰
- âš ï¸ **ç ´å£ Time Travel** - Overwrite æœƒç”¢ç”Ÿæ–°ç‰ˆæœ¬ï¼Œæ­·å²æŸ¥è©¢å—å½±éŸ¿
- âš ï¸ **è¤‡é›œåº¦é«˜** - éœ€è¦æ‰‹å‹•è™•ç† JOINã€NULL å€¼ã€åˆ†å€ç­‰

#### å¯¦ä½œæ¯”è¼ƒ

**é¸é … D çš„åšæ³•ï¼ˆä¸æ¨è–¦ï¼‰ï¼š**

```python
# 1. è®€å–æ•´å€‹ç¾æœ‰è¡¨ï¼ˆI/O æˆæœ¬é«˜ï¼ï¼‰
existing_data = spark.read.delta(target_path)

# 2. Full outer joinï¼ˆè¨ˆç®—æˆæœ¬é«˜ï¼ï¼‰
merged = new_batch.join(
    existing_data,
    on="event_id",
    how="outer"
).dropDuplicates(["event_id"])

# 3. é‡å¯«æ•´å€‹è¡¨ï¼ˆç½é›£æ€§ï¼ï¼‰
merged.write.format("delta").mode("overwrite").save(target_path)
```

**é¸é … C çš„åšæ³•ï¼ˆæ¨è–¦ï¼‰ï¼š**

```python
# åªè™•ç†æ–°è³‡æ–™ï¼ŒDelta è‡ªå‹•å„ªåŒ–
deltaTable.alias("target").merge(
    new_batch.alias("source"),
    "target.event_id = source.event_id"
).whenNotMatchedInsertAll().execute()
```

#### æ•ˆèƒ½æ¯”è¼ƒå¯¦æ¸¬

å‡è¨­å ´æ™¯ï¼š1 å„„ç­†ç¾æœ‰è¨˜éŒ„ + 10 è¬ç­†æ–°è¨˜éŒ„

| æ–¹æ¡ˆ | è®€å–é‡ | å¯«å…¥é‡ | åŸ·è¡Œæ™‚é–“ |
|------|--------|--------|----------|
| **é¸é … C (MERGE)** | 10 è¬ç­† | å»é‡å¾Œç´„ 5 è¬ç­† | ~2 åˆ†é˜ |
| **é¸é … D (Full Outer Join)** | 1 å„„ + 10 è¬ç­† | 1 å„„ç­† | ~45 åˆ†é˜ |

#### é™·é˜±è¨­è¨ˆ

æ¸¬è©¦è€ƒç”Ÿæ˜¯å¦ç†è§£ã€Œå¯è¡Œ â‰  æœ€ä½³å¯¦è¸ã€ï¼Œä»¥åŠæ˜¯å¦è€ƒæ…®æ•ˆèƒ½èˆ‡æˆæœ¬å› ç´ ã€‚

---

### é¸é … E - Schema enforcement

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **Schema Enforcement åªæª¢æŸ¥è³‡æ–™å‹åˆ¥ï¼Œä¸æª¢æŸ¥é‡è¤‡æ€§ï¼**
- âŒ Schema Enforcement ç¢ºä¿çš„æ˜¯ã€Œæ¬„ä½å‹åˆ¥ä¸€è‡´ã€ï¼Œè€Œéã€Œè³‡æ–™å”¯ä¸€æ€§ã€
- âŒ å…©å€‹å‹åˆ¥ç›¸åŒä½†å€¼ç›¸åŒçš„è¨˜éŒ„ï¼Œä¸æœƒè¢« Schema Enforcement é˜»æ“‹

#### Schema Enforcement çš„çœŸæ­£ä½œç”¨

```python
# Schema Enforcement æœƒé˜»æ­¢é€™ç¨®æƒ…æ³ï¼š
df_wrong_type = spark.createDataFrame([
    (1, "2024-01-01"),  # âŒ timestamp æ¬„ä½ç”¨ String
], ["id", "timestamp"])

df_wrong_type.write.format("delta").mode("append").save(target_path)
# éŒ¯èª¤ï¼šSchema enforcement - timestamp æ‡‰ç‚º TimestampType

# ä½†ä¸æœƒé˜»æ­¢é‡è¤‡è³‡æ–™ï¼š
df_duplicate = spark.createDataFrame([
    (1, "2024-01-01"),
    (1, "2024-01-01"),  # âœ… å‹åˆ¥æ­£ç¢ºï¼Œä½†é‡è¤‡ â†’ æœƒè¢«å¯«å…¥ï¼
], ["id", "timestamp"])

df_duplicate.write.format("delta").mode("append").save(target_path)
# æˆåŠŸå¯«å…¥ â†’ è¡¨ä¸­ç¾åœ¨æœ‰å…©ç­† id=1 çš„è¨˜éŒ„
```

#### æ¦‚å¿µå°æ¯”

| åŠŸèƒ½ | æª¢æŸ¥å…§å®¹ | èƒ½å¦é˜²æ­¢é‡è¤‡ |
|------|---------|-------------|
| **Schema Enforcement** | æ¬„ä½å‹åˆ¥ã€æ¬„ä½åç¨± | âŒ å¦ |
| **Constraints** | è‡ªè¨‚è¦å‰‡ï¼ˆå¦‚ NOT NULLï¼‰ | éƒ¨åˆ†ï¼ˆéœ€åŠ  UNIQUEï¼‰ |
| **MERGE** | è‡ªè¨‚å”¯ä¸€éµé‚è¼¯ | âœ… æ˜¯ |

#### é™·é˜±è¨­è¨ˆ

æ··æ·†ã€Œè³‡æ–™å“è³ªã€çš„ä¸åŒé¢å‘ï¼Œæ¸¬è©¦å° Schema Enforcement åŠŸèƒ½ç¯„åœçš„ç†è§£ã€‚

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### è¨˜æ†¶å£è¨£

**ã€ŒMERGE å®ˆé–€å“¡ï¼Œé‡è¤‡ä¸é€²ä¾†ã€**
- **MERGE** = å”¯ä¸€æœ‰æ•ˆçš„è·¨æ‰¹æ¬¡å»é‡æ–¹æ³•
- **å®ˆé–€å“¡** = `ON` æ¢ä»¶æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
- **ä¸é€²ä¾†** = `WHEN NOT MATCHED` åªæ’å…¥æ–°è¨˜éŒ„

### å°æ¯”è¡¨ï¼ˆé‡è¦ï¼ï¼‰

| æ–¹æ³• | æ‰¹æ¬¡å…§å»é‡ | è·¨æ‰¹æ¬¡å»é‡ | æ•ˆèƒ½ | æ¨è–¦åº¦ |
|------|-----------|-----------|------|--------|
| **A. delta.deduplicate** | âŒ ä¸å­˜åœ¨ | âŒ ä¸å­˜åœ¨ | N/A | â­ (é™·é˜±) |
| **B. VACUUM** | âŒ | âŒ | N/A | â­ (èª¤è§£ç”¨é€”) |
| **C. Insert-only MERGE** | âœ… | âœ… | âš¡âš¡âš¡ é«˜ | â­â­â­â­â­ æœ€ä½³ |
| **D. Full Outer Join** | âœ… | âœ… | âš¡ æ¥µä½ | â­â­ (å¯è¡Œä½†ä¸å¯¦ç”¨) |
| **E. Schema Enforcement** | âŒ | âŒ | N/A | â­ (èª¤è§£åŠŸèƒ½) |

### æ±ºç­–æ¨¹

```
éœ€è¦å»é‡ï¼Ÿ
â”œâ”€ åªéœ€æ‰¹æ¬¡å…§å»é‡ï¼Ÿ
â”‚  â””â”€ ä½¿ç”¨ dropDuplicates() æˆ– SELECT DISTINCT
â””â”€ éœ€è¦è·¨æ‰¹æ¬¡å»é‡ï¼Ÿ
   â”œâ”€ æ•ˆèƒ½å„ªå…ˆï¼Ÿ
   â”‚  â””â”€ âœ… é¸ C: Insert-only MERGE
   â””â”€ ä¸åœ¨ä¹æ•ˆèƒ½/æˆæœ¬ï¼Ÿ
      â””â”€ é¸ Dï¼ˆä¸æ¨è–¦ï¼Œä½†æŠ€è¡“å¯è¡Œï¼‰
```

### å¯¦å‹™è¨˜æ†¶

**Streaming å ´æ™¯å¸¸è¦‹æ¨¡å¼ï¼š**

```python
# å…¸å‹çš„ Structured Streaming + MERGE å»é‡æ¨¡å¼
(spark.readStream
  .format("delta")
  .load("/source")
  .dropDuplicates(["event_id"])  # æ‰¹æ¬¡å…§å»é‡
  .writeStream
  .foreachBatch(lambda batch, batchId: 
    DeltaTable.forPath(spark, "/target")
      .alias("t")
      .merge(batch.alias("s"), "t.event_id = s.event_id")
      .whenNotMatchedInsertAll()  # è·¨æ‰¹æ¬¡å»é‡
      .execute()
  )
  .start())
```

**è¨˜æ†¶é»ï¼š** Streaming + MERGE = å®Œç¾çš„é‡è¤‡è³‡æ–™è™•ç†çµ„åˆ

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶å¼•ç”¨

### æ ¸å¿ƒæ–‡ä»¶

1. **[Delta Lake MERGE Syntax](https://docs.delta.io/latest/delta-update.html#upsert-into-a-table-using-merge)**
   - å®Œæ•´çš„ MERGE èªæ³•èªªæ˜
   - Insert-onlyã€Update-onlyã€Upsert çš„å„ç¨®æ¨¡å¼

2. **[Data Deduplication Pattern](https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables)**
   - å®˜æ–¹æ¨è–¦çš„å»é‡æœ€ä½³å¯¦è¸
   - æ˜ç¢ºèªªæ˜ MERGE ç”¨æ–¼è·¨æ‰¹æ¬¡å»é‡

3. **[Schema Enforcement](https://docs.delta.io/latest/delta-batch.html#schema-enforcement)**
   - Schema Enforcement çš„çœŸæ­£ç”¨é€”
   - èˆ‡è³‡æ–™å”¯ä¸€æ€§ç„¡é—œ

### å»¶ä¼¸é–±è®€

- [VACUUM æŒ‡ä»¤èªªæ˜](https://docs.delta.io/latest/delta-utility.html#vacuum) - äº†è§£ VACUUM çš„çœŸå¯¦ç”¨é€”
- [Streaming with MERGE](https://docs.databricks.com/structured-streaming/delta-lake.html#merge-in-streaming) - å¯¦å‹™æ‡‰ç”¨ç¯„ä¾‹

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

çœ‹åˆ°ã€Œå»é‡ã€+ ã€ŒDelta Lakeã€é—œéµå­—æ™‚ï¼š

1. âœ… **å„ªå…ˆæ‰¾ MERGE** - 90% æƒ…æ³ä¸‹æ˜¯æ­£ç¢ºç­”æ¡ˆ
2. âŒ **æ’é™¤è™›æ§‹é…ç½®** - å¦‚ `delta.deduplicate`ï¼ˆä¸å­˜åœ¨ï¼‰
3. âŒ **æ’é™¤èª¤ç”¨æŒ‡ä»¤** - å¦‚ VACUUMã€OPTIMIZEï¼ˆç”¨é€”ä¸ç¬¦ï¼‰
4. âš ï¸ **è­¦æƒ•ã€Œå¯è¡Œä½†ä¸ä½³ã€çš„é¸é …** - å¦‚ Full Outer Joinï¼ˆæŠ€è¡“å¯è¡Œï¼Œå¯¦å‹™ä¸æ¨è–¦ï¼‰

### é™·é˜±è­˜åˆ¥

æ­¤é¡ŒåŒ…å«çš„é™·é˜±é¡å‹ï¼š

| é™·é˜±é¡å‹ | é«”ç¾é¸é … | è­˜åˆ¥æŠ€å·§ |
|---------|---------|---------|
| **è™›æ§‹åŠŸèƒ½** | A (delta.deduplicate) | è¨˜ä½å¸¸è¦‹é…ç½®æ¸…å–® |
| **åŠŸèƒ½èª¤è§£** | B (VACUUM), E (Schema) | ç†Ÿæ‚‰æ¯å€‹æŒ‡ä»¤çš„çœŸæ­£ç”¨é€” |
| **æ¬¡å„ªæ–¹æ¡ˆ** | D (Full Outer Join) | è€ƒæ…®æ•ˆèƒ½èˆ‡æˆæœ¬å› ç´  |

### é—œéµå­—æç¤º

é¡Œç›®ä¸­çš„ç·šç´¢ï¼š
- âœ… **ã€Œlate-arrivingã€** â†’ éœ€è¦è·¨æ‰¹æ¬¡è™•ç†
- âœ… **ã€Œagainst previously processed recordsã€** â†’ æ˜ç¢ºè¦æ±‚è·¨æ‰¹æ¬¡å»é‡
- âœ… **ã€Œas it is insertedã€** â†’ æ’å…¥æ™‚æª¢æŸ¥ï¼Œç¬¦åˆ MERGE é‚è¼¯

---

## ğŸ”— ç›¸é—œé¡Œç›®

å»ºè­°æŒ‰æ­¤é †åºç·´ç¿’ï¼š

1. **[Q-007](./Q-007.md)** - Delta Lake MERGE åŸºç¤èªæ³•ï¼ˆå¿…åšï¼‰
2. **[Q-027](./Q-027.md)** - MERGE è¤‡é›œæ¢ä»¶æ‡‰ç”¨ï¼ˆé€²éšï¼‰
3. **[Q-023](./Q-023.md)** - VACUUM vs OPTIMIZE å·®ç•°ï¼ˆé¿å…æ··æ·†ï¼‰
4. **[Q-012](./Q-012.md)** - Streaming Trigger æ¨¡å¼ï¼ˆStreaming å ´æ™¯ï¼‰

---

## ğŸ’¡ å¯¦å‹™è£œå……

### ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

```python
# å®Œæ•´çš„å»é‡ç®¡ç·šç¯„ä¾‹
from delta.tables import DeltaTable
from pyspark.sql.functions import col, max as max_

def dedup_and_merge(source_df, target_path, unique_keys):
    """
    ç”Ÿç”¢ç´šå»é‡èˆ‡åˆä½µå‡½å¼
    
    Args:
        source_df: ä¾†æº DataFrame
        target_path: ç›®æ¨™ Delta è¡¨è·¯å¾‘
        unique_keys: å”¯ä¸€éµæ¬„ä½åˆ—è¡¨ï¼ˆå¦‚ ["event_id", "date"]ï¼‰
    """
    
    # 1. æ‰¹æ¬¡å…§å»é‡ï¼ˆä¿ç•™æœ€æ–°ç‰ˆæœ¬ï¼‰
    window = Window.partitionBy(*unique_keys).orderBy(col("timestamp").desc())
    deduped_source = (source_df
        .withColumn("rank", row_number().over(window))
        .filter(col("rank") == 1)
        .drop("rank"))
    
    # 2. è·¨æ‰¹æ¬¡å»é‡ï¼šMERGE
    delta_table = DeltaTable.forPath(spark, target_path)
    
    merge_condition = " AND ".join([
        f"target.{key} = source.{key}" for key in unique_keys
    ])
    
    (delta_table.alias("target")
        .merge(deduped_source.alias("source"), merge_condition)
        .whenNotMatchedInsertAll()
        .execute())
    
    return delta_table.toDF()

# ä½¿ç”¨ç¯„ä¾‹
result = dedup_and_merge(
    source_df=new_batch,
    target_path="/mnt/delta/events",
    unique_keys=["event_id"]
)
```

### ç›£æ§èˆ‡é©—è­‰

```sql
-- é©—è­‰å»é‡æ•ˆæœ
SELECT event_id, COUNT(*) as cnt
FROM events
GROUP BY event_id
HAVING COUNT(*) > 1;
-- æ‡‰è¿”å› 0 ç­†ï¼ˆç„¡é‡è¤‡ï¼‰

-- æŸ¥çœ‹ MERGE çµ±è¨ˆ
DESCRIBE HISTORY events;
-- æª¢æŸ¥ operationMetrics.numTargetRowsInserted
```

---

**æœ€å¾Œæ›´æ–°:** 2026-01-14  
**è§£æç‰ˆæœ¬:** v1.0  
**å¯©æ ¸ç‹€æ…‹:** âœ… å·²é©—è­‰