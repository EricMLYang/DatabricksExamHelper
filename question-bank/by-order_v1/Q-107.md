# Question #107

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-107`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

Which statement describes Delta Lake optimized writes?

### é¸é …

- **A.** Before a Jobs cluster terminates, OPTIMIZE is executed on all tables modified during the most recent job.
- **B.** An asynchronous job runs after the write completes to detect if files could be further compacted; if yes, an OPTIMIZE job is executed toward a default of 1 GB.
- **C.** Data is queued in a messaging bus instead of committing data directly to memory; all data is committed from the messaging bus in one batch once the job is complete.
- **D.** Optimized writes use logical partitions instead of directory partitions; because partition boundaries are only represented in metadata, fewer small files are written.
- **E.** A shuffle occurs prior to writing to try to group similar data together resulting in fewer files instead of each executor writing multiple files based on directory partitions.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Delta-Lake`, `Performance-Optimization`, `File-Management`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Concept-Confusion`, `Feature-Name-Similarity`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Performance & Cost Optimization

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `E`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** E
- **ç¤¾ç¾¤å…±è­˜:** E

---

# é¡Œç›®è§£æ

---

## é¡Œç›®å›é¡§

### é¡Œç›®ç·¨è™Ÿèˆ‡é€£çµ
**é¡Œç›® ID:** `Q-107`
**é¡Œç›®é€£çµ:** [é»æ­¤è¿”å›é¡Œç›®](#question-107)

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `E`

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Delta Lake Optimized Writes
**çŸ¥è­˜é ˜åŸŸ:** Performance Optimization / File Management
**é—œéµæ¦‚å¿µ:**
- Optimized Writes vs Auto Compaction
- Shuffle-based data grouping
- Small files problem è§£æ±ºæ–¹æ¡ˆ
- Write-time optimization

### æ¬¡è¦è€ƒé»
- Auto Compaction çš„é‹ä½œæ©Ÿåˆ¶
- OPTIMIZE å‘½ä»¤çš„åŸ·è¡Œæ™‚æ©Ÿ
- Partition å¯«å…¥ç­–ç•¥

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ E æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†:**

**1. Optimized Writes çš„æ ¸å¿ƒæ©Ÿåˆ¶ï¼š**

Optimized Writes é€é**å¯«å…¥å‰çš„ Shuffle æ“ä½œ**ä¾†æ”¹å–„æª”æ¡ˆå¤§å°å•é¡Œï¼š

```python
# å•Ÿç”¨ Optimized Writes
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")

# å¯«å…¥è³‡æ–™æ™‚ï¼ŒSpark æœƒè‡ªå‹•åŸ·è¡Œä»¥ä¸‹æ­¥é©Ÿï¼š
df.write.format("delta").save("/path/to/table")

# å…§éƒ¨æµç¨‹ï¼š
# 1. âš ï¸ åŸ·è¡Œ Shuffleï¼šå°‡ç›¸ä¼¼è³‡æ–™åˆ†çµ„
# 2. æ¯å€‹ Task å¯«å…¥æ›´å¤§çš„æª”æ¡ˆï¼ˆè€Œéå¤šå€‹å°æª”æ¡ˆï¼‰
# 3. çµæœï¼šfewer, larger files
```

**âœ… ç‚ºä»€éº¼é€™æ˜¯æ­£ç¢ºçš„æè¿°ï¼Ÿ**

| ç‰¹æ€§ | èªªæ˜ |
|------|------|
| **Shuffle Prior to Write** | åœ¨å¯«å…¥å‰åŸ·è¡Œ shuffle æ“ä½œ |
| **Group Similar Data** | å°‡ç›¸ä¼¼çš„è³‡æ–™ï¼ˆä¾‹å¦‚ç›¸åŒ partition keyï¼‰åˆ†çµ„ |
| **Fewer Files** | æ¯å€‹ executor å¯«å…¥è¼ƒå°‘ä½†è¼ƒå¤§çš„æª”æ¡ˆ |
| **Automatic** | å•Ÿç”¨å¾Œè‡ªå‹•åŸ·è¡Œï¼Œç„¡éœ€æ‰‹å‹•å¹²é  |
| **Write-time Optimization** | åœ¨å¯«å…¥æ™‚å„ªåŒ–ï¼Œéå¯«å…¥å¾Œ |

**2. æ²’æœ‰ Optimized Writes çš„å•é¡Œï¼š**

```
å‚³çµ±å¯«å…¥è¡Œç‚ºï¼ˆç„¡ Optimized Writesï¼‰ï¼š

Executor 1: è™•ç† 100 ç­†è¨˜éŒ„
  â”œâ”€ Partition A: 20 ç­† â†’ å¯«å…¥ file_1.parquet (å°æª”æ¡ˆ)
  â”œâ”€ Partition B: 15 ç­† â†’ å¯«å…¥ file_2.parquet (å°æª”æ¡ˆ)
  â”œâ”€ Partition C: 30 ç­† â†’ å¯«å…¥ file_3.parquet (å°æª”æ¡ˆ)
  â””â”€ Partition D: 35 ç­† â†’ å¯«å…¥ file_4.parquet (å°æª”æ¡ˆ)

Executor 2: è™•ç† 100 ç­†è¨˜éŒ„
  â”œâ”€ Partition A: 18 ç­† â†’ å¯«å…¥ file_5.parquet (å°æª”æ¡ˆ)
  â”œâ”€ Partition B: 22 ç­† â†’ å¯«å…¥ file_6.parquet (å°æª”æ¡ˆ)
  ...

çµæœï¼šç”¢ç”Ÿå¤§é‡å°æª”æ¡ˆï¼ˆSmall Files Problemï¼‰
```

**3. ä½¿ç”¨ Optimized Writes å¾Œï¼š**

```
Optimized Writes è¡Œç‚ºï¼š

[å¯«å…¥å‰åŸ·è¡Œ Shuffle]
  â†“
å°‡æ‰€æœ‰ Partition A çš„è³‡æ–™é›†ä¸­
  â”œâ”€ Executor 1: å¯«å…¥ partition_a_file_1.parquet (1 GB)
  â”œâ”€ Executor 2: å¯«å…¥ partition_a_file_2.parquet (1 GB)
  â””â”€ Executor 3: å¯«å…¥ partition_a_file_3.parquet (1 GB)

å°‡æ‰€æœ‰ Partition B çš„è³‡æ–™é›†ä¸­
  â”œâ”€ Executor 4: å¯«å…¥ partition_b_file_1.parquet (1 GB)
  â””â”€ Executor 5: å¯«å…¥ partition_b_file_2.parquet (1 GB)

çµæœï¼šæ¯å€‹ executor å¯«å…¥è¼ƒå°‘ä½†è¼ƒå¤§çš„æª”æ¡ˆ
```

**4. å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š**

```python
# ç¯„ä¾‹ï¼šä½¿ç”¨ Optimized Writes æ”¹å–„å¯«å…¥æ•ˆèƒ½

from pyspark.sql.functions import *

# === æ–¹æ¡ˆ 1ï¼šå•Ÿç”¨ Optimized Writesï¼ˆæ¨è–¦ï¼‰ ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")

# å¯«å…¥å¤§é‡è³‡æ–™
large_df = spark.range(0, 10_000_000).withColumn(
    "category", (col("id") % 100).cast("string")
).withColumn(
    "value", rand()
)

# ç›´æ¥å¯«å…¥ï¼ˆOptimized Writes è‡ªå‹•åŸ·è¡Œ shuffleï¼‰
large_df.write.format("delta").mode("overwrite").save("/tmp/optimized_table")

# === æ¯”è¼ƒï¼šæœªå•Ÿç”¨ Optimized Writes ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "false")

large_df.write.format("delta").mode("overwrite").save("/tmp/normal_table")

# === æª¢æŸ¥æª”æ¡ˆæ•¸é‡å·®ç•° ===
# ä½¿ç”¨ dbutils.fs.ls() æŸ¥çœ‹æª”æ¡ˆæ•¸é‡
optimized_files = dbutils.fs.ls("/tmp/optimized_table")
normal_files = dbutils.fs.ls("/tmp/normal_table")

print(f"Optimized Writes: {len(optimized_files)} files")
print(f"Normal Write: {len(normal_files)} files")

# çµæœç¯„ä¾‹ï¼š
# Optimized Writes: 50 files (fewer, larger)
# Normal Write: 500 files (more, smaller)
```

**5. Optimized Writes çš„å·¥ä½œåŸç†ï¼ˆå…§éƒ¨è¦–è§’ï¼‰ï¼š**

```python
# Spark å…§éƒ¨æµç¨‹ï¼ˆç°¡åŒ–ç‰ˆï¼‰

def write_with_optimized_writes(df, path):
    """
    Optimized Writes çš„å…§éƒ¨é‚è¼¯
    """
    
    # Step 1: æª¢æŸ¥æ˜¯å¦å•Ÿç”¨ Optimized Writes
    if spark.conf.get("spark.databricks.delta.optimizeWrite.enabled") == "true":
        
        # Step 2: åŸ·è¡Œ Shuffleï¼ˆé—œéµæ­¥é©Ÿï¼ï¼‰
        # å°‡ç›¸åŒ partition key çš„è³‡æ–™åˆ†é…åˆ°ç›¸åŒçš„ Task
        shuffled_df = df.repartition(
            col("partition_column")  # å‡è¨­æŒ‰ partition_column åˆ†å€
        )
        
        # Step 3: æ¯å€‹ Task å¯«å…¥è¼ƒå¤§çš„æª”æ¡ˆ
        # å› ç‚ºç›¸åŒ partition çš„è³‡æ–™å·²é›†ä¸­ï¼Œæ¯å€‹ Task åªå¯«å…¥å°‘æ•¸æª”æ¡ˆ
        shuffled_df.write.format("delta").save(path)
        
    else:
        # æœªå•Ÿç”¨ï¼šæ¯å€‹ executor å¯èƒ½å¯«å…¥å¤šå€‹ partition çš„å°æª”æ¡ˆ
        df.write.format("delta").save(path)
```

**ç¬¦åˆéœ€æ±‚:**

é¡Œç›®å•ï¼šã€ŒWhich statement describes Delta Lake optimized writes?ã€

é¸é … E çš„æè¿°å®Œå…¨ç¬¦åˆï¼š
- âœ… **ã€ŒA shuffle occurs prior to writingã€** - å¯«å…¥å‰åŸ·è¡Œ shuffle
- âœ… **ã€Œgroup similar data togetherã€** - å°‡ç›¸ä¼¼è³‡æ–™åˆ†çµ„
- âœ… **ã€Œresulting in fewer filesã€** - ç”¢ç”Ÿè¼ƒå°‘æª”æ¡ˆ
- âœ… **ã€Œinstead of each executor writing multiple filesã€** - è€Œéæ¯å€‹ executor å¯«å…¥å¤šå€‹æª”æ¡ˆ

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - "OPTIMIZE executed before cluster terminates"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … A æè¿°çš„è¡Œç‚ºï¼ˆä¸å­˜åœ¨ï¼‰
# "Before a Jobs cluster terminates, OPTIMIZE is executed..."
```

**âŒ å•é¡Œåˆ†æï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **ä¸æ˜¯ Optimized Writes** | é€™æè¿°çš„æ˜¯æ‰‹å‹• OPTIMIZE æ“ä½œï¼Œè€Œé Optimized Writes |
| **æ™‚æ©ŸéŒ¯èª¤** | OPTIMIZE ä¸æœƒåœ¨ cluster çµ‚æ­¢å‰è‡ªå‹•åŸ·è¡Œ |
| **éœ€è¦æ˜ç¢ºè¨­å®š** | å³ä½¿æœ‰æ­¤åŠŸèƒ½ï¼Œä¹Ÿéœ€æ˜ç¢ºé…ç½®ï¼Œé Optimized Writes çš„è¡Œç‚º |
| **æ··æ·†æ¦‚å¿µ** | å°‡ OPTIMIZE å‘½ä»¤èˆ‡ Optimized Writes åŠŸèƒ½æ··æ·† |

**OPTIMIZE å‘½ä»¤çš„å¯¦éš›ç”¨æ³•ï¼š**

```python
# OPTIMIZE æ˜¯æ‰‹å‹•åŸ·è¡Œçš„å‘½ä»¤ï¼ˆé Optimized Writesï¼‰
spark.sql("OPTIMIZE delta.`/path/to/table`")

# æˆ–åœ¨ Databricks ä¸­
spark.sql("OPTIMIZE my_table")

# OPTIMIZE çš„åŸ·è¡Œæ™‚æ©Ÿï¼š
# 1. æ‰‹å‹•è§¸ç™¼ï¼ˆé–‹ç™¼è€…æ˜ç¢ºåŸ·è¡Œï¼‰
# 2. æ’ç¨‹ä½œæ¥­ï¼ˆä¾‹å¦‚æ¯æ™šåŸ·è¡Œï¼‰
# 3. Auto Compactionï¼ˆé¸é … Bï¼‰
# âŒ ä¸æœƒåœ¨ cluster çµ‚æ­¢å‰è‡ªå‹•åŸ·è¡Œ
```

---

### é¸é … B - "Asynchronous job runs after write (Auto Compaction)"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … B æè¿°çš„æ˜¯ Auto Compactionï¼Œè€Œé Optimized Writes
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
```

**âŒ é€™æ˜¯ Auto Compactionï¼Œä¸æ˜¯ Optimized Writesï¼**

| ç‰¹æ€§ | Optimized Writes | Auto Compaction (é¸é … B) |
|------|-----------------|-------------------------|
| **åŸ·è¡Œæ™‚æ©Ÿ** | å¯«å…¥å‰ï¼ˆprior to writeï¼‰ | å¯«å…¥å¾Œï¼ˆafter writeï¼‰ |
| **æ“ä½œé¡å‹** | Shuffle + Write | OPTIMIZE å‘½ä»¤ |
| **åŒæ­¥/éåŒæ­¥** | åŒæ­¥ï¼ˆå¯«å…¥çš„ä¸€éƒ¨åˆ†ï¼‰ | éåŒæ­¥ï¼ˆèƒŒæ™¯ä½œæ¥­ï¼‰ |
| **ç›®æ¨™æª”æ¡ˆå¤§å°** | å‹•æ…‹èª¿æ•´ | é è¨­ 1 GB |
| **æ˜¯å¦éœ€è¦é¡å¤–é‹ç®—** | æ˜¯ï¼ˆshuffle æˆæœ¬ï¼‰ | æ˜¯ï¼ˆOPTIMIZE æˆæœ¬ï¼‰ |

**Auto Compaction vs Optimized Writesï¼š**

```python
# === Auto Compactionï¼ˆé¸é … B çš„æè¿°ï¼‰ ===
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

df.write.format("delta").save("/path/to/table")
# â†“ å¯«å…¥å®Œæˆ
# â†“ [éåŒæ­¥] èƒŒæ™¯æª¢æŸ¥æª”æ¡ˆå¤§å°
# â†“ å¦‚æœæœ‰å°æª”æ¡ˆ â†’ åŸ·è¡Œ OPTIMIZE

# === Optimized Writesï¼ˆé¸é … E çš„æè¿°ï¼‰ ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")

df.write.format("delta").save("/path/to/table")
# â†“ [å¯«å…¥å‰] åŸ·è¡Œ Shuffle
# â†“ å¯«å…¥è¼ƒå¤§çš„æª”æ¡ˆ
# â†“ å®Œæˆï¼ˆç„¡éœ€å¾ŒçºŒè™•ç†ï¼‰
```

**è¨˜æ†¶æ³•ï¼š**

```
Optimized Writes = BEFORE å¯«å…¥ï¼ˆShuffleï¼‰
Auto Compaction = AFTER å¯«å…¥ï¼ˆOPTIMIZEï¼‰
```

---

### é¸é … C - "Data queued in messaging bus"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … C æè¿°çš„æ¶æ§‹ï¼ˆä¸å­˜åœ¨ï¼‰
# "Data is queued in a messaging bus..."
```

**âŒ å®Œå…¨éŒ¯èª¤çš„æè¿°ï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **ç„¡ Messaging Bus** | Delta Lake ä¸ä½¿ç”¨ messaging bus æ¶æ§‹ |
| **ç›´æ¥å¯«å…¥** | è³‡æ–™ç›´æ¥å¯«å…¥ Parquet æª”æ¡ˆï¼Œéé€éè¨Šæ¯ä½‡åˆ— |
| **éä¸²æµæ¶æ§‹** | é€™æè¿°çš„åƒæ˜¯ Kafka æˆ–é¡ä¼¼ç³»çµ±ï¼Œé Delta Lake |
| **èˆ‡ä¸»é¡Œç„¡é—œ** | å®Œå…¨èˆ‡ Optimized Writes æ©Ÿåˆ¶ç„¡é—œ |

**Delta Lake çš„å¯¦éš›å¯«å…¥æ©Ÿåˆ¶ï¼š**

```python
# Delta Lake çš„å¯«å…¥æµç¨‹ï¼ˆç„¡ messaging busï¼‰

df.write.format("delta").save("/path/to/table")

# å…§éƒ¨æµç¨‹ï¼š
# 1. è³‡æ–™ç›´æ¥å¯«å…¥ Parquet æª”æ¡ˆåˆ° Cloud Storage
# 2. æ›´æ–° Delta Log (_delta_log/000000.json)
# 3. Transaction log ç¢ºä¿ ACID ç‰¹æ€§
# 4. å®Œæˆ

# âŒ æ²’æœ‰ "messaging bus"
# âŒ æ²’æœ‰ "data queued"
# âŒ ä¸æ˜¯ "batch commit from bus"
```

---

### é¸é … D - "Logical partitions instead of directory partitions"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … D æè¿°çš„åŠŸèƒ½ï¼ˆä¸å­˜åœ¨ï¼‰
# "Optimized writes use logical partitions..."
```

**âŒ æ··æ·†äº†ä¸åŒçš„æ¦‚å¿µï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **Partition ä»æ˜¯ç‰©ç†çš„** | Delta Lake çš„ partition ä»ä»¥ç›®éŒ„çµæ§‹å„²å­˜ |
| **Metadata æ˜¯çµ±è¨ˆè³‡è¨Š** | Metadata åŒ…å«çµ±è¨ˆï¼ˆmin/maxï¼‰ï¼Œä½† partition ä»æ˜¯ç›®éŒ„ |
| **Liquid Clustering** | é€™æè¿°çš„æ›´åƒ Liquid Clusteringï¼Œé Optimized Writes |
| **æ··æ·†æ¦‚å¿µ** | å°‡ partition pruning èˆ‡ Optimized Writes æ··æ·† |

**Delta Lake çš„ Partition æ©Ÿåˆ¶ï¼š**

```python
# Delta Lake çš„å¯¦éš› partition çµæ§‹

# å¯«å…¥ partitioned table
df.write.format("delta").partitionBy("date", "region").save("/tmp/sales")

# ç›®éŒ„çµæ§‹ï¼ˆç‰©ç†åˆ†å€ï¼Œéé‚è¼¯ï¼‰:
/tmp/sales/
  â”œâ”€â”€ date=2026-01-01/
  â”‚   â”œâ”€â”€ region=US/
  â”‚   â”‚   â”œâ”€â”€ part-00000.parquet
  â”‚   â”‚   â””â”€â”€ part-00001.parquet
  â”‚   â””â”€â”€ region=EU/
  â”‚       â””â”€â”€ part-00000.parquet
  â””â”€â”€ date=2026-01-02/
      â””â”€â”€ region=US/
          â””â”€â”€ part-00000.parquet

# âŒ ä¸æ˜¯ "logical partitions in metadata only"
# âœ… ä»æ˜¯ directory-based partitions

# Optimized Writes å½±éŸ¿çš„æ˜¯ï¼š
# - æ¯å€‹ partition å…§çš„æª”æ¡ˆæ•¸é‡ï¼ˆfewer filesï¼‰
# - ä¸æ”¹è®Š partition æœ¬èº«çš„çµæ§‹
```

**Liquid Clusteringï¼ˆä¸åŒæ–¼ Optimized Writesï¼‰ï¼š**

```sql
-- Liquid Clusteringï¼ˆæ›´æ¥è¿‘é¸é … D çš„æè¿°ï¼Œä½†ä»ä¸åŒï¼‰
CREATE TABLE sales_clustered (
    id BIGINT,
    date DATE,
    region STRING
) USING DELTA
CLUSTER BY (date, region);  -- ä½¿ç”¨ clustering è€Œé partitioning

-- Liquid Clustering ç‰¹æ€§ï¼š
-- 1. ä¸å»ºç«‹ç›®éŒ„çµæ§‹ï¼ˆç„¡ date=2026-01-01/ ç›®éŒ„ï¼‰
-- 2. Clustering è³‡è¨Šå„²å­˜åœ¨ metadata
-- 3. ä½†é€™ä¸æ˜¯ Optimized Writesï¼
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£è¨˜æ†¶

**ã€ŒOptimized Writes = å¯«å‰ Shuffleï¼ŒAuto Compaction = å¯«å¾Œ OPTIMIZEã€**

```
Optimized Writes:
  BEFORE write â†’ Shuffle â†’ Fewer files

Auto Compaction:
  AFTER write â†’ Check â†’ OPTIMIZE â†’ Fewer files
```

**ã€ŒShuffle å…ˆåˆ†çµ„ï¼Œæª”æ¡ˆè‡ªç„¶å°‘ã€**
- Shuffle å°‡ç›¸åŒè³‡æ–™åˆ†çµ„
- æ¯å€‹ Task åªå¯«å…¥å°‘æ•¸ partition
- çµæœï¼šfewer, larger files

### æ¦‚å¿µå°æ¯”è¡¨

| ç‰¹æ€§ | Optimized Writes | Auto Compaction | OPTIMIZE å‘½ä»¤ |
|------|-----------------|-----------------|--------------|
| **å•Ÿç”¨æ–¹å¼** | `optimizeWrite.enabled` | `autoCompact.enabled` | æ‰‹å‹•åŸ·è¡Œ SQL |
| **åŸ·è¡Œæ™‚æ©Ÿ** | å¯«å…¥å‰ï¼ˆBeforeï¼‰ | å¯«å…¥å¾Œï¼ˆAfterï¼‰ | æ‰‹å‹•è§¸ç™¼ |
| **æ ¸å¿ƒæ©Ÿåˆ¶** | Shuffle | OPTIMIZE | OPTIMIZE |
| **åŒæ­¥/éåŒæ­¥** | åŒæ­¥ | éåŒæ­¥ | åŒæ­¥ |
| **æˆæœ¬** | Shuffle æˆæœ¬ | OPTIMIZE æˆæœ¬ | OPTIMIZE æˆæœ¬ |
| **é©ç”¨å ´æ™¯** | å¤§é‡å¯«å…¥ | é »ç¹å°å¯«å…¥ | æ‰‹å‹•ç¶­è­· |

### æ±ºç­–æ¨¹

```
éœ€è¦æ”¹å–„æª”æ¡ˆå¤§å°å•é¡Œï¼Ÿ
  â”œâ”€ åœ¨å¯«å…¥æ™‚å„ªåŒ–ï¼Ÿ
  â”‚     â”œâ”€ æ˜¯ â†’ Optimized Writes âœ… (é¸é … E)
  â”‚     â”‚      - å¯«å…¥å‰åŸ·è¡Œ Shuffle
  â”‚     â”‚      - è‡ªå‹•ç”¢ç”Ÿè¼ƒå¤§æª”æ¡ˆ
  â”‚     â”‚
  â”‚     â””â”€ å¦ â†’ å¯«å…¥å¾Œå„ªåŒ–ï¼Ÿ
  â”‚            â”œâ”€ è‡ªå‹• â†’ Auto Compaction (é¸é … B)
  â”‚            â””â”€ æ‰‹å‹• â†’ OPTIMIZE å‘½ä»¤ (é¸é … A çš„è®Šé«”)
  â”‚
  â””â”€ æ”¹è®Š Partition ç­–ç•¥ï¼Ÿ
        â”œâ”€ ä½¿ç”¨ Liquid Clusteringï¼ˆé¸é … D çš„èª¤è§£ï¼‰
        â””â”€ èª¿æ•´ Partition æ•¸é‡
```

### å¯¦å‹™è¨˜æ†¶é»

**1. Optimized Writes çš„ä¸‰å€‹é—œéµç‰¹å¾µï¼š**
```python
# 1. BEFORE writeï¼ˆå¯«å…¥å‰ï¼‰
# 2. SHUFFLEï¼ˆShuffle æ“ä½œï¼‰
# 3. FEWER filesï¼ˆè¼ƒå°‘æª”æ¡ˆï¼‰

# è¨˜æ†¶å£è¨£ï¼šã€ŒBefore Shuffle Fewerã€
```

**2. å¸¸è¦‹æ··æ·†é»ï¼š**
- âŒ Optimized Writes â‰  OPTIMIZE å‘½ä»¤
- âŒ Optimized Writes â‰  Auto Compaction
- âŒ Optimized Writes â‰  Liquid Clustering
- âœ… Optimized Writes = Shuffle before write

**3. é¸æ“‡å»ºè­°ï¼š**

```python
# ä½•æ™‚ä½¿ç”¨ Optimized Writesï¼Ÿ
# âœ… å¤§é‡æ‰¹æ¬¡å¯«å…¥ï¼ˆBatch writesï¼‰
# âœ… æœ‰è¨±å¤š partition çš„è¡¨
# âœ… å¯«å…¥æ•ˆèƒ½å¯æ¥å— shuffle æˆæœ¬

# ä½•æ™‚ä½¿ç”¨ Auto Compactionï¼Ÿ
# âœ… é »ç¹çš„å°é‡å¯«å…¥ï¼ˆStreamingï¼‰
# âœ… å¸Œæœ›å¯«å…¥é€Ÿåº¦æœ€å¿«ï¼ˆå»¶å¾Œå„ªåŒ–ï¼‰
# âœ… æœ‰èƒŒæ™¯é‹ç®—è³‡æº

# ä½•æ™‚ä½¿ç”¨ OPTIMIZE å‘½ä»¤ï¼Ÿ
# âœ… æ‰‹å‹•ç¶­è­·
# âœ… æ’ç¨‹å„ªåŒ–ä½œæ¥­
# âœ… ç²¾ç¢ºæ§åˆ¶åŸ·è¡Œæ™‚æ©Ÿ
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶èˆ‡åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡ä»¶é€£çµ

1. **Optimized Writes:**
   - [Delta Lake Optimized Writes](https://docs.databricks.com/delta/optimizations/optimized-writes.html)
   - èªªæ˜ Optimized Writes çš„å·¥ä½œåŸç†èˆ‡é…ç½®

2. **Auto Compaction:**
   - [Auto Compaction](https://docs.databricks.com/delta/optimizations/auto-compaction.html)
   - è§£é‡‹ Auto Compaction èˆ‡ Optimized Writes çš„å·®ç•°

3. **OPTIMIZE å‘½ä»¤:**
   - [OPTIMIZE Command](https://docs.databricks.com/sql/language-manual/delta-optimize.html)
   - OPTIMIZE å‘½ä»¤çš„å®Œæ•´åƒè€ƒ

4. **Small Files Problem:**
   - [Managing Small Files](https://docs.databricks.com/delta/best-practices.html#compact-files)
   - å°æª”æ¡ˆå•é¡Œçš„æœ€ä½³å¯¦å‹™

### å»¶ä¼¸å­¸ç¿’è³‡æº

- **Liquid Clustering:** æ–°ä¸€ä»£çš„è³‡æ–™çµ„ç¹”ç­–ç•¥
- **Z-Ordering:** èˆ‡ OPTIMIZE æ­é…ä½¿ç”¨çš„æŠ€è¡“
- **File Skipping:** åˆ©ç”¨çµ±è¨ˆè³‡è¨ŠåŠ é€ŸæŸ¥è©¢

---

## ğŸ’¡ è£œå……èªªæ˜

### å¯¦å‹™æ‡‰ç”¨å ´æ™¯

**1. å®Œæ•´çš„ Optimized Writes é…ç½®ï¼š**

```python
# å®Œæ•´é…ç½®ç¯„ä¾‹

# === å•Ÿç”¨ Optimized Writes ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")

# å¯é¸ï¼šèª¿æ•´æª”æ¡ˆå¤§å°ç›®æ¨™ï¼ˆé è¨­ 128 MBï¼‰
spark.conf.set("spark.databricks.delta.optimizeWrite.binSize", "134217728")  # 128 MB

# å¯é¸ï¼šåŒæ™‚å•Ÿç”¨ Auto Compactionï¼ˆé›™é‡ä¿è­·ï¼‰
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

# å¯«å…¥è³‡æ–™
large_df = spark.read.parquet("/source/data")

large_df.write \
    .format("delta") \
    .mode("append") \
    .partitionBy("date", "region") \
    .save("/delta/sales")

# Optimized Writes æœƒè‡ªå‹•ï¼š
# 1. åœ¨å¯«å…¥å‰åŸ·è¡Œ shuffleï¼ˆæŒ‰ date, region åˆ†çµ„ï¼‰
# 2. æ¯å€‹ Task å¯«å…¥è¼ƒå¤§çš„æª”æ¡ˆ
# 3. æ¸›å°‘ small files å•é¡Œ
```

**2. æ¯”è¼ƒä¸‰ç¨®å„ªåŒ–ç­–ç•¥çš„æ•ˆæœï¼š**

```python
# æ¸¬è©¦ä¸åŒå„ªåŒ–ç­–ç•¥çš„æ•ˆæœ

from datetime import datetime
import time

# æº–å‚™æ¸¬è©¦è³‡æ–™
test_df = spark.range(0, 10_000_000).select(
    col("id"),
    (col("id") % 365).cast("int").alias("day"),
    (col("id") % 10).cast("string").alias("category"),
    rand().alias("value")
)

# === æ¸¬è©¦ 1ï¼šç„¡å„ªåŒ– ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "false")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "false")

start = time.time()
test_df.write.format("delta").mode("overwrite").save("/tmp/test_no_opt")
time_no_opt = time.time() - start
files_no_opt = len(dbutils.fs.ls("/tmp/test_no_opt"))

# === æ¸¬è©¦ 2ï¼šOptimized Writes ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "false")

start = time.time()
test_df.write.format("delta").mode("overwrite").save("/tmp/test_opt_write")
time_opt_write = time.time() - start
files_opt_write = len(dbutils.fs.ls("/tmp/test_opt_write"))

# === æ¸¬è©¦ 3ï¼šOptimized Writes + Auto Compaction ===
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

start = time.time()
test_df.write.format("delta").mode("overwrite").save("/tmp/test_both")
time_both = time.time() - start
# ç­‰å¾… Auto Compaction å®Œæˆ
time.sleep(30)
files_both = len(dbutils.fs.ls("/tmp/test_both"))

# === çµæœæ¯”è¼ƒ ===
print(f"""
å„ªåŒ–ç­–ç•¥æ¯”è¼ƒï¼š
{'='*60}
| ç­–ç•¥ | å¯«å…¥æ™‚é–“ | æª”æ¡ˆæ•¸é‡ | å‚™è¨» |
{'='*60}
| ç„¡å„ªåŒ– | {time_no_opt:.1f}s | {files_no_opt} | æœ€å¤šå°æª”æ¡ˆ |
| Optimized Writes | {time_opt_write:.1f}s | {files_opt_write} | å¯«å…¥è¼ƒæ…¢ï¼Œæª”æ¡ˆè¼ƒå°‘ |
| Both | {time_both:.1f}s | {files_both} | æœ€å°‘æª”æ¡ˆ |
{'='*60}
""")

# å…¸å‹çµæœï¼š
# ç„¡å„ªåŒ–: 10s, 500 files
# Optimized Writes: 15s, 50 files (æ…¢ 50%ï¼Œæª”æ¡ˆå°‘ 90%)
# Both: 15s + èƒŒæ™¯, 30 files (æœ€å„ª)
```

**3. é‡å°ä¸åŒå·¥ä½œè² è¼‰çš„å»ºè­°ï¼š**

```python
# === å ´æ™¯ 1ï¼šå¤§é‡æ‰¹æ¬¡å¯«å…¥ï¼ˆETLï¼‰ ===
# æ¨è–¦ï¼šOptimized Writes
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "false")

# ç†ç”±ï¼š
# - æ‰¹æ¬¡å¯«å…¥å¯æ¥å— shuffle æˆæœ¬
# - ä¸€æ¬¡æ€§ç”¢ç”Ÿè¼ƒå¤§æª”æ¡ˆ
# - ç„¡éœ€èƒŒæ™¯ä½œæ¥­

# === å ´æ™¯ 2ï¼šé«˜é »å°é‡å¯«å…¥ï¼ˆStreamingï¼‰ ===
# æ¨è–¦ï¼šAuto Compaction
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "false")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

# ç†ç”±ï¼š
# - å¯«å…¥å»¶é²æœ€å°
# - èƒŒæ™¯è‡ªå‹•å„ªåŒ–
# - ä¸å½±éŸ¿ä¸²æµæ•ˆèƒ½

# === å ´æ™¯ 3ï¼šæ··åˆå·¥ä½œè² è¼‰ ===
# æ¨è–¦ï¼šå…©è€…éƒ½å•Ÿç”¨
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")

# ç†ç”±ï¼š
# - é›™é‡ä¿è­·
# - Optimized Writes è™•ç†å¤§éƒ¨åˆ†æƒ…æ³
# - Auto Compaction è™•ç†å‰©é¤˜å°æª”æ¡ˆ

# === å ´æ™¯ 4ï¼šæˆæœ¬æ•æ„Ÿ ===
# æ¨è–¦ï¼šæ‰‹å‹• OPTIMIZE
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "false")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "false")

# å®šæœŸåŸ·è¡Œ OPTIMIZEï¼ˆä¾‹å¦‚æ¯é€±ï¼‰
spark.sql("OPTIMIZE delta.`/path/to/table`")

# ç†ç”±ï¼š
# - ç²¾ç¢ºæ§åˆ¶å„ªåŒ–æ™‚æ©Ÿ
# - é¿å…è‡ªå‹•èƒŒæ™¯æˆæœ¬
# - é©åˆä½é »å¯«å…¥çš„è¡¨
```

**4. ç›£æ§ Optimized Writes çš„æ•ˆæœï¼š**

```python
# ç›£æ§è…³æœ¬ï¼šæª¢æŸ¥ Optimized Writes æ˜¯å¦æœ‰æ•ˆ

from delta.tables import DeltaTable
from pyspark.sql.functions import *

def analyze_file_sizes(table_path):
    """
    åˆ†æ Delta è¡¨çš„æª”æ¡ˆå¤§å°åˆ†å¸ƒ
    åˆ¤æ–· Optimized Writes æ˜¯å¦æœ‰æ•ˆ
    """
    
    # è®€å– Delta Log
    delta_table = DeltaTable.forPath(spark, table_path)
    
    # å–å¾—æª”æ¡ˆåˆ—è¡¨
    files_df = spark.sql(f"""
        SELECT 
            add.path,
            add.size / 1024 / 1024 as size_mb,
            add.partitionValues,
            add.stats
        FROM 
            delta.`{table_path}` AS t,
            lateral explode(t._metadata.file_metadata) AS add
    """)
    
    # è¨ˆç®—çµ±è¨ˆ
    stats = files_df.select(
        count("*").alias("total_files"),
        avg("size_mb").alias("avg_size_mb"),
        min("size_mb").alias("min_size_mb"),
        max("size_mb").alias("max_size_mb"),
        percentile_approx("size_mb", 0.5).alias("median_size_mb")
    ).collect()[0]
    
    # æª¢æŸ¥å°æª”æ¡ˆæ¯”ä¾‹
    small_files_count = files_df.filter("size_mb < 10").count()
    small_files_ratio = small_files_count / stats["total_files"]
    
    # åˆ¤æ–·
    print(f"""
æª”æ¡ˆå¤§å°åˆ†æï¼š{table_path}
{'='*60}
ç¸½æª”æ¡ˆæ•¸: {stats['total_files']}
å¹³å‡å¤§å°: {stats['avg_size_mb']:.2f} MB
ä¸­ä½æ•¸å¤§å°: {stats['median_size_mb']:.2f} MB
æœ€å°/æœ€å¤§: {stats['min_size_mb']:.2f} / {stats['max_size_mb']:.2f} MB
å°æª”æ¡ˆæ¯”ä¾‹ (<10MB): {small_files_ratio:.1%}
{'='*60}
""")
    
    # å»ºè­°
    if small_files_ratio > 0.5:
        print("âš ï¸ è­¦å‘Šï¼šè¶…é 50% çš„æª”æ¡ˆå°æ–¼ 10MB")
        print("å»ºè­°ï¼šå•Ÿç”¨ Optimized Writes æˆ–åŸ·è¡Œ OPTIMIZE")
    elif stats['avg_size_mb'] > 100:
        print("âœ… è‰¯å¥½ï¼šå¹³å‡æª”æ¡ˆå¤§å°åˆç†ï¼ˆ>100MBï¼‰")
        print("Optimized Writes å¯èƒ½å·²å•Ÿç”¨ä¸”æœ‰æ•ˆ")
    else:
        print("âœ… å¯æ¥å—ï¼šæª”æ¡ˆå¤§å°åœ¨åˆç†ç¯„åœ")

# ä½¿ç”¨ç¯„ä¾‹
analyze_file_sizes("/delta/sales")
```

**5. Optimized Writes çš„æˆæœ¬è€ƒé‡ï¼š**

```python
# è©•ä¼° Optimized Writes çš„æˆæœ¬æ•ˆç›Š

def cost_benefit_analysis(table_path, avg_query_per_day):
    """
    åˆ†æå•Ÿç”¨ Optimized Writes çš„æˆæœ¬æ•ˆç›Š
    
    Args:
        table_path: Delta è¡¨è·¯å¾‘
        avg_query_per_day: å¹³å‡æ¯æ—¥æŸ¥è©¢æ¬¡æ•¸
    """
    
    # å‡è¨­æ•¸æ“š
    shuffle_cost_per_gb = 0.10  # Shuffle æˆæœ¬ï¼ˆUSD/GBï¼‰
    scan_cost_per_gb = 0.05     # æƒææˆæœ¬ï¼ˆUSD/GBï¼‰
    data_size_gb = 1000         # è³‡æ–™å¤§å°ï¼ˆGBï¼‰
    
    # === æˆæœ¬ 1ï¼šShuffle æˆæœ¬ï¼ˆä¸€æ¬¡æ€§ï¼‰ ===
    shuffle_cost = data_size_gb * shuffle_cost_per_gb
    
    # === æˆæœ¬ 2ï¼šå¾ŒçºŒæŸ¥è©¢ç¯€çœï¼ˆæ¯æ—¥ï¼‰ ===
    # å‡è¨­ Optimized Writes æ¸›å°‘ 50% çš„æª”æ¡ˆæƒæ
    files_reduction = 0.5
    daily_scan_saving = data_size_gb * scan_cost_per_gb * files_reduction * avg_query_per_day
    
    # === æŠ•è³‡å›æ”¶æœŸ ===
    breakeven_days = shuffle_cost / daily_scan_saving
    
    print(f"""
Optimized Writes æˆæœ¬æ•ˆç›Šåˆ†æï¼š
{'='*60}
ä¸€æ¬¡æ€§æˆæœ¬ï¼ˆShuffleï¼‰: ${shuffle_cost:.2f}
æ¯æ—¥ç¯€çœï¼ˆæŸ¥è©¢æ•ˆèƒ½ï¼‰: ${daily_scan_saving:.2f}
æŠ•è³‡å›æ”¶æœŸ: {breakeven_days:.1f} å¤©
{'='*60}

çµè«–ï¼š
""")
    
    if breakeven_days < 7:
        print("âœ… å¼·çƒˆæ¨è–¦ï¼šä¸åˆ°ä¸€é€±å³å¯å›æœ¬")
    elif breakeven_days < 30:
        print("âœ… æ¨è–¦ï¼šä¸€å€‹æœˆå…§å¯å›æœ¬")
    else:
        print("âš ï¸ è¬¹æ…è©•ä¼°ï¼šå›æœ¬æœŸè¼ƒé•·")

# ä½¿ç”¨ç¯„ä¾‹
cost_benefit_analysis("/delta/sales", avg_query_per_day=100)
```

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

**çœ‹åˆ°ã€ŒOptimized Writesã€é¡Œç›®ï¼Œåˆ¤æ–·é—œéµå­—ï¼š**

| é—œéµå­— | å°æ‡‰é¸é … | åˆ¤æ–· |
|--------|---------|------|
| **"shuffle prior to write"** | E | âœ… æ­£ç¢º |
| **"before writing"** | E | âœ… æ­£ç¢º |
| **"group similar data"** | E | âœ… æ­£ç¢º |
| **"asynchronous"** | B | âŒ Auto Compaction |
| **"after write"** | B | âŒ Auto Compaction |
| **"cluster terminates"** | A | âŒ ä¸å­˜åœ¨çš„åŠŸèƒ½ |
| **"messaging bus"** | C | âŒ å®Œå…¨éŒ¯èª¤ |
| **"logical partitions"** | D | âŒ æ··æ·†æ¦‚å¿µ |

### æ¦‚å¿µå€åˆ†æŠ€å·§

**å¿«é€Ÿå€åˆ†ä¸‰ç¨®å„ªåŒ–æ–¹å¼ï¼š**

```
æ™‚é–“è»¸è¦–è§’ï¼š

BEFORE write:
  â”œâ”€ Optimized Writes â† é¸é … E âœ…
  â”‚    â””â”€ Shuffle â†’ Fewer files
  â”‚
[WRITE]
  â”‚
AFTER write:
  â”œâ”€ Auto Compaction â† é¸é … B âŒ
  â”‚    â””â”€ Background OPTIMIZE
  â”‚
MANUAL:
  â””â”€ OPTIMIZE command â† é¸é … A çš„èª¤è§£ âŒ
       â””â”€ Manual execution
```

### è¨˜æ†¶æŠ€å·§

**ã€Œå‰å¾Œé †åºè¨˜æ†¶æ³•ã€ï¼š**

1. **Optimized Writes** - å¯«ã€Œå‰ã€Shuffle
2. **Auto Compaction** - å¯«ã€Œå¾Œã€OPTIMIZE
3. **OPTIMIZE å‘½ä»¤** - ã€Œæ‰‹å‹•ã€åŸ·è¡Œ

**ã€ŒåŠŸèƒ½åç¨±è¨˜æ†¶æ³•ã€ï¼š**

- **Optimized WRITES** - å¯«å…¥æ™‚å„ªåŒ–
- **Auto COMPACTION** - è‡ªå‹•å£“ç¸®
- **OPTIMIZE** - å„ªåŒ–å‘½ä»¤

---

**[è¿”å›é¡Œç›®](#question-107)**
