# Question #29

---

## 題目資訊

### 題目編號
**ID:** `Q-01-029`

### 來源
**來源:** Real Exam Recall

### 難度等級
**難度:** `L2-Intermediate`

---

## 題目內容

### 題幹

A new data engineer notices that a critical field was omitted from an application that writes its Kafka source to Delta Lake.

This happened even though the critical field was in the Kafka source.

That field was further missing from data written to dependent, long-term storage.

The retention threshold on the Kafka service is seven days. The pipeline has been in production for three months.

Which describes how Delta Lake can help to avoid data loss of this nature in the future?

### 選項

- **A.** The Delta log and Structured Streaming checkpoints record the full history of the Kafka producer.
- **B.** Delta Lake schema evolution can retroactively calculate the correct value for newly added fields, as long as the data was in the original source.
- **C.** Delta Lake automatically checks that all fields present in the source data are included in the ingestion layer.
- **D.** Data can never be permanently dropped or deleted from Delta Lake, so data loss is not possible under any circumstance.
- **E.** Ingesting all raw data and metadata from Kafka to a bronze Delta table creates a permanent, replayable history of the data state.

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Data-Loss-Prevention`, `Medallion-Architecture`, `Kafka`, `Delta-Lake`, `Bronze-Layer`

### Trap Tags (陷阱類型標籤)
**Traps:** `Architecture-Pattern`, `Data-Recovery`

### Knowledge Domain (知識領域)
**Domain:** `Data Engineering`

---

## 答案與解析連結

### 正確答案
**正解:** `E`

### 解析檔案
**詳細解析:** [點此查看解析](../analysis/Q-01-029-analysis.md)

---

## 相關資源

### 官方文件
- [Medallion Architecture](https://docs.databricks.com/lakehouse/medallion.html)
- [Kafka Integration](https://docs.databricks.com/structured-streaming/kafka.html)
- [Bronze Layer Best Practices](https://www.databricks.com/glossary/medallion-architecture)
