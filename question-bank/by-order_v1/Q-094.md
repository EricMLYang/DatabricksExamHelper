# Q-094

## é¡Œç›®è³‡è¨Š

**ID:** `Q-094`

**ä¾†æº:** Mock Exam

**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

Spill occurs as a result of executing various wide transformations. However, diagnosing spill requires one to proactively look for key indicators.

Where in the Spark UI are two of the primary indicators that a partition is spilling to disk?

### é¸é …

- **A.** Query's detail screen and Job's detail screen
- **B.** Stage's detail screen and Executor's log files
- **C.** Driver's and Executor's log files
- **D.** Executor's detail screen and Executor's log files
- **E.** Stage's detail screen and Query's detail screen

---

## æ¨™ç±¤ç³»çµ±

**Topics:** `Spark-Performance`, `Monitoring`, `Troubleshooting`

**Traps:** `UI-Component-Confusion`

**Domain:** `æˆæœ¬èˆ‡æ•ˆèƒ½å„ªåŒ– (Cost & Performance Optimization)`

---

## ç­”æ¡ˆèˆ‡åˆ†æ

### æ­£ç¢ºç­”æ¡ˆ

**æ­£è§£:** `B`

**ç¤¾ç¾¤æŠ•ç¥¨:** B (90%)
**ä¾†æºæ¨™è¨»:** B

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Spark Spill è¨ºæ–·èˆ‡ Spark UI ç›£æ§
**çŸ¥è­˜é ˜åŸŸ:** æˆæœ¬èˆ‡æ•ˆèƒ½å„ªåŒ– - æ•ˆèƒ½è¨ºæ–·èˆ‡èª¿æ ¡
**é—œéµæ¦‚å¿µ:** 
- Spill to disk çš„æ¦‚å¿µèˆ‡å½±éŸ¿
- Stage detail screen çš„æŒ‡æ¨™
- Executor log files çš„è³‡è¨Š

### æ¬¡è¦è€ƒé»
- Wide transformations èˆ‡ shuffle
- Memory management in Spark
- Spark UI å„å€‹é¢æ¿çš„ç”¨é€”

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ B æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æŠ€è¡“åŸç†:**

**Spill çš„å®šç¾©èˆ‡ç™¼ç”Ÿæ™‚æ©Ÿï¼š**

```
Spill (æº¢å‡ºåˆ°ç£ç¢Ÿ) ç™¼ç”Ÿåœ¨ï¼š
1. Wide Transformations (éœ€è¦ shuffle)
   - groupBy
   - join
   - reduceByKey
   - repartition

2. è¨˜æ†¶é«”ä¸è¶³æ™‚
   - Executor memory ç„¡æ³•å®¹ç´ä¸­é–“è³‡æ–™
   - Shuffle data è¶…é buffer å¤§å°
   - Aggregation state è¶…éè¨˜æ†¶é«”é™åˆ¶

Spill çš„å½±éŸ¿ï¼š
- æ•ˆèƒ½é¡¯è‘—ä¸‹é™ï¼ˆç£ç¢Ÿ I/O æ¯”è¨˜æ†¶é«”æ…¢ 100-1000 å€ï¼‰
- Task åŸ·è¡Œæ™‚é–“å¢åŠ 
- å¯èƒ½å°è‡´ OOM (Out of Memory)
```

**è¨ºæ–· Spill çš„å…©å€‹ä¸»è¦ä½ç½®ï¼š**

### ä½ç½® 1: Stage's Detail Screen âœ…

**åœ¨ Spark UI ä¸­çš„è·¯å¾‘ï¼š**
```
Spark UI â†’ Jobs â†’ [Job ID] â†’ [Stage ID] â†’ Stage Details

é—œéµæŒ‡æ¨™ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage Detail - Summary Metrics              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shuffle Read:                               â”‚
â”‚   - Shuffle Read Size                       â”‚
â”‚   - Shuffle Spill (Memory) â† é‡è¦ï¼         â”‚
â”‚   - Shuffle Spill (Disk) â† é‡è¦ï¼           â”‚
â”‚                                             â”‚
â”‚ Shuffle Write:                              â”‚
â”‚   - Shuffle Write Size                      â”‚
â”‚   - Shuffle Spill (Memory) â† é‡è¦ï¼         â”‚
â”‚   - Shuffle Spill (Disk) â† é‡è¦ï¼           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Stage Detail Screen é¡¯ç¤ºçš„ Spill æŒ‡æ¨™ï¼š**

```
Task Metrics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric              â”‚ Min â”‚ Median â”‚ Max    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Duration            â”‚ 1s  â”‚ 2s     â”‚ 5s     â”‚
â”‚ GC Time             â”‚ 100msâ”‚ 200ms â”‚ 500ms  â”‚
â”‚ Shuffle Read Size   â”‚ 10MB â”‚ 50MB  â”‚ 100MB  â”‚
â”‚ Shuffle Spill (Mem) â”‚ 0MB  â”‚ 100MB â”‚ 500MB  â”‚ â† é€™è£¡ï¼
â”‚ Shuffle Spill (Disk)â”‚ 0MB  â”‚ 50MB  â”‚ 200MB  â”‚ â† é€™è£¡ï¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è§£è®€ï¼š
- Shuffle Spill (Memory) > 0: è¡¨ç¤ºè¨˜æ†¶é«”å·²æ»¿ï¼Œéœ€è¦æº¢å‡º
- Shuffle Spill (Disk) > 0: è¡¨ç¤ºå·²ç¶“å¯«å…¥ç£ç¢Ÿï¼ˆæ•ˆèƒ½å•é¡Œï¼ï¼‰
- Max å€¼å¾ˆå¤§: è¡¨ç¤ºæŸäº› partition æœ‰åš´é‡çš„ spill
```

### ä½ç½® 2: Executor's Log Files âœ…

**åœ¨ Spark UI ä¸­çš„è·¯å¾‘ï¼š**
```
Spark UI â†’ Executors â†’ [Executor ID] â†’ View Logs

æˆ–åœ¨ Databricks:
Cluster â†’ Driver Logs / Executor Logs
```

**Executor Log ä¸­çš„ Spill è¨Šæ¯ï¼š**

```log
# å…¸å‹çš„ Spill è­¦å‘Šè¨Šæ¯

INFO ExternalSorter: Thread 123 spilling in-memory map of 128.0 MB to disk (1 time so far)

WARN ShuffleExternalSorter: Spilling data because number of spilledFiles reached 50

INFO UnsafeExternalSorter: Thread 456 spilling sort data of 256.0 MB to disk (2 times so far)

WARN MemoryStore: Not enough space to cache rdd_10_2 in memory! (computed 512.0 MB so far)

INFO ExternalAppendOnlyMap: Spilling in-memory map of 1024.0 MB to disk
```

**Log è¨Šæ¯çš„é—œéµè³‡è¨Šï¼š**

```
åˆ†æ Log æ‰¾å‡ºï¼š
1. å“ªå€‹ Thread/Task ç™¼ç”Ÿ spill
2. Spill çš„è³‡æ–™é‡ï¼ˆMB/GBï¼‰
3. Spill çš„æ¬¡æ•¸ï¼ˆtimes so farï¼‰
4. Spill çš„åŸå› ï¼ˆmemory full, too many filesï¼‰
```

**ç¬¦åˆéœ€æ±‚:**

```python
# å°è‡´ Spill çš„ç¯„ä¾‹ç¨‹å¼ç¢¼

from pyspark.sql.functions import col, sum

# å¤§å‹è³‡æ–™é›†çš„ groupBy aggregation
large_df = spark.read.parquet("/data/large_dataset")  # 10GB

# Wide transformation: groupBy + aggregation
# å¦‚æœ executor memory ä¸è¶³ï¼Œæœƒç™¼ç”Ÿ spill
result = large_df.groupBy("category").agg(
    sum("amount").alias("total_amount"),
    count("*").alias("count")
)

result.write.parquet("/output/aggregated")

# åœ¨åŸ·è¡Œéç¨‹ä¸­ï¼š
# 1. æŸ¥çœ‹ Stage Detail Screen:
#    - Shuffle Spill (Memory): 2.5 GB
#    - Shuffle Spill (Disk): 800 MB  â† æ•ˆèƒ½å•é¡Œï¼
#
# 2. æŸ¥çœ‹ Executor Logs:
#    - "Thread 123 spilling in-memory map of 500 MB to disk (3 times so far)"
```

**å¯¦å‹™æ‡‰ç”¨:**

**è¨ºæ–· Spill çš„å®Œæ•´æµç¨‹ï¼š**

```
Step 1: ç™¼ç¾æ•ˆèƒ½å•é¡Œ
- Job åŸ·è¡Œæ™‚é–“ç•°å¸¸é•·
- Task å¤±æ•—æˆ– OOM

Step 2: æŸ¥çœ‹ Spark UI - Stage Detail
- é€²å…¥ Stages é é¢
- æ‰¾åˆ°åŸ·è¡Œæ™‚é–“é•·çš„ Stage
- æŸ¥çœ‹ Summary Metrics:
  âœ“ Shuffle Spill (Memory) > 0? â†’ è¨˜æ†¶é«”å£“åŠ›
  âœ“ Shuffle Spill (Disk) > 0? â†’ å·²ç¶“ spillï¼Œæ•ˆèƒ½å•é¡Œ
  âœ“ Max å€¼é å¤§æ–¼ Median? â†’ è³‡æ–™å‚¾æ–œ

Step 3: æŸ¥çœ‹ Executor Logs
- æœå°‹é—œéµå­—: "spilling", "spill"
- ç¢ºèª spill çš„è©³ç´°è³‡è¨Š
- æ‰¾å‡ºå“ªäº› RDD/operation å°è‡´ spill

Step 4: æ¡å–å„ªåŒ–æªæ–½
- å¢åŠ  executor memory
- èª¿æ•´ spark.sql.shuffle.partitions
- ä½¿ç”¨ broadcast joinï¼ˆå¦‚æœé©ç”¨ï¼‰
- å„ªåŒ–è³‡æ–™åˆ†å‰²ç­–ç•¥
```

**Spill å„ªåŒ–ç¯„ä¾‹ï¼š**

```python
# å•é¡Œï¼šå¤§å‹ join å°è‡´ spill

# åŸå§‹ç¨‹å¼ç¢¼ï¼ˆæœƒ spillï¼‰
large_df1 = spark.read.parquet("/data/orders")  # 50GB
large_df2 = spark.read.parquet("/data/customers")  # 10GB

result = large_df1.join(large_df2, "customer_id")
# â†’ Stage Detail é¡¯ç¤º: Shuffle Spill (Disk) = 5GB

# å„ªåŒ– 1: å¢åŠ  shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", "400")  # é è¨­ 200
result = large_df1.join(large_df2, "customer_id")
# â†’ Spill æ¸›å°‘åˆ° 2GB

# å„ªåŒ– 2: å¦‚æœä¸€å€‹è¡¨è¼ƒå°ï¼Œä½¿ç”¨ broadcast join
from pyspark.sql.functions import broadcast

result = large_df1.join(broadcast(large_df2), "customer_id")
# â†’ æ²’æœ‰ shuffleï¼Œæ²’æœ‰ spillï¼

# å„ªåŒ– 3: å¢åŠ  executor memory
# Cluster configuration:
# spark.executor.memory: 8g â†’ 16g
# spark.executor.memoryOverhead: 2g â†’ 4g
# â†’ Spill å¤§å¹…æ¸›å°‘æˆ–æ¶ˆå¤±
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - Query's detail screen and Job's detail screen

**éŒ¯èª¤åŸå› :** é€™äº›é é¢**ä¸é¡¯ç¤ºè©³ç´°çš„ spill æŒ‡æ¨™**

**è©³ç´°åˆ†æ:**

**Query Detail Screen:**
- åœ¨ Databricks SQL æˆ– Spark SQL ä¸­ä½¿ç”¨
- é¡¯ç¤ºæŸ¥è©¢è¨ˆç•«ï¼ˆPhysical Planï¼‰
- é¡¯ç¤ºæŸ¥è©¢åŸ·è¡Œæ™‚é–“
- **ä¸é¡¯ç¤º spill çš„å…·é«”æ•¸å€¼**

**Job Detail Screen:**
- é¡¯ç¤º Job åŒ…å«çš„ Stages
- é¡¯ç¤ºæ•´é«”åŸ·è¡Œæ™‚é–“
- é¡¯ç¤ºæˆåŠŸ/å¤±æ•—çš„ tasks æ•¸é‡
- **ä¸é¡¯ç¤ºè©³ç´°çš„ shuffle spill æŒ‡æ¨™**

**å°æ¯”æ­£ç¢ºä½ç½®ï¼š**

```
Job Detail Screenï¼ˆé«˜å±¤æ¬¡ï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Job 12                      â”‚
â”‚ Description: parquet at ... â”‚
â”‚ Duration: 5 min             â”‚
â”‚ Stages: 3                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“ ç¼ºå°‘è©³ç´°çš„ spill è³‡è¨Š

Stage Detail Screenï¼ˆè©³ç´°ï¼‰:âœ…
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Stage 5                     â”‚
â”‚ Shuffle Spill (Memory): 2GB â”‚ â† æœ‰ spill æŒ‡æ¨™ï¼
â”‚ Shuffle Spill (Disk): 500MBâ”‚ â† æœ‰ spill æŒ‡æ¨™ï¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ˜“æ··æ·†é»:**
å¯èƒ½èª¤ä»¥ç‚º Job æˆ– Query å±¤ç´šæœƒé¡¯ç¤º spillï¼Œä½†å¯¦éš›ä¸Šéœ€è¦æ·±å…¥ Stage å±¤ç´š

---

### é¸é … C - Driver's and Executor's log files

**éŒ¯èª¤åŸå› :** **Driver log** é€šå¸¸ä¸åŒ…å« spill è³‡è¨Šï¼Œspill ç™¼ç”Ÿåœ¨ **Executor** ä¸Š

**è©³ç´°åˆ†æ:**

**Driver vs Executor çš„è§’è‰²ï¼š**

```
Driver:
- å”èª¿æ•´é«”åŸ·è¡Œ
- è§£ææŸ¥è©¢è¨ˆç•«
- åˆ†é… tasks çµ¦ executors
- æ”¶é›†çµæœ
â†’ ä¸åŸ·è¡Œå¯¦éš›çš„è³‡æ–™è™•ç†
â†’ ä¸æœƒç™¼ç”Ÿ spill

Executor:
- åŸ·è¡Œ tasks
- è™•ç†è³‡æ–™ï¼ˆshuffle, aggregationï¼‰
- ç®¡ç† memory å’Œ disk
â†’ è³‡æ–™è™•ç†ç™¼ç”Ÿåœ¨é€™è£¡
â†’ Spill ç™¼ç”Ÿåœ¨é€™è£¡ âœ“
```

**Driver Log çš„å…§å®¹ï¼š**

```log
# Driver Log ç¯„ä¾‹ï¼ˆä¸åŒ…å« spill ç´°ç¯€ï¼‰

INFO SparkContext: Starting job: parquet at <console>:1
INFO DAGScheduler: Got job 0 with 200 output partitions
INFO DAGScheduler: Final stage: ResultStage 3
INFO DAGScheduler: Submitting 200 tasks
INFO TaskSchedulerImpl: Adding task set 3.0 with 200 tasks
```

**Executor Log çš„å…§å®¹ï¼š**

```log
# Executor Log ç¯„ä¾‹ï¼ˆåŒ…å« spill è³‡è¨Šï¼‰âœ…

INFO ExternalSorter: Thread 123 spilling in-memory map of 500 MB to disk
WARN ShuffleExternalSorter: Spilling data because number of spilledFiles reached 50
INFO UnsafeExternalSorter: Thread 456 spilling sort data of 256 MB to disk (2 times so far)
```

**ç‚ºä½•é¸é … C éŒ¯èª¤ï¼š**
- Driver log ä¸åŒ…å« spill è³‡è¨Š
- åªæœ‰ Executor log æœ‰ç”¨
- é¸é … B æ›´å®Œæ•´ï¼ˆStage detail + Executor logï¼‰

---

### é¸é … D - Executor's detail screen and Executor's log files

**éŒ¯èª¤åŸå› :** **Executor detail screen** é¡¯ç¤ºæ•´é«”æŒ‡æ¨™ï¼Œä½†**ä¸å¦‚ Stage detail æ¸…æ¥šé¡¯ç¤º spill**

**è©³ç´°åˆ†æ:**

**Executor Detail Screen çš„å…§å®¹ï¼š**

```
Spark UI â†’ Executors

é¡¯ç¤ºçš„è³‡è¨Šï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Executor ID | Address | Status | RDD Blocksâ”‚
â”‚ 1           | 10.0.1.5| Active | 20        â”‚
â”‚ 2           | 10.0.1.6| Active | 18        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Summary:
- Storage Memory Used: 2.5 GB / 8.0 GB
- Disk Used: 500 MB
- Tasks: 200 (200 succeeded, 0 failed)
- Input: 10 GB
- Shuffle Read: 5 GB
- Shuffle Write: 3 GB
```

**Executor Detail vs Stage Detailï¼š**

| ç‰¹æ€§ | Executor Detail | Stage Detail |
|------|----------------|--------------|
| é¡¯ç¤º Shuffle Spill? | âŒ ä¸æ˜ç¢º | âœ… æ˜ç¢ºé¡¯ç¤º |
| é¡¯ç¤º Memory Spill? | âŒ ä¸æ˜ç¢º | âœ… æ˜ç¢ºé¡¯ç¤º |
| é¡¯ç¤º Disk Spill? | âŒ ä¸æ˜ç¢º | âœ… æ˜ç¢ºé¡¯ç¤º |
| ç²’åº¦ | Executor å±¤ç´š | Task/Stage å±¤ç´š |

**ç‚ºä½• Stage Detail æ›´å¥½ï¼š**

```
Stage Detail Screen:
- æ˜ç¢ºé¡¯ç¤º "Shuffle Spill (Memory)" æŒ‡æ¨™
- æ˜ç¢ºé¡¯ç¤º "Shuffle Spill (Disk)" æŒ‡æ¨™
- é¡¯ç¤º Min/Median/Max åˆ†å¸ƒ
- å¯ä»¥è­˜åˆ¥è³‡æ–™å‚¾æ–œ

Executor Detail Screen:
- åªé¡¯ç¤ºç¸½é«”çš„ Disk Used
- æ²’æœ‰æ˜ç¢ºçš„ "Spill" æ¨™ç±¤
- ç„¡æ³•å€åˆ†æ­£å¸¸çš„ disk ä½¿ç”¨èˆ‡ spill
```

**æ˜“æ··æ·†é»:**
Executor detail ç¢ºå¯¦é¡¯ç¤ºä¸€äº›ç›¸é—œè³‡è¨Šï¼Œä½†ä¸å¦‚ Stage detail æ˜ç¢º

---

### é¸é … E - Stage's detail screen and Query's detail screen

**éŒ¯èª¤åŸå› :** å‰åŠæ­£ç¢ºï¼Œä½† **Query detail screen** ä¸é¡¯ç¤º spill çš„å…·é«”æ•¸å€¼

**è©³ç´°åˆ†æ:**

**Stage Detail Screen:** âœ… æ­£ç¢ºï¼ˆå¦‚å‰è¿°ï¼‰

**Query Detail Screen:** âŒ ä¸é©åˆè¨ºæ–· spill

```
Query Detail çš„ç”¨é€”ï¼š
- æŸ¥çœ‹æŸ¥è©¢åŸ·è¡Œè¨ˆç•«ï¼ˆLogical/Physical Planï¼‰
- æŸ¥çœ‹æŸ¥è©¢åŸ·è¡Œæ™‚é–“
- æŸ¥çœ‹æ¶‰åŠçš„è¡¨å’Œè³‡æ–™é‡
- ä¸é¡¯ç¤ºè©³ç´°çš„ shuffle spill æŒ‡æ¨™

ç¯„ä¾‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query Detail                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SQL: SELECT * FROM orders          â”‚
â”‚ Duration: 2 min                    â”‚
â”‚ Scan: 10 GB                        â”‚
â”‚ Physical Plan:                     â”‚
â”‚   - HashAggregate                  â”‚
â”‚   - Exchange                       â”‚
â”‚   - FileScan parquet               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ æ²’æœ‰ spill çš„å…·é«”æ•¸å€¼
```

**ç‚ºä½• Executor Log æ›´å¥½ï¼š**

```
Executor Log:
âœ… æ˜ç¢ºè¨˜éŒ„ spill äº‹ä»¶
âœ… åŒ…å« spill çš„è³‡æ–™é‡
âœ… è¨˜éŒ„ spill çš„æ¬¡æ•¸
âœ… å¯è¿½è¹¤åˆ°å…·é«”çš„ Thread/Task

Query Detail:
âŒ åªé¡¯ç¤ºé«˜å±¤æ¬¡è³‡è¨Š
âŒ æ²’æœ‰ spill ç´°ç¯€
âŒ ç„¡æ³•è¿½è¹¤å…·é«”çš„ spill äº‹ä»¶
```

**é¸é … B vs E çš„å°æ¯”ï¼š**

| ä½ç½® | é¸é … B | é¸é … E |
|------|--------|--------|
| ç¬¬ä¸€å€‹ | Stage Detail âœ… | Stage Detail âœ… |
| ç¬¬äºŒå€‹ | Executor Log âœ… | Query Detail âŒ |
| ç¶œåˆè©•åƒ¹ | æœ€ä½³çµ„åˆ | ä¸å®Œæ•´ |

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### å£è¨£
**ã€ŒStage çœ‹æ•¸å­—ï¼ŒLog çœ‹ç´°ç¯€ã€**

### Spill è¨ºæ–·ä½ç½®å°æ¯”è¡¨

| ä½ç½® | æ˜¯å¦é¡¯ç¤º Spill | è©³ç´°ç¨‹åº¦ | ç”¨é€” |
|------|--------------|---------|------|
| **Stage Detail** | âœ… æ˜¯ | é«˜ï¼ˆæ•¸å€¼æŒ‡æ¨™ï¼‰| é‡åŒ– spill ç¨‹åº¦ |
| **Executor Log** | âœ… æ˜¯ | é«˜ï¼ˆäº‹ä»¶ç´°ç¯€ï¼‰| è¿½è¹¤ spill äº‹ä»¶ |
| Job Detail | âŒ å¦ | ä½ | æ•´é«”æ¦‚è¦½ |
| Query Detail | âŒ å¦ | ä½ | æŸ¥è©¢è¨ˆç•« |
| Executor Detail | âš ï¸ ä¸æ˜ç¢º | ä¸­ | Executor ç‹€æ…‹ |
| Driver Log | âŒ å¦ | ä½ | å”èª¿è³‡è¨Š |

### Spill è¨ºæ–·æµç¨‹è¦–è¦ºåŒ–

```
ç™¼ç¾æ•ˆèƒ½å•é¡Œ
    â†“
æ­¥é©Ÿ 1: Spark UI â†’ Stages
    â†“
æ‰¾åˆ°æ…¢çš„ Stage â†’ é»æ“Šé€²å…¥ Stage Detail
    â†“
æŸ¥çœ‹ Summary Metrics:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shuffle Spill (Memory): 2GB â”‚ â† æŒ‡æ¨™ 1
â”‚ Shuffle Spill (Disk): 500MBâ”‚ â† æŒ‡æ¨™ 2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ ç¢ºèªæœ‰ spill
    â†“
æ­¥é©Ÿ 2: Executors â†’ View Logs
    â†“
æœå°‹ "spilling"
    â†“
æ‰¾åˆ°è©³ç´°çš„ spill äº‹ä»¶:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ INFO ExternalSorter: Thread 123     â”‚
â”‚ spilling in-memory map of 500 MB   â”‚
â”‚ to disk (3 times so far)            â”‚ â† ç´°ç¯€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
çµåˆå…©è€…è³‡è¨Š â†’ å®šä½å•é¡Œ â†’ å„ªåŒ–

çµè«–ï¼šé¸é … B æ­£ç¢º âœ…
```

### åˆ¤æ–·æŠ€å·§

**è­˜åˆ¥é—œéµå­—ï¼š**
- âœ… "Stage's detail screen" â†’ é¡¯ç¤ºé‡åŒ–æŒ‡æ¨™
- âœ… "Executor's log files" â†’ é¡¯ç¤ºäº‹ä»¶ç´°ç¯€
- âŒ "Query's detail" â†’ ä¸é¡¯ç¤º spill
- âŒ "Job's detail" â†’ ä¸é¡¯ç¤ºè©³ç´° spill
- âŒ "Driver's log" â†’ ä¸åŒ…å« executor çš„ spill

**å¿«é€Ÿæ’é™¤æ³•ï¼š**

```
é¡Œç›®å•ã€Œè¨ºæ–· spill çš„ä½ç½®ã€

éœ€è¦ä»€éº¼è³‡è¨Šï¼Ÿ
1. é‡åŒ–æŒ‡æ¨™ï¼ˆå¤šå°‘ MB spillï¼‰
2. äº‹ä»¶ç´°ç¯€ï¼ˆä½•æ™‚ã€å“ªå€‹ taskï¼‰

å“ªäº›ä½ç½®æä¾›ï¼Ÿ
- Stage Detail: âœ… é‡åŒ–æŒ‡æ¨™
- Executor Log: âœ… äº‹ä»¶ç´°ç¯€
- Query Detail: âŒ æ²’æœ‰ spill æŒ‡æ¨™
- Job Detail: âŒ å¤ªé«˜å±¤æ¬¡
- Driver Log: âŒ ä¸åœ¨ executor ä¸Š

â†’ é¸é … B: Stage + Executor Log âœ…
```

### Spark UI å°èˆªè¨˜æ†¶åœ–

```
Spark UI çµæ§‹ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark UI                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Jobs                            â”‚ â† æ•´é«”æ¦‚è¦½
â”‚   â””â”€ Job Detail                 â”‚ â† ä¸é¡¯ç¤º spill
â”‚       â””â”€ Stages                 â”‚
â”‚           â””â”€ Stage Detail       â”‚ â† âœ… Spill æŒ‡æ¨™åœ¨é€™è£¡ï¼
â”‚               â””â”€ Tasks          â”‚
â”‚                                 â”‚
â”‚ Stages                          â”‚ â† ç›´æ¥é€²å…¥ stages
â”‚   â””â”€ Stage Detail               â”‚ â† âœ… Spill æŒ‡æ¨™åœ¨é€™è£¡ï¼
â”‚                                 â”‚
â”‚ Storage                         â”‚ â† RDD å¿«å–è³‡è¨Š
â”‚                                 â”‚
â”‚ Environment                     â”‚ â† é…ç½®è³‡è¨Š
â”‚                                 â”‚
â”‚ Executors                       â”‚ â† Executor ç‹€æ…‹
â”‚   â””â”€ Executor Detail            â”‚ â† éƒ¨åˆ†ç›¸é—œè³‡è¨Š
â”‚   â””â”€ Thread Dump                â”‚
â”‚   â””â”€ Logs (stdout/stderr)       â”‚ â† âœ… Spill ç´°ç¯€åœ¨é€™è£¡ï¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¨ºæ–· Spill è·¯å¾‘ï¼š
1. Stages â†’ Stage Detail â†’ çœ‹ Shuffle Spill æŒ‡æ¨™
2. Executors â†’ Logs â†’ æœå°‹ "spilling"
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

### å®˜æ–¹æ–‡ä»¶
- [Spark Monitoring - Web UI](https://spark.apache.org/docs/latest/monitoring.html)
- [Spark Performance Tuning - Spill](https://spark.apache.org/docs/latest/tuning.html#memory-management)
- [Databricks - Monitor Spark jobs](https://docs.databricks.com/spark/latest/spark-sql/spark-ui.html)

### ç›¸é—œæ¦‚å¿µ
- Memory Management in Spark
- Shuffle Operations
- Wide vs Narrow Transformations
- Data Skew Detection

### å¯¦å‹™ç¯„ä¾‹

**1. å®Œæ•´çš„ Spill è¨ºæ–·æµç¨‹ï¼š**

```python
# å•é¡Œå ´æ™¯ï¼šåŸ·è¡Œå¤§å‹ aggregation æ™‚æ•ˆèƒ½å¾ˆæ…¢

from pyspark.sql.functions import col, sum, avg, count

# è®€å–å¤§å‹è³‡æ–™é›†
transactions = spark.read.parquet("/data/transactions")  # 100 GB
# transaction_id, customer_id, amount, category, date

# åŸ·è¡Œ groupBy aggregationï¼ˆæœƒç”¢ç”Ÿ shuffleï¼‰
result = transactions.groupBy("category", "customer_id").agg(
    sum("amount").alias("total_amount"),
    avg("amount").alias("avg_amount"),
    count("*").alias("transaction_count")
)

# å¯«å…¥çµæœ
result.write.mode("overwrite").parquet("/output/aggregated")

# è¨ºæ–·æ­¥é©Ÿï¼š
"""
æ­¥é©Ÿ 1: åŸ·è¡Œå¾Œï¼Œå‰å¾€ Spark UI

æ­¥é©Ÿ 2: é»æ“Š Stages é é¢
- æ‰¾åˆ°è€—æ™‚æœ€é•·çš„ Stageï¼ˆä¾‹å¦‚ Stage 3ï¼‰
- é»æ“Šé€²å…¥ Stage Detail

æ­¥é©Ÿ 3: åœ¨ Stage Detail æŸ¥çœ‹ Summary Metrics
ç™¼ç¾ï¼š
- Shuffle Spill (Memory): 15 GB
- Shuffle Spill (Disk): 8 GB  â† å¤§é‡ spillï¼
- Duration: 30 min
- Tasks: 200

æ­¥é©Ÿ 4: é»æ“Š Executors é é¢
- é¸æ“‡ä¸€å€‹ Executor
- é»æ“Š "stderr" æŸ¥çœ‹ log

æ­¥é©Ÿ 5: åœ¨ Log ä¸­æœå°‹ "spilling"
æ‰¾åˆ°ï¼š
```
INFO ExternalSorter: Thread 123 spilling in-memory map of 1.2 GB to disk (5 times so far)
WARN ShuffleExternalSorter: Spilling data because memory threshold exceeded
INFO UnsafeExternalSorter: Thread 456 spilling sort data of 800 MB to disk (3 times so far)
```

çµè«–ï¼š
- è¨˜æ†¶é«”ä¸è¶³å°è‡´å¤§é‡ spill
- éœ€è¦å¢åŠ  executor memory æˆ–èª¿æ•´ partitions
"""
```

**2. Spill å„ªåŒ–å¯¦æˆ°ï¼š**

```python
# å•é¡Œï¼šå¤§é‡ spill å°è‡´æ•ˆèƒ½å•é¡Œ

# å„ªåŒ–å‰
spark.conf.set("spark.sql.shuffle.partitions", "200")  # é è¨­
spark.conf.set("spark.executor.memory", "4g")

result = large_df.groupBy("key").agg(sum("value"))
# Stage Detail é¡¯ç¤º:
# - Shuffle Spill (Disk): 10 GB
# - Duration: 45 min

# å„ªåŒ– 1: å¢åŠ  shuffle partitionsï¼ˆæ¸›å°‘æ¯å€‹ partition çš„è³‡æ–™é‡ï¼‰
spark.conf.set("spark.sql.shuffle.partitions", "800")
result = large_df.groupBy("key").agg(sum("value"))
# Stage Detail é¡¯ç¤º:
# - Shuffle Spill (Disk): 3 GB  â† æ”¹å–„ï¼
# - Duration: 25 min

# å„ªåŒ– 2: å¢åŠ  executor memory
# ä¿®æ”¹ cluster é…ç½®:
# spark.executor.memory: 8g
# spark.executor.memoryOverhead: 2g
result = large_df.groupBy("key").agg(sum("value"))
# Stage Detail é¡¯ç¤º:
# - Shuffle Spill (Disk): 0 GB  â† å®Œå…¨æ¶ˆé™¤ï¼
# - Duration: 15 min

# å„ªåŒ– 3: èª¿æ•´ memory fraction
spark.conf.set("spark.memory.fraction", "0.8")  # é è¨­ 0.6
spark.conf.set("spark.memory.storageFraction", "0.3")  # é è¨­ 0.5
result = large_df.groupBy("key").agg(sum("value"))
# çµ¦ execution memory æ›´å¤šç©ºé–“
```

**3. ä½¿ç”¨ç¨‹å¼ç¢¼ç›£æ§ Spillï¼š**

```python
# å–å¾— Spark UI metrics

from pyspark import SparkContext

sc = SparkContext.getOrCreate()

# åŸ·è¡Œæ“ä½œ
df = spark.read.parquet("/data/large_dataset")
result = df.groupBy("category").count()
result.write.parquet("/output/result")

# å–å¾— stage metrics
stages = sc.statusTracker().getStageInfo()

for stage in stages:
    if stage:
        stage_id = stage.stageId
        stage_info = sc.statusTracker().getStageInfo(stage_id)
        print(f"Stage {stage_id}:")
        print(f"  Tasks: {stage_info.numTasks}")
        print(f"  Active Tasks: {stage_info.numActiveTasks}")
        print(f"  Completed Tasks: {stage_info.numCompletedTasks}")
```

**4. è‡ªå‹•åŒ– Spill æª¢æ¸¬è…³æœ¬ï¼š**

```python
# è§£æ Executor logs æ‰¾å‡º spill äº‹ä»¶

import re

def analyze_executor_log(log_file_path):
    """åˆ†æ executor log æ‰¾å‡º spill äº‹ä»¶"""
    spill_events = []
    
    with open(log_file_path, 'r') as f:
        for line in f:
            # å°‹æ‰¾ spill ç›¸é—œè¨Šæ¯
            if 'spilling' in line.lower():
                # æå–è³‡æ–™é‡
                size_match = re.search(r'(\d+\.?\d*)\s*(MB|GB)', line)
                # æå–æ¬¡æ•¸
                times_match = re.search(r'\((\d+)\s+time', line)
                
                event = {
                    'message': line.strip(),
                    'size': size_match.group(0) if size_match else 'Unknown',
                    'times': times_match.group(1) if times_match else '1'
                }
                spill_events.append(event)
    
    return spill_events

# ä½¿ç”¨
log_path = "/databricks/driver/logs/executor-1-stderr.log"
spill_events = analyze_executor_log(log_path)

print(f"ç™¼ç¾ {len(spill_events)} å€‹ spill äº‹ä»¶")
for event in spill_events[:5]:  # é¡¯ç¤ºå‰ 5 å€‹
    print(f"  - {event['size']}, {event['times']} times")
    print(f"    {event['message'][:100]}...")
```

**5. Databricks å°ˆç”¨çš„ Spill ç›£æ§ï¼š**

```python
# åœ¨ Databricks notebook ä¸­æª¢æŸ¥ spill

# å–å¾—æœ€è¿‘çš„ job metrics
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# åŸ·è¡Œæœ‰ shuffle çš„æ“ä½œ
df = spark.read.table("large_table")
result = df.groupBy("category").agg({"amount": "sum"})
result.write.mode("overwrite").saveAsTable("aggregated_table")

# ä½¿ç”¨ Databricks çš„ display åŠŸèƒ½æŸ¥çœ‹ metrics
# display(result)  # æœƒé¡¯ç¤º query plan å’Œ metrics

# æˆ–ä½¿ç”¨ explain
result.explain(mode="extended")

# åœ¨ Databricks UI ä¸­ï¼š
# 1. é»æ“Š cluster â†’ Spark UI
# 2. é€²å…¥ Stages â†’ é¸æ“‡ stage
# 3. æŸ¥çœ‹ Task Metrics è¡¨æ ¼ä¸­çš„ "Spill (Memory)" å’Œ "Spill (Disk)"
```

**6. Spill é é˜²æœ€ä½³å¯¦è¸ï¼š**

```python
# æœ€ä½³å¯¦è¸é…ç½®

# 1. é©ç•¶çš„ executor memory
spark.conf.set("spark.executor.memory", "16g")
spark.conf.set("spark.executor.memoryOverhead", "4g")

# 2. åˆç†çš„ shuffle partitions
# ç¶“é©—æ³•å‰‡ï¼šæ¯å€‹ partition 100-200MB
total_data_size_gb = 100
target_partition_size_mb = 128
num_partitions = int(total_data_size_gb * 1024 / target_partition_size_mb)
spark.conf.set("spark.sql.shuffle.partitions", str(num_partitions))

# 3. å•Ÿç”¨ adaptive query execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")

# 4. èª¿æ•´ memory fractions
spark.conf.set("spark.memory.fraction", "0.8")
spark.conf.set("spark.memory.storageFraction", "0.3")

# 5. ä½¿ç”¨æ›´æœ‰æ•ˆç‡çš„è³‡æ–™æ ¼å¼
# Parquet > ORC > JSON > CSV
df.write.format("parquet").mode("overwrite").save("/optimized/data")

# 6. Broadcast å°è¡¨
from pyspark.sql.functions import broadcast

large_df.join(broadcast(small_df), "key")  # é¿å… shuffle
```

**7. Spill å‘Šè­¦è¨­å®šï¼š**

```python
# å»ºç«‹ç°¡å–®çš„ spill ç›£æ§

def check_spill_and_alert(stage_metrics, threshold_gb=1.0):
    """
    æª¢æŸ¥ spill æ˜¯å¦è¶…éé–¾å€¼ï¼Œä¸¦ç™¼å‡ºå‘Šè­¦
    
    Args:
        stage_metrics: Stage çš„ metrics
        threshold_gb: Spill é–¾å€¼ï¼ˆGBï¼‰
    """
    spill_memory = stage_metrics.get('shuffle_spill_memory', 0)
    spill_disk = stage_metrics.get('shuffle_spill_disk', 0)
    
    threshold_bytes = threshold_gb * 1024 * 1024 * 1024
    
    if spill_disk > threshold_bytes:
        print(f"âš ï¸ WARNING: Significant disk spill detected!")
        print(f"   Shuffle Spill (Memory): {spill_memory / 1e9:.2f} GB")
        print(f"   Shuffle Spill (Disk): {spill_disk / 1e9:.2f} GB")
        print(f"   Recommendation:")
        print(f"   - Increase executor memory")
        print(f"   - Increase shuffle partitions")
        print(f"   - Check for data skew")
        
        # ç™¼é€å‘Šè­¦ï¼ˆæ•´åˆ Slack, Email ç­‰ï¼‰
        # send_alert(f"Spill detected: {spill_disk / 1e9:.2f} GB")
        
        return True
    
    return False

# ä½¿ç”¨ç¯„ä¾‹
# stage_metrics = get_stage_metrics(stage_id)
# check_spill_and_alert(stage_metrics, threshold_gb=2.0)
```

---

## ğŸ”— ç›¸é—œé¡Œç›®
- Q-XXX: Spark Memory Management é…ç½®
- Q-XXX: Wide vs Narrow Transformations
- Q-XXX: Shuffle Operations å„ªåŒ–
- Q-XXX: Data Skew è¨ºæ–·èˆ‡è§£æ±º
- Q-086: Ganglia Metrics å¢é›†ç›£æ§ï¼ˆå·²æœ‰æ­¤é¡Œï¼‰
