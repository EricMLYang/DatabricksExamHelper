# Question #105

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-105`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

The data science team has created and logged a production model using MLflow. The model accepts a list of column names and returns a new column of type DOUBLE.

The following code correctly imports the production model, loads the customers table containing the `customer_id` key column into a DataFrame, and defines the feature columns needed for the model.

```python
model = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/prod")
df = spark.table("customers")
columns = ["account_age", "time_since_last_seen", "app_rating"]
```

Which code block will output a DataFrame with the schema "customer_id LONG, predictions DOUBLE"?

### é¸é …

- **A.** `df.map(lambda x:model(x[columns])).select("customer_id, predictions")`
- **B.** `df.select("customer_id", model(*columns).alias("predictions"))`
- **C.** `model.predict(df, columns)`
- **D.** `df.select("customer_id", pandas_udf(model, columns).alias("predictions"))`
- **E.** `df.apply(model, columns).select("customer_id, predictions")`

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `MLflow`, `Machine-Learning`, `Spark-UDF`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `API-Misuse`, `Function-Signature`, `UDF-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Machine Learning Integration

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** Dï¼ˆâŒ æ­¤æ¨™è¨»è¢«ç¤¾ç¾¤å»£æ³›èªç‚ºæ˜¯éŒ¯èª¤çš„ï¼‰
- **ç¤¾ç¾¤æŠ•ç¥¨ç­”æ¡ˆ:** B (100%)

---

# é¡Œç›®è§£æ

---

## é¡Œç›®å›é¡§

### é¡Œç›®ç·¨è™Ÿèˆ‡é€£çµ
**é¡Œç›® ID:** `Q-105`
**é¡Œç›®é€£çµ:** [é»æ­¤è¿”å›é¡Œç›®](#question-105)

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B`

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** MLflow Model as Spark UDF
**çŸ¥è­˜é ˜åŸŸ:** Machine Learning / Model Deployment
**é—œéµæ¦‚å¿µ:**
- `mlflow.pyfunc.spark_udf()` çš„ä½¿ç”¨æ–¹å¼
- Spark UDF çš„å‘¼å«èªæ³•
- åˆ—åå±•é–‹ï¼ˆ`*columns`ï¼‰vs åˆ—è¡¨å‚³é

### æ¬¡è¦è€ƒé»
- DataFrame æ“ä½œï¼ˆ`select`ï¼‰
- UDF èˆ‡ Pandas UDF çš„å·®ç•°
- MLflow æ¨¡å‹éƒ¨ç½²æœ€ä½³å¯¦å‹™

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ B æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æŠ€è¡“åŸç†:**

**1. `mlflow.pyfunc.spark_udf()` çš„é—œéµç‰¹æ€§ï¼š**

```python
model = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/prod")
```

é€™è¡Œç¨‹å¼ç¢¼åšäº†ä»€éº¼ï¼Ÿ
- âœ… å°‡ MLflow æ¨¡å‹ **è½‰æ›ç‚º Spark UDF**
- âœ… è¿”å›çš„ `model` **å°±æ˜¯ä¸€å€‹ Spark UDF å‡½æ•¸**
- âœ… å¯ä»¥ç›´æ¥åœ¨ `select()` ä¸­å‘¼å«ï¼Œ**ä¸éœ€è¦** é¡å¤–çš„ `pandas_udf()` åŒ…è£

**2. Spark UDF çš„æ­£ç¢ºå‘¼å«æ–¹å¼ï¼ˆé¸é … Bï¼‰ï¼š**

```python
# âœ… æ­£ç¢ºï¼šä½¿ç”¨ *columns å±•é–‹åˆ—åä½œç‚ºåƒæ•¸
df.select("customer_id", model(*columns).alias("predictions"))
```

**ç‚ºä»€éº¼é€™æ¨£å¯«æ˜¯æ­£ç¢ºçš„ï¼Ÿ**

**å±•é–‹èªæ³•ï¼ˆ`*columns`ï¼‰çš„ä½œç”¨ï¼š**

```python
columns = ["account_age", "time_since_last_seen", "app_rating"]

# *columns æœƒå±•é–‹ç‚ºï¼š
model(*columns)
# ç­‰åŒæ–¼ï¼š
model("account_age", "time_since_last_seen", "app_rating")

# é€™æ¨£ UDF æœƒæ¥æ”¶ä¸‰å€‹ç¨ç«‹çš„æ¬„ä½ä½œç‚ºè¼¸å…¥
```

**å®Œæ•´åŸ·è¡Œæµç¨‹ï¼š**

```python
# 1. è¼‰å…¥æ¨¡å‹ç‚º Spark UDF
model = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/prod")
# model ç¾åœ¨æ˜¯ä¸€å€‹ Spark UDFï¼Œå¯ä»¥æ¥å—æ¬„ä½åç¨±ä½œç‚ºåƒæ•¸

# 2. è¼‰å…¥è³‡æ–™
df = spark.table("customers")
# å‡è¨­è³‡æ–™ï¼š
# +-------------+-------------+---------------------+------------+
# | customer_id | account_age | time_since_last_seen| app_rating |
# +-------------+-------------+---------------------+------------+
# | 1           | 365         | 7                   | 4.5        |
# | 2           | 180         | 14                  | 3.8        |
# +-------------+-------------+---------------------+------------+

# 3. å®šç¾©ç‰¹å¾µæ¬„ä½
columns = ["account_age", "time_since_last_seen", "app_rating"]

# 4. æ‡‰ç”¨æ¨¡å‹ï¼ˆé¸é … Bï¼‰
result = df.select("customer_id", model(*columns).alias("predictions"))
# åŸ·è¡Œéç¨‹ï¼š
# - model(*columns) â†’ model("account_age", "time_since_last_seen", "app_rating")
# - UDF è®€å–é€™ä¸‰å€‹æ¬„ä½çš„å€¼
# - å°æ¯ä¸€è¡Œè³‡æ–™åŸ·è¡Œæ¨¡å‹é æ¸¬
# - è¿”å› DOUBLE é¡å‹çš„é æ¸¬å€¼
# - ä½¿ç”¨ .alias("predictions") å‘½åçµæœæ¬„ä½

# 5. æœ€çµ‚è¼¸å‡ºï¼š
# +-------------+-------------+
# | customer_id | predictions |
# +-------------+-------------+
# | 1           | 0.23        |
# | 2           | 0.67        |
# +-------------+-------------+
# Schema: customer_id LONG, predictions DOUBLE âœ…
```

**ç¬¦åˆéœ€æ±‚:**

é¡Œç›®è¦æ±‚ï¼š
- âœ… ä¿ç•™ `customer_id` æ¬„ä½
- âœ… æ–°å¢ `predictions` æ¬„ä½ï¼ˆDOUBLE é¡å‹ï¼‰
- âœ… Schema: `customer_id LONG, predictions DOUBLE`

é¸é … B å®Œç¾ç¬¦åˆï¼š
```python
df.select(
    "customer_id",                    # â† ä¿ç•™ customer_id
    model(*columns).alias("predictions")  # â† æ–°å¢ predictions (DOUBLE)
)
```

**å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹ï¼š**

```python
import mlflow

# æƒ…å¢ƒï¼šå®¢æˆ¶æµå¤±é æ¸¬æ¨¡å‹
# è¼‰å…¥ç”Ÿç”¢ç’°å¢ƒæ¨¡å‹
model = mlflow.pyfunc.spark_udf(
    spark, 
    model_uri="models:/churn/prod",  # æ¨¡å‹è¨»å†Šè¡¨ä¸­çš„ç”Ÿç”¢ç‰ˆæœ¬
    result_type="double"              # æ˜ç¢ºæŒ‡å®šè¿”å›é¡å‹ï¼ˆå¯é¸ï¼‰
)

# è¼‰å…¥å®¢æˆ¶è³‡æ–™
customers_df = spark.table("customers")

# å®šç¾©ç‰¹å¾µæ¬„ä½
feature_columns = ["account_age", "time_since_last_seen", "app_rating"]

# æ‰¹æ¬¡é æ¸¬ï¼ˆé¸é … B çš„æ–¹å¼ï¼‰
predictions_df = customers_df.select(
    "customer_id",
    "customer_name",  # å¯ä»¥ä¿ç•™å…¶ä»–æ¬„ä½
    model(*feature_columns).alias("churn_probability")
)

# ç¯©é¸é«˜é¢¨éšªå®¢æˆ¶
high_risk_customers = predictions_df.filter("churn_probability > 0.7")

# å„²å­˜çµæœ
high_risk_customers.write.mode("overwrite").saveAsTable("high_risk_customers")
```

**é€²éšæ‡‰ç”¨ï¼šå¤šæ¨¡å‹æ¯”è¼ƒ**

```python
# è¼‰å…¥å¤šå€‹æ¨¡å‹ç‰ˆæœ¬
model_v1 = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/v1")
model_v2 = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/v2")
model_prod = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/prod")

# åŒæ™‚åŸ·è¡Œå¤šå€‹æ¨¡å‹é æ¸¬
comparison_df = customers_df.select(
    "customer_id",
    model_v1(*feature_columns).alias("prediction_v1"),
    model_v2(*feature_columns).alias("prediction_v2"),
    model_prod(*feature_columns).alias("prediction_prod")
)

# åˆ†ææ¨¡å‹å·®ç•°
comparison_df.withColumn(
    "max_diff", 
    greatest("prediction_v1", "prediction_v2", "prediction_prod") - 
    least("prediction_v1", "prediction_v2", "prediction_prod")
).filter("max_diff > 0.3").show()
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - `df.map(lambda x:model(x[columns])).select("customer_id, predictions")`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

âŒ **èªæ³•éŒ¯èª¤** - `map()` ä¸æ˜¯ Spark DataFrame çš„æ–¹æ³•
- Spark DataFrame **æ²’æœ‰** `map()` æ–¹æ³•ï¼ˆé€™æ˜¯ RDD çš„æ–¹æ³•ï¼‰
- å¦‚æœè¦ä½¿ç”¨ `map()`ï¼Œéœ€è¦å…ˆè½‰æ›ç‚º RDDï¼š`df.rdd.map(...)`

âŒ **é‚è¼¯éŒ¯èª¤** - `x[columns]` èªæ³•ä¸æ­£ç¢º
- åœ¨ Row ç‰©ä»¶ä¸­ï¼Œç„¡æ³•ç”¨åˆ—è¡¨ç´¢å¼•å–å€¼
- æ‡‰è©²ä½¿ç”¨ `x["column_name"]` æˆ– `x.column_name`

âŒ **å‹åˆ¥éŒ¯èª¤** - `map()` è¿”å› RDDï¼Œä¸æ˜¯ DataFrame
- å³ä½¿ä¿®æ­£ç‚º `df.rdd.map(...)`ï¼Œè¿”å›çš„æ˜¯ RDD
- ç„¡æ³•ç›´æ¥ä½¿ç”¨ `.select()` æ–¹æ³•ï¼ˆDataFrame å°ˆç”¨ï¼‰

**éŒ¯èª¤ç¤ºç¯„èˆ‡ä¿®æ­£ï¼š**

```python
# âŒ éŒ¯èª¤ï¼ˆé¸é … Aï¼‰
df.map(lambda x: model(x[columns])).select("customer_id, predictions")
# éŒ¯èª¤ 1: DataFrame æ²’æœ‰ map() æ–¹æ³•
# éŒ¯èª¤ 2: x[columns] èªæ³•ä¸æ”¯æ´
# éŒ¯èª¤ 3: å³ä½¿ map å¯ç”¨ï¼Œè¿”å› RDD ç„¡æ³•ç”¨ select()

# âŒ å³ä½¿æ”¹æˆ RDD ä¹Ÿä¸å°
df.rdd.map(lambda x: model(x[columns])).select("customer_id, predictions")
# éŒ¯èª¤ï¼šRDD æ²’æœ‰ select() æ–¹æ³•

# âœ… æ­£ç¢ºæ–¹å¼ï¼ˆé¸é … Bï¼‰
df.select("customer_id", model(*columns).alias("predictions"))
# ç›´æ¥åœ¨ DataFrame ä¸Šä½¿ç”¨ select() + UDF
```

---

### é¸é … C - `model.predict(df, columns)`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

âŒ **æ–¹æ³•ä¸å­˜åœ¨** - Spark UDF æ²’æœ‰ `.predict()` æ–¹æ³•
- `mlflow.pyfunc.spark_udf()` è¿”å›çš„æ˜¯ **Spark UDF å‡½æ•¸**
- UDF å‡½æ•¸æœ¬èº«æ²’æœ‰ `.predict()` æ–¹æ³•

âŒ **æ¦‚å¿µæ··æ·†** - æ··æ·†äº†ä¸åŒçš„ API
- `.predict()` æ˜¯ **Python æ¨¡å‹ç‰©ä»¶** çš„æ–¹æ³•ï¼ˆscikit-learn é¢¨æ ¼ï¼‰
- Spark UDF çš„ä½¿ç”¨æ–¹å¼æ˜¯ **åœ¨ select() ä¸­å‘¼å«**

**API å°æ¯”ï¼š**

```python
# Python æ¨¡å‹ç‰©ä»¶ï¼ˆæœ‰ .predict() æ–¹æ³•ï¼‰
python_model = mlflow.pyfunc.load_model("models:/churn/prod")
predictions = python_model.predict(pandas_df)  # âœ… Python API

# Spark UDFï¼ˆæ²’æœ‰ .predict() æ–¹æ³•ï¼‰
spark_udf = mlflow.pyfunc.spark_udf(spark, "models:/churn/prod")
spark_udf.predict(df, columns)  # âŒ éŒ¯èª¤ï¼UDF æ²’æœ‰æ­¤æ–¹æ³•
df.select(spark_udf(*columns))  # âœ… æ­£ç¢ºä½¿ç”¨æ–¹å¼
```

**æ­£ç¢ºçš„ä¸åŒè¼‰å…¥æ–¹å¼å°æ¯”ï¼š**

```python
# æ–¹å¼ 1ï¼šè¼‰å…¥ç‚º Python å‡½æ•¸ï¼ˆå–®æ©Ÿé æ¸¬ï¼‰
python_model = mlflow.pyfunc.load_model(model_uri="models:/churn/prod")
# ä½¿ç”¨ .predict() æ–¹æ³•
predictions = python_model.predict(pandas_df[columns])  # âœ… Pandas DataFrame

# æ–¹å¼ 2ï¼šè¼‰å…¥ç‚º Spark UDFï¼ˆåˆ†æ•£å¼é æ¸¬ï¼‰
spark_udf = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/prod")
# ä½¿ç”¨ select() + UDF å‘¼å«
predictions_df = df.select(spark_udf(*columns))  # âœ… Spark DataFrame

# âŒ ä¸èƒ½æ··ç”¨
spark_udf.predict(df, columns)  # éŒ¯èª¤ï¼šUDF æ²’æœ‰ predict()
```

---

### é¸é … D - `df.select("customer_id", pandas_udf(model, columns).alias("predictions"))`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

âŒ **é‡è¤‡åŒ…è£** - `model` å·²ç¶“æ˜¯ Spark UDFï¼Œä¸éœ€è¦å†ç”¨ `pandas_udf()` åŒ…è£
- `mlflow.pyfunc.spark_udf()` **å·²ç¶“å»ºç«‹äº† UDF**
- å†ç”¨ `pandas_udf()` åŒ…è£æœƒé€ æˆéŒ¯èª¤

âŒ **èªæ³•éŒ¯èª¤** - `pandas_udf()` çš„ä½¿ç”¨æ–¹å¼ä¸æ­£ç¢º
- `pandas_udf()` æ˜¯ **è£é£¾å™¨** æˆ–ç”¨æ–¼åŒ…è£ **Python å‡½æ•¸**
- ä¸èƒ½ç›´æ¥åŒ…è£å·²å­˜åœ¨çš„ UDF ç‰©ä»¶

âŒ **åƒæ•¸å‚³ééŒ¯èª¤** - `pandas_udf(model, columns)` ä¸æ˜¯æœ‰æ•ˆçš„èªæ³•
- `pandas_udf()` çš„ç¬¬ä¸€å€‹åƒæ•¸æ‡‰è©²æ˜¯ **Python å‡½æ•¸å®šç¾©**
- ä¸æ¥å— `columns` é€™ç¨®åƒæ•¸

**ç‚ºä»€éº¼é€™æ˜¯æœ€å¤§çš„é™·é˜±é¸é …ï¼Ÿ**

é€™å€‹é¸é …è¨­è¨ˆä¾†æ¸¬è©¦ä½ æ˜¯å¦ç†è§£ï¼š
1. `mlflow.pyfunc.spark_udf()` **æœ¬èº«å°±è¿”å› UDF**
2. ä¸éœ€è¦é¡å¤–çš„ UDF åŒ…è£

**æ­£ç¢ºçš„ pandas_udf ä½¿ç”¨æ–¹å¼ï¼ˆå°æ¯”ï¼‰ï¼š**

```python
# æƒ…å¢ƒ 1ï¼šè‡ªå®šç¾©å‡½æ•¸éœ€è¦è½‰æ›ç‚º Pandas UDF
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf("double")
def custom_prediction(account_age: pd.Series, time_since: pd.Series, rating: pd.Series) -> pd.Series:
    # è‡ªå®šç¾©é‚è¼¯
    return (account_age * 0.1 + time_since * 0.2 + rating * 0.3)

# âœ… ä½¿ç”¨è‡ªå®šç¾© Pandas UDF
df.select("customer_id", custom_prediction("account_age", "time_since_last_seen", "app_rating").alias("predictions"))

# æƒ…å¢ƒ 2ï¼šMLflow æ¨¡å‹å·²ç¶“æ˜¯ UDFï¼ˆä¸éœ€è¦ pandas_udfï¼‰
model = mlflow.pyfunc.spark_udf(spark, model_uri="models:/churn/prod")
# âœ… ç›´æ¥ä½¿ç”¨ï¼ˆé¸é … Bï¼‰
df.select("customer_id", model(*columns).alias("predictions"))

# âŒ éŒ¯èª¤ï¼šé‡è¤‡åŒ…è£ï¼ˆé¸é … Dï¼‰
df.select("customer_id", pandas_udf(model, columns).alias("predictions"))
```

**å¯¦éš›éŒ¯èª¤è¨Šæ¯ï¼š**

```python
# åŸ·è¡Œé¸é … D æœƒçœ‹åˆ°é¡ä¼¼çš„éŒ¯èª¤ï¼š
# TypeError: Invalid function: not a function or callable (__call__)
# æˆ–
# ValueError: Invalid udf: the udf argument must be a pandas_udf's function
```

**é—œéµç†è§£ï¼š**

| å ´æ™¯ | éœ€è¦ `pandas_udf()` å—ï¼Ÿ | æ­£ç¢ºå¯«æ³• |
|------|------------------------|---------|
| è‡ªå®šç¾© Python å‡½æ•¸ | âœ… éœ€è¦ | `pandas_udf(my_function)` æˆ– `@pandas_udf` è£é£¾å™¨ |
| MLflow Spark UDF | âŒ ä¸éœ€è¦ | ç›´æ¥ä½¿ç”¨ `model(*columns)` |
| å·²è¨»å†Šçš„ UDF | âŒ ä¸éœ€è¦ | ç›´æ¥å‘¼å« UDF åç¨± |

---

### é¸é … E - `df.apply(model, columns).select("customer_id, predictions")`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

âŒ **æ–¹æ³•ä¸å­˜åœ¨** - Spark DataFrame **æ²’æœ‰** `apply()` æ–¹æ³•
- `apply()` æ˜¯ **Pandas DataFrame** çš„æ–¹æ³•
- Spark DataFrame ä½¿ç”¨ä¸åŒçš„ APIï¼ˆ`select()`, `withColumn()`, `transform()` ç­‰ï¼‰

âŒ **API æ··æ·†** - æ··æ·† Pandas å’Œ Spark çš„èªæ³•
- Pandas: `df.apply(func, axis=1)`
- Spark: `df.select(udf(...))` æˆ– `df.withColumn(...)`

**Pandas vs Spark èªæ³•å°æ¯”ï¼š**

```python
# Pandas DataFrameï¼ˆå–®æ©Ÿï¼‰
import pandas as pd

pandas_df = pd.DataFrame({
    "customer_id": [1, 2, 3],
    "account_age": [365, 180, 90],
    "time_since_last_seen": [7, 14, 3],
    "app_rating": [4.5, 3.8, 5.0]
})

# âœ… Pandas ä½¿ç”¨ apply()
def predict_row(row):
    return row["account_age"] * 0.1 + row["time_since_last_seen"] * 0.2

pandas_df["predictions"] = pandas_df.apply(predict_row, axis=1)

# Spark DataFrameï¼ˆåˆ†æ•£å¼ï¼‰
spark_df = spark.createDataFrame(pandas_df)

# âŒ éŒ¯èª¤ï¼šSpark æ²’æœ‰ apply()
spark_df.apply(model, columns)  # AttributeError: 'DataFrame' object has no attribute 'apply'

# âœ… æ­£ç¢ºï¼šSpark ä½¿ç”¨ select() + UDF
spark_df.select("customer_id", model(*columns).alias("predictions"))

# âœ… æˆ–ä½¿ç”¨ withColumn()
spark_df.withColumn("predictions", model(*columns))
```

**Spark çš„æ›¿ä»£æ–¹æ³•ï¼š**

```python
# Spark ä¸­å¯¦ç¾é¡ä¼¼ apply() çš„åŠŸèƒ½

# æ–¹æ³• 1ï¼šä½¿ç”¨ select() + UDFï¼ˆæ¨è–¦ï¼‰
result = df.select("customer_id", model(*columns).alias("predictions"))

# æ–¹æ³• 2ï¼šä½¿ç”¨ withColumn() + UDF
result = df.withColumn("predictions", model(*columns))

# æ–¹æ³• 3ï¼šä½¿ç”¨ transform()ï¼ˆSpark 3.0+ï¼‰
from pyspark.sql.functions import transform

def add_predictions(input_df):
    return input_df.withColumn("predictions", model(*columns))

result = df.transform(add_predictions).select("customer_id", "predictions")

# æ–¹æ³• 4ï¼šä½¿ç”¨ SQL è¡¨é”å¼
spark.udf.register("predict_churn", model)
df.createOrReplaceTempView("customers")
result = spark.sql("""
    SELECT customer_id, 
           predict_churn(account_age, time_since_last_seen, app_rating) as predictions
    FROM customers
""")
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£è¨˜æ†¶

**ã€ŒMLflow Spark UDFï¼šå·²ç¶“æ˜¯ UDFï¼Œç›´æ¥ select å°±å¯ä»¥ã€**

```python
model = mlflow.pyfunc.spark_udf(...)  # â† å·²ç¶“æ˜¯ UDF
df.select(model(*columns))            # â† ç›´æ¥ç”¨ï¼Œä¸éœ€åŒ…è£
```

**ã€Œæ˜Ÿè™Ÿå±•é–‹ç‰¹å¾µï¼Œalias å‘½åçµæœã€**

```python
df.select(
    "customer_id",
    model(*columns).alias("predictions")  # *columns å±•é–‹ï¼Œalias å‘½å
)
```

### æ¦‚å¿µå°æ¯”è¡¨

| å°æ¯”é … | Pandas (å–®æ©Ÿ) | Spark (åˆ†æ•£å¼) |
|--------|--------------|---------------|
| **DataFrame é¡å‹** | `pandas.DataFrame` | `pyspark.sql.DataFrame` |
| **æ¨¡å‹è¼‰å…¥** | `mlflow.pyfunc.load_model()` | `mlflow.pyfunc.spark_udf()` |
| **é æ¸¬æ–¹æ³•** | `model.predict(df)` | `df.select(model(*cols))` |
| **è¡Œç´šæ“ä½œ** | `df.apply(func, axis=1)` | `df.select(udf(...))` |
| **è¿”å›é¡å‹** | Pandas Series/DataFrame | Spark DataFrame |

### API ä½¿ç”¨é€ŸæŸ¥

**MLflow æ¨¡å‹åœ¨ä¸åŒç’°å¢ƒçš„è¼‰å…¥æ–¹å¼ï¼š**

| ç’°å¢ƒ | è¼‰å…¥æ–¹æ³• | ä½¿ç”¨æ–¹å¼ | ç¯„ä¾‹ |
|------|---------|---------|------|
| **Python/Pandas** | `mlflow.pyfunc.load_model()` | `.predict(df)` | `model.predict(pandas_df)` |
| **Spark UDF** | `mlflow.pyfunc.spark_udf()` | `df.select(model(*cols))` | `df.select(model(*features))` |
| **Spark SQL** | è¨»å†Š UDF | SQL æŸ¥è©¢ | `spark.udf.register()` + SQL |

### å¯¦å‹™è¨˜æ†¶é»

**1. è¨˜ä½ `spark_udf()` çš„è¼¸å‡ºï¼š**
```python
model = mlflow.pyfunc.spark_udf(...)
# model æ˜¯ä»€éº¼ï¼Ÿ â†’ Spark UDF å‡½æ•¸ï¼ˆå¯ç›´æ¥å‘¼å«ï¼‰
# ä¸æ˜¯ä»€éº¼ï¼Ÿ â†’ ä¸æ˜¯æ¨¡å‹ç‰©ä»¶ï¼ˆæ²’æœ‰ .predict() æ–¹æ³•ï¼‰
```

**2. è¨˜ä½æ˜Ÿè™Ÿå±•é–‹ï¼ˆ`*`ï¼‰çš„ä½œç”¨ï¼š**
```python
columns = ["col1", "col2", "col3"]
model(*columns)  # â†’ model("col1", "col2", "col3")
# UDF æ¥æ”¶ä¸‰å€‹ç¨ç«‹åƒæ•¸ï¼ˆæ¬„ä½åç¨±ï¼‰
```

**3. è¨˜ä½ä¸éœ€è¦é‡è¤‡åŒ…è£ï¼š**
```python
model = mlflow.pyfunc.spark_udf(...)  # å·²ç¶“æ˜¯ UDF
# âœ… ç›´æ¥ç”¨
df.select(model(*cols))
# âŒ ä¸è¦å†åŒ…è£
df.select(pandas_udf(model, cols))  # éŒ¯èª¤ï¼
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶èˆ‡åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡ä»¶é€£çµ

1. **MLflow - Python Function (pyfunc):**
   - [MLflow Python Function Model](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html)
   - èªªæ˜ `spark_udf()` çš„ä½¿ç”¨æ–¹å¼

2. **MLflow - Spark UDF:**
   - [Deploy MLflow Models as Spark UDFs](https://mlflow.org/docs/latest/models.html#deploy-mlflow-models-as-spark-udfs)
   - å®Œæ•´çš„ Spark UDF éƒ¨ç½²ç¯„ä¾‹

3. **Databricks - MLflow Model Serving:**
   - [MLflow Model Registry](https://docs.databricks.com/mlflow/model-registry.html)
   - æ¨¡å‹è¨»å†Šèˆ‡ç‰ˆæœ¬ç®¡ç†

4. **PySpark - User Defined Functions:**
   - [PySpark UDF Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)
   - Spark UDF çš„åº•å±¤æ©Ÿåˆ¶

### å»¶ä¼¸å­¸ç¿’è³‡æº

- **æ‰¹æ¬¡æ¨è«–æœ€ä½³å¯¦å‹™ï¼š** å¤§è¦æ¨¡æ¨¡å‹é æ¸¬çš„æ•ˆèƒ½å„ªåŒ–
- **æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ï¼š** ä½¿ç”¨ MLflow Model Registry ç®¡ç†ç”Ÿç”¢æ¨¡å‹
- **A/B æ¸¬è©¦ï¼š** åŒæ™‚éƒ¨ç½²å¤šå€‹æ¨¡å‹ç‰ˆæœ¬é€²è¡Œæ¯”è¼ƒ

---

## ğŸ’¡ è£œå……èªªæ˜

### å¯¦å‹™æ‡‰ç”¨å ´æ™¯

**1. æ‰¹æ¬¡æ¨è«– Pipelineï¼š**

```python
# å®Œæ•´çš„æ‰¹æ¬¡æ¨è«–å·¥ä½œæµç¨‹
from pyspark.sql import SparkSession
import mlflow

# 1. è¼‰å…¥ç”Ÿç”¢æ¨¡å‹
model = mlflow.pyfunc.spark_udf(
    spark, 
    model_uri="models:/customer_churn/Production"
)

# 2. è¼‰å…¥éœ€è¦é æ¸¬çš„è³‡æ–™
customers_df = spark.table("prod.customers").filter("last_prediction_date < current_date()")

# 3. å®šç¾©ç‰¹å¾µ
feature_cols = ["account_age", "time_since_last_seen", "app_rating", 
                "total_purchases", "avg_order_value"]

# 4. åŸ·è¡Œæ‰¹æ¬¡é æ¸¬
predictions_df = customers_df.select(
    "customer_id",
    "customer_name",
    "email",
    model(*feature_cols).alias("churn_probability"),
    current_timestamp().alias("prediction_timestamp")
)

# 5. å„²å­˜é æ¸¬çµæœ
predictions_df.write.mode("overwrite").partitionBy("prediction_timestamp") \
    .saveAsTable("prod.customer_churn_predictions")

# 6. è§¸ç™¼ä¸‹æ¸¸è¡Œå‹•ï¼ˆé«˜é¢¨éšªå®¢æˆ¶é€šçŸ¥ï¼‰
high_risk = predictions_df.filter("churn_probability > 0.8")
high_risk.write.mode("overwrite").saveAsTable("prod.high_risk_customers_alert")
```

**2. æ¨¡å‹ A/B æ¸¬è©¦ï¼š**

```python
# åŒæ™‚è©•ä¼°å¤šå€‹æ¨¡å‹ç‰ˆæœ¬
model_champion = mlflow.pyfunc.spark_udf(spark, "models:/churn/Production")
model_challenger = mlflow.pyfunc.spark_udf(spark, "models:/churn/Staging")

# å°ç›¸åŒè³‡æ–™åŸ·è¡Œé æ¸¬
ab_test_df = test_data.select(
    "customer_id",
    model_champion(*features).alias("champion_score"),
    model_challenger(*features).alias("challenger_score")
)

# è¨ˆç®—é æ¸¬å·®ç•°
from pyspark.sql.functions import abs as spark_abs

ab_test_df = ab_test_df.withColumn(
    "prediction_diff",
    spark_abs(col("champion_score") - col("challenger_score"))
)

# åˆ†æé¡¯è‘—å·®ç•°çš„æ¡ˆä¾‹
significant_diff = ab_test_df.filter("prediction_diff > 0.2")
print(f"é¡¯è‘—å·®ç•°æ¡ˆä¾‹æ•¸: {significant_diff.count()}")

# èˆ‡å¯¦éš›çµæœæ¯”å°ï¼ˆå¦‚æœæœ‰ ground truthï¼‰
evaluation_df = ab_test_df.join(actual_churn_df, "customer_id")
# è¨ˆç®—å„æ¨¡å‹çš„æº–ç¢ºç‡ã€AUC ç­‰æŒ‡æ¨™
```

**3. å³æ™‚ç‰¹å¾µå·¥ç¨‹ + é æ¸¬ï¼š**

```python
# çµåˆç‰¹å¾µå·¥ç¨‹èˆ‡æ¨¡å‹é æ¸¬
from pyspark.sql.functions import datediff, current_date

# è¼‰å…¥æ¨¡å‹
model = mlflow.pyfunc.spark_udf(spark, "models:/churn/prod")

# è®€å–åŸå§‹è³‡æ–™ä¸¦é€²è¡Œç‰¹å¾µå·¥ç¨‹
raw_df = spark.table("raw.customer_events")

# è¨ˆç®—è¡ç”Ÿç‰¹å¾µ
features_df = raw_df.select(
    "customer_id",
    datediff(current_date(), col("account_created_date")).alias("account_age"),
    datediff(current_date(), col("last_activity_date")).alias("time_since_last_seen"),
    col("app_rating"),
    (col("total_spent") / col("total_orders")).alias("avg_order_value")
)

# å®šç¾©æ¨¡å‹æ‰€éœ€ç‰¹å¾µ
model_features = ["account_age", "time_since_last_seen", "app_rating", "avg_order_value"]

# ä¸€æ¬¡æ€§å®Œæˆç‰¹å¾µå·¥ç¨‹ + é æ¸¬
predictions_df = features_df.select(
    "customer_id",
    model(*model_features).alias("churn_probability")
)
```

**4. Delta Lake + MLflow æ•´åˆï¼š**

```python
# å°‡é æ¸¬çµæœå¯«å…¥ Delta Lake ä¸¦å•Ÿç”¨è®Šæ›´è³‡æ–™æ•ç²
from delta.tables import DeltaTable

# åŸ·è¡Œé æ¸¬
predictions_df = customers_df.select(
    "customer_id",
    model(*features).alias("churn_probability"),
    current_timestamp().alias("prediction_time")
)

# Merge åˆ° Delta Tableï¼ˆé¿å…é‡è¤‡é æ¸¬ï¼‰
if DeltaTable.isDeltaTable(spark, "prod.churn_predictions"):
    delta_table = DeltaTable.forName(spark, "prod.churn_predictions")
    
    delta_table.alias("target").merge(
        predictions_df.alias("source"),
        "target.customer_id = source.customer_id"
    ).whenMatchedUpdate(set={
        "churn_probability": "source.churn_probability",
        "prediction_time": "source.prediction_time"
    }).whenNotMatchedInsertAll().execute()
else:
    predictions_df.write.format("delta").saveAsTable("prod.churn_predictions")

# å•Ÿç”¨ CDC è¿½è¹¤é æ¸¬è®ŠåŒ–
spark.sql("""
    ALTER TABLE prod.churn_predictions 
    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")
```

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

**çœ‹åˆ° MLflow + Spark é¡Œç›®ï¼Œç«‹å³æª¢æŸ¥ï¼š**

1. âœ… **æ¨¡å‹è¼‰å…¥æ–¹å¼** - `mlflow.pyfunc.spark_udf()` è¿”å› UDF
2. âœ… **UDF ä½¿ç”¨æ–¹å¼** - åœ¨ `select()` ä¸­ç›´æ¥å‘¼å«
3. âœ… **åƒæ•¸å±•é–‹** - ä½¿ç”¨ `*columns` å±•é–‹åˆ—å
4. âŒ **é¿å…é‡è¤‡åŒ…è£** - ä¸éœ€è¦ `pandas_udf()` å†åŒ…è£
5. âŒ **é¿å…éŒ¯èª¤æ–¹æ³•** - Spark æ²’æœ‰ `map()`, `apply()`, `.predict()`

### é™·é˜±è­˜åˆ¥

**æœ¬é¡Œçš„é™·é˜±è¨­è¨ˆï¼š**

- **é¸é … A** - æ··ç”¨ RDD èªæ³•ï¼ˆ`map()`ï¼‰èˆ‡ DataFrame èªæ³•ï¼ˆ`select()`ï¼‰
- **é¸é … C** - èª¤ä»¥ç‚º UDF æœ‰ `.predict()` æ–¹æ³•ï¼ˆæ¦‚å¿µæ··æ·†ï¼‰
- **é¸é … D** - é‡è¤‡åŒ…è£é™·é˜±ï¼ˆæœ€å¤§è¿·æƒ‘ï¼šçœ‹èµ·ä¾†å¾ˆå°ˆæ¥­ï¼‰
- **é¸é … E** - æ··æ·† Pandas å’Œ Spark èªæ³•ï¼ˆ`apply()` ä¸å­˜åœ¨ï¼‰
- **é¸é … B** - âœ… æ­£ç¢ºï¼šç›´æ¥ä½¿ç”¨ UDF + æ˜Ÿè™Ÿå±•é–‹

**å¿«é€Ÿæ’é™¤æŠ€å·§ï¼š**

| é¸é … | å¿«é€Ÿæ’é™¤é—œéµå­— | åŸå›  |
|------|--------------|------|
| A | `df.map()` | Spark DataFrame æ²’æœ‰ `map()` |
| C | `model.predict()` | UDF æ²’æœ‰ `.predict()` æ–¹æ³• |
| D | `pandas_udf(model, ...)` | é‡è¤‡åŒ…è£ + èªæ³•éŒ¯èª¤ |
| E | `df.apply()` | Spark DataFrame æ²’æœ‰ `apply()` |
| B | âœ… `select()` + `model(*cols)` | æ¨™æº– Spark UDF ç”¨æ³• |

---

## âš ï¸ ä¾†æºç­”æ¡ˆçˆ­è­°èªªæ˜

### ç‚ºä»€éº¼ä¾†æºæ¨™è¨» D æ˜¯éŒ¯èª¤çš„ï¼Ÿ

**åŸå› åˆ†æï¼š**

1. **æŠ€è¡“ä¸Šä¸æ­£ç¢ºï¼š** `pandas_udf(model, columns)` ä¸æ˜¯æœ‰æ•ˆèªæ³•
2. **é‡è¤‡åŒ…è£ï¼š** `mlflow.pyfunc.spark_udf()` å·²ç¶“è¿”å› UDF
3. **ç¤¾ç¾¤å…±è­˜ï¼š** 100% æŠ•ç¥¨æ”¯æŒé¸é … B

**å¯èƒ½çš„æ¨™è¨»éŒ¯èª¤ä¾†æºï¼š**

- å‡ºé¡Œè€…å¯èƒ½èª¤ä»¥ç‚ºéœ€è¦é¡å¤–çš„ UDF åŒ…è£
- æ··æ·†äº†è‡ªå®šç¾©å‡½æ•¸ï¼ˆéœ€è¦ `pandas_udf()`ï¼‰èˆ‡ MLflow UDFï¼ˆä¸éœ€è¦ï¼‰

**æ­£ç¢ºç†è§£ï¼š**

```python
# MLflow å·²ç¶“å¹«ä½ å»ºç«‹ UDF
model = mlflow.pyfunc.spark_udf(spark, model_uri)
# âœ… ç›´æ¥ä½¿ç”¨ï¼ˆé¸é … Bï¼‰
df.select(model(*cols))
# âŒ ä¸éœ€è¦å†åŒ…è£ï¼ˆé¸é … Dï¼‰
df.select(pandas_udf(model, cols))
```

---

**[è¿”å›é¡Œç›®](#question-105)**