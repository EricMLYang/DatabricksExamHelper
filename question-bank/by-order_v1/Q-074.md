# Question #74

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-074`

### ä¾†æº
**ä¾†æº:** Community Contributed (Real Exam-style)

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹
Which statement describes the correct use of `pyspark.sql.functions.broadcast`?

### é¸é …
- **A.** It marks a column as having low enough cardinality to properly map distinct values to available partitions, allowing a broadcast join.
- **B.** It marks a column as small enough to store in memory on all executors, allowing a broadcast join.
- **C.** It caches a copy of the indicated table on attached storage volumes for all active clusters within a Databricks workspace.
- **D.** It marks a DataFrame as small enough to store in memory on all executors, allowing a broadcast join.
- **E.** It caches a copy of the indicated table on all nodes in the cluster for use in all future queries during the cluster lifetime.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `PySpark`, `Spark-Joins`, `Spark-Performance`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Similar-Function`, `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** `Spark`

---

## ç­”æ¡ˆèˆ‡è§£æé€£çµ

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

### è§£ææª”æ¡ˆ
**è©³ç´°è§£æ:** ä¸‹æ–¹æ•´åˆè§£æ

---

## ç›¸é—œè³‡æº

### å®˜æ–¹æ–‡ä»¶
- [Broadcast joins in Spark](https://spark.apache.org/docs/latest/sql-performance-tuning.html#broadcast-hint-for-sql-queries)
- [pyspark.sql.functions.broadcast API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html)

---

## è§£é¡Œè§£æï¼ˆæ•´åˆï¼‰

### ç‚ºä»€éº¼ D æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**broadcast() å‡½æ•¸çš„æ ¸å¿ƒåŠŸèƒ½ï¼š**
- ä½œç”¨å°è±¡ï¼š**DataFrame**ï¼ˆä¸æ˜¯ columnï¼‰
- ç›®çš„ï¼šæ¨™è¨˜æ­¤ DataFrame è¶³å¤ å°ï¼Œå¯ä»¥å®Œæ•´è¤‡è£½åˆ°**æ‰€æœ‰ executor çš„è¨˜æ†¶é«”**ä¸­
- ä½¿ç”¨æƒ…å¢ƒï¼šå„ªåŒ– join æ“ä½œï¼Œç‰¹åˆ¥æ˜¯**å°è¡¨ join å¤§è¡¨**æ™‚
- æ•ˆèƒ½å„ªå‹¢ï¼šé¿å… shuffle æ“ä½œï¼Œå°‡å°è¡¨å»£æ’­åˆ°æ‰€æœ‰ç¯€é»é€²è¡Œæœ¬åœ° join

**å¯¦å‹™æ‡‰ç”¨ï¼š**
```python
from pyspark.sql.functions import broadcast

# å°è¡¨ (ä¾‹å¦‚ï¼šç¶­åº¦è¡¨ã€lookup è¡¨)
small_df = spark.table("dim_product")

# å¤§è¡¨ (ä¾‹å¦‚ï¼šäº‹å¯¦è¡¨)
large_df = spark.table("fact_sales")

# ä½¿ç”¨ broadcast hint å„ªåŒ– join
result = large_df.join(
    broadcast(small_df),  # æ¨™è¨˜ small_df ç‚º broadcast
    "product_id"
)
```

**é‹ä½œåŸç†ï¼š**
1. Driver æ”¶é›†å°è¡¨çš„å®Œæ•´è³‡æ–™
2. å°‡å°è¡¨å»£æ’­ï¼ˆbroadcastï¼‰åˆ°æ¯å€‹ executor
3. å„ executor åœ¨æœ¬åœ°è¨˜æ†¶é«”ä¸­ä¿å­˜å°è¡¨å‰¯æœ¬
4. Join æ“ä½œåœ¨æœ¬åœ°é€²è¡Œï¼Œç„¡éœ€ shuffle å¤§è¡¨è³‡æ–™

**æœ€ä½³å¯¦å‹™ï¼š**
- å°è¡¨å¤§å°ï¼šé€šå¸¸ < 10 MBï¼ˆå¯é€é `spark.sql.autoBroadcastJoinThreshold` èª¿æ•´ï¼‰
- é¿å…å°å¤§è¡¨ä½¿ç”¨ broadcastï¼šæœƒå°è‡´ OOM (Out of Memory)
- Spark æœ‰æ™‚æœƒè‡ªå‹•é¸æ“‡ broadcast joinï¼Œä½†æ‰‹å‹•æŒ‡å®šæ›´æ˜ç¢º

---

### éŒ¯èª¤é¸é …æ’é™¤

**é¸é … A - æ¨™è¨˜ column å…·æœ‰ä½åŸºæ•¸ï¼š**
- âŒ **ä½œç”¨å°è±¡éŒ¯èª¤**ï¼š`broadcast()` ä½œç”¨æ–¼ **DataFrame**ï¼Œä¸æ˜¯ column
- âŒ **æ¦‚å¿µæ··æ·†**ï¼šä½åŸºæ•¸ï¼ˆlow cardinalityï¼‰èˆ‡ broadcast join ç„¡é—œï¼›é€™æ›´åƒæ˜¯åˆ†å€ç­–ç•¥çš„è€ƒé‡
- âŒ åŸºæ•¸ï¼ˆcardinalityï¼‰æŒ‡çš„æ˜¯ä¸åŒå€¼çš„æ•¸é‡ï¼Œèˆ‡è³‡æ–™å¤§å°ç„¡é—œ

**é¸é … B - æ¨™è¨˜ column è¶³å¤ å°ï¼š**
- âŒ **ä½œç”¨å°è±¡éŒ¯èª¤**ï¼šåŒæ¨£èª¤èªç‚ºä½œç”¨æ–¼ column
- âŒ å–®ä¸€ column ç„¡æ³•å–®ç¨ broadcastï¼›å¿…é ˆæ˜¯å®Œæ•´çš„ DataFrame

**é¸é … C - å¿«å–åˆ°æ‰€æœ‰å¢é›†çš„å„²å­˜å·ï¼š**
- âŒ **ç¯„åœéŒ¯èª¤**ï¼šbroadcast åªåœ¨**å–®ä¸€å¢é›†**å…§ç”Ÿæ•ˆï¼Œä¸è·¨å¢é›†
- âŒ **å„²å­˜ä½ç½®éŒ¯èª¤**ï¼šæ˜¯å¿«å–åˆ° **executor è¨˜æ†¶é«”**ï¼Œä¸æ˜¯ã€Œattached storage volumesã€
- âŒ **æ¦‚å¿µæ··æ·†**ï¼šèˆ‡ Databricks çš„è·¨å·¥ä½œå€è³‡æ–™å…±äº«æ©Ÿåˆ¶æ··æ·†

**é¸é … E - å¿«å–åˆ°æ‰€æœ‰ç¯€é»ä¾›æœªä¾†æŸ¥è©¢ä½¿ç”¨ï¼š**
- âŒ **ç”Ÿå‘½é€±æœŸéŒ¯èª¤**ï¼šbroadcast çš„è³‡æ–™åªåœ¨**ç•¶å‰æŸ¥è©¢**ä¸­æœ‰æ•ˆ
- âš ï¸ **èˆ‡ cache/persist æ··æ·†**ï¼š
  - `cache()`/`persist()`ï¼šå¿«å– DataFrame ä¾›åŒä¸€ session å…§å¤šæ¬¡ä½¿ç”¨
  - `broadcast()`ï¼šåƒ…åœ¨å–®æ¬¡ join æ“ä½œä¸­å»£æ’­è³‡æ–™
- âŒ ä¸æœƒåœ¨ã€Œcluster lifetimeã€å…§æŒçºŒå­˜åœ¨

---

### broadcast() vs cache() vs persist() æ¯”è¼ƒ

| åŠŸèƒ½ | broadcast() | cache() | persist() |
|------|-------------|---------|-----------|
| **ç›®çš„** | å„ªåŒ– join æ“ä½œ | é‡è¤‡ä½¿ç”¨ DataFrame | é‡è¤‡ä½¿ç”¨ DataFrameï¼ˆå¯é¸å„²å­˜ç´šåˆ¥ï¼‰ |
| **ä½œç”¨æ™‚æ©Ÿ** | å–®æ¬¡ join æŸ¥è©¢ | å¤šæ¬¡æŸ¥è©¢ | å¤šæ¬¡æŸ¥è©¢ |
| **è³‡æ–™åˆ†ä½ˆ** | è¤‡è£½åˆ°æ‰€æœ‰ executor | ä¾åˆ†å€å¿«å– | ä¾åˆ†å€å¿«å– |
| **é©ç”¨å ´æ™¯** | å°è¡¨ join å¤§è¡¨ | éœ€å¤šæ¬¡è®€å–çš„ä¸­é–“çµæœ | åŒå·¦ï¼Œä½†å¯è‡ªè¨‚å„²å­˜ç­–ç•¥ |
| **ç”Ÿå‘½é€±æœŸ** | ç•¶å‰æŸ¥è©¢ | ç›´åˆ° unpersist æˆ– session çµæŸ | åŒå·¦ |

---

### è¨˜æ†¶å£è¨£
- **ã€Œbroadcast = å°è¡¨å…¨å»£æ’­ã€**ï¼šå°‡å° DataFrame å®Œæ•´è¤‡è£½åˆ°æ‰€æœ‰ executor è¨˜æ†¶é«”ï¼Œé¿å… shuffleã€‚
- **ã€Œä½œç”¨å°è±¡ï¼šDataFrameï¼Œä¸æ˜¯ columnã€**ï¼šè¨˜ä½æ˜¯æ¨™è¨˜æ•´å€‹è¡¨ï¼Œä¸æ˜¯å–®ä¸€æ¬„ä½ã€‚

### è§£é¡Œæ­¥é©Ÿ
1. **è­˜åˆ¥ä½œç”¨å°è±¡**ï¼šçœ‹åˆ° `broadcast()` â†’ ä½œç”¨æ–¼ **DataFrame**ï¼Œå…ˆæ’é™¤ Aã€B
2. **ç¢ºèªç¯„åœ**ï¼šåªåœ¨ç•¶å‰å¢é›†èˆ‡ç•¶å‰æŸ¥è©¢æœ‰æ•ˆ â†’ æ’é™¤ Cã€E
3. **é©—è­‰ç›®çš„**ï¼šæ˜¯ç‚ºäº†å„ªåŒ– joinï¼Œå°‡å°è¡¨å­˜å…¥ executor è¨˜æ†¶é«” â†’ é¸ D

### å¸¸è¦‹é™·é˜±è­¦ç¤º
- âš ï¸ **æ··æ·† column èˆ‡ DataFrame**ï¼šbroadcast ä¸æ“ä½œå–®ä¸€æ¬„ä½
- âš ï¸ **èˆ‡ cache/persist æ··æ·†**ï¼šbroadcast åªåœ¨ join æ™‚æœ‰æ•ˆï¼Œä¸æœƒæŒçºŒå¿«å–
- âš ï¸ **èª¤ä»¥ç‚ºè·¨å¢é›†ç”Ÿæ•ˆ**ï¼šbroadcast åªåœ¨å–®ä¸€å¢é›†å…§æœ‰æ•ˆ

---

### å¯¦æˆ°ç¯„ä¾‹èˆ‡æœ€ä½³å¯¦è¸

**ä½•æ™‚ä½¿ç”¨ broadcast joinï¼š**
```python
# æƒ…å¢ƒï¼šå¤§äº‹å¯¦è¡¨ (1TB) join å°ç¶­åº¦è¡¨ (5MB)
fact_df = spark.table("sales_fact")  # 1TB
dim_df = spark.table("product_dim")   # 5MB

# âœ… æ¨è–¦ï¼šæ˜ç¢ºä½¿ç”¨ broadcast
result = fact_df.join(broadcast(dim_df), "product_id")

# âŒ ä¸æ¨è–¦ï¼šSpark å¯èƒ½é¸æ“‡ SortMergeJoinï¼Œå°è‡´ shuffle
result = fact_df.join(dim_df, "product_id")
```

**æª¢æŸ¥æ˜¯å¦çœŸæ­£ä½¿ç”¨ broadcast joinï¼š**
```python
# æŸ¥çœ‹åŸ·è¡Œè¨ˆç•«
result.explain()

# æ‡‰è©²çœ‹åˆ°ï¼šBroadcastHashJoin
# å¦‚æœçœ‹åˆ° SortMergeJoinï¼Œè¡¨ç¤º broadcast æœªç”Ÿæ•ˆ
```

**èª¿æ•´ broadcast é–¾å€¼ï¼š**
```python
# é è¨­ 10MBï¼Œå¯èª¿æ•´ç‚º 20MB
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 20 * 1024 * 1024)

# æˆ–é—œé–‰è‡ªå‹• broadcastï¼ˆå¼·åˆ¶ä½¿ç”¨æ‰‹å‹•æ¨™è¨˜ï¼‰
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
```

---

## ğŸ·ï¸ æ¨™ç±¤èˆ‡åˆ†é¡
**Topics:** `PySpark`, `Spark-Joins`, `Spark-Performance`
**Traps:** `Similar-Function`, `Concept-Confusion`
**Difficulty:** `L2-Intermediate`
