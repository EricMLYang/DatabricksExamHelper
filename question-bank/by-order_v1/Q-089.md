# Q-089

## é¡Œç›®è³‡è¨Š

**ID:** `Q-089`

**ä¾†æº:** Mock Exam

**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A Databricks job has been configured with 3 tasks, each of which is a Databricks notebook. Task A does not depend on other tasks. Tasks B and C run in parallel, with each having a serial dependency on Task A.

If task A fails during a scheduled run, which statement describes the results of this run?

### é¸é …

- **A.** Because all tasks are managed as a dependency graph, no changes will be committed to the Lakehouse until all tasks have successfully been completed.
- **B.** Tasks B and C will attempt to run as configured; any changes made in task A will be rolled back due to task failure.
- **C.** Unless all tasks complete successfully, no changes will be committed to the Lakehouse; because task A failed, all commits will be rolled back automatically.
- **D.** Tasks B and C will be skipped; some logic expressed in task A may have been committed before task failure.
- **E.** Tasks B and C will not commit any changes because of stage failure.

---

## æ¨™ç±¤ç³»çµ±

**Topics:** `Databricks-Workflows`, `Job-Orchestration`, `Error-Handling`

**Traps:** `Transaction-Misconception`, `Rollback-Assumption`

**Domain:** `ç¶­é‹èˆ‡è‡ªå‹•åŒ– (Operations & Automation)`

---

## ç­”æ¡ˆèˆ‡åˆ†æ

### æ­£ç¢ºç­”æ¡ˆ

**æ­£è§£:** `D`

**ç¤¾ç¾¤æŠ•ç¥¨:** D
**ä¾†æºæ¨™è¨»:** C

**æ·±å…¥æ´å¯Ÿ:** ä¾†æºæ¨™è¨»ç­”æ¡ˆ C è¢«ç¤¾ç¾¤å°ˆå®¶ï¼ˆmouad_attaqi, aragorn_bregoï¼‰æŒ‡å‡ºæ˜¯éŒ¯èª¤çš„ã€‚Databricks Workflows **æ²’æœ‰è‡ªå‹•å›æ»¾æ©Ÿåˆ¶**ï¼Œå¤±æ•—çš„ä»»å‹™å¯èƒ½å·²ç¶“æäº¤äº†éƒ¨åˆ†è®Šæ›´ã€‚

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Databricks Workflows (Jobs) ä»»å‹™ä¾è³´èˆ‡å¤±æ•—è™•ç†
**çŸ¥è­˜é ˜åŸŸ:** ç¶­é‹èˆ‡è‡ªå‹•åŒ– - Job ç·¨æ’èˆ‡éŒ¯èª¤è™•ç†
**é—œéµæ¦‚å¿µ:** 
- ä»»å‹™ä¾è³´åœ– (Task Dependency Graph)
- å¤±æ•—ä»»å‹™çš„ä¸‹æ¸¸è¡Œç‚º
- Databricks Workflows **ç„¡è‡ªå‹•å›æ»¾**æ©Ÿåˆ¶

### æ¬¡è¦è€ƒé»
- å€åˆ†ã€Œå·¥ä½œæµç¨‹ç·¨æ’ã€èˆ‡ã€Œè³‡æ–™åº«äº‹å‹™ã€
- ç†è§£ Databricks Jobs çš„åŸ·è¡Œæ¨¡å‹

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æŠ€è¡“åŸç†:**

Databricks Workflows (Jobs) çš„å¤±æ•—è™•ç†æ©Ÿåˆ¶ï¼š

1. **ä¾è³´åœ–åŸ·è¡Œé‚è¼¯**
   - ç•¶æŸå€‹ä»»å‹™å¤±æ•—æ™‚ï¼Œæ‰€æœ‰ä¾è³´è©²ä»»å‹™çš„ä¸‹æ¸¸ä»»å‹™æœƒè¢«**è‡ªå‹•è·³é (skipped)**
   - é€™æ˜¯å·¥ä½œæµç¨‹ç·¨æ’çš„æ¨™æº–è¡Œç‚º

2. **ç„¡è‡ªå‹•å›æ»¾æ©Ÿåˆ¶** âš ï¸
   - Databricks Jobs **ä¸æ˜¯è³‡æ–™åº«äº‹å‹™ç³»çµ±**
   - å¤±æ•—çš„ä»»å‹™å¯èƒ½å·²ç¶“åŸ·è¡Œäº†éƒ¨åˆ†ç¨‹å¼ç¢¼
   - å·²ç¶“æäº¤çš„è®Šæ›´ï¼ˆå¦‚å¯«å…¥ Delta Lakeï¼‰**ä¸æœƒè‡ªå‹•å›æ»¾**

3. **ä»»å‹™å¤±æ•—çš„ç‹€æ…‹**
   - å¤±æ•—ä»»å‹™ï¼šæ¨™è¨˜ç‚º `Failed`
   - ä¸‹æ¸¸ä»»å‹™ï¼šæ¨™è¨˜ç‚º `Skipped`ï¼ˆå› ç‚ºä¸Šæ¸¸ä¾è³´å¤±æ•—ï¼‰

**ç¬¦åˆé¡Œç›®æƒ…å¢ƒ:**

```
ä»»å‹™ä¾è³´åœ–ï¼š
    Task A (root task)
       â†“
    â”Œâ”€â”€â”´â”€â”€â”
    â†“     â†“
 Task B  Task C (ä¸¦è¡ŒåŸ·è¡Œï¼Œéƒ½ä¾è³´ A)
```

**ç•¶ Task A å¤±æ•—æ™‚ï¼š**

```python
# Task A åŸ·è¡Œéç¨‹ä¸­å¤±æ•—çš„ç¯„ä¾‹
# Task A notebook:

# æ­¥é©Ÿ 1: è®€å–è³‡æ–™ âœ… æˆåŠŸ
df = spark.read.csv("/source/data.csv")

# æ­¥é©Ÿ 2: è½‰æ›è³‡æ–™ âœ… æˆåŠŸ
cleaned_df = df.filter(col("value").isNotNull())

# æ­¥é©Ÿ 3: å¯«å…¥ Bronze å±¤ âœ… æˆåŠŸæäº¤ï¼
cleaned_df.write.format("delta").mode("append").save("/mnt/bronze/table1")

# æ­¥é©Ÿ 4: åŸ·è¡Œè¤‡é›œè½‰æ› âŒ å¤±æ•—ï¼ˆä¾‹å¦‚é™¤ä»¥é›¶ï¼‰
result_df = cleaned_df.withColumn("ratio", col("numerator") / col("denominator"))
# â†’ ZeroDivisionError! Task A å¤±æ•—

# çµæœï¼š
# - æ­¥é©Ÿ 3 çš„å¯«å…¥å·²ç¶“æäº¤åˆ° Delta Lakeï¼ˆç„¡æ³•å›æ»¾ï¼‰âœ“
# - Task B å’Œ C è¢«è·³é
```

**åŸ·è¡Œçµæœï¼š**
- âŒ Task A: `Failed`ï¼ˆä½†éƒ¨åˆ†è®Šæ›´å·²æäº¤ï¼‰
- â­ï¸ Task B: `Skipped`ï¼ˆå› ç‚ºä¾è³´çš„ Task A å¤±æ•—ï¼‰
- â­ï¸ Task C: `Skipped`ï¼ˆå› ç‚ºä¾è³´çš„ Task A å¤±æ•—ï¼‰

**å¯¦å‹™æ‡‰ç”¨:**

åœ¨ Databricks Workflows ä¸­è¨­è¨ˆå®¹éŒ¯æ©Ÿåˆ¶ï¼š

```python
# æœ€ä½³å¯¦è¸ 1: ä½¿ç”¨ try-except è™•ç†éŒ¯èª¤
try:
    # åŸ·è¡Œå¯èƒ½å¤±æ•—çš„é‚è¼¯
    risky_transformation()
    write_to_delta()
except Exception as e:
    # è¨˜éŒ„éŒ¯èª¤ä½†ä¸è®“æ•´å€‹ä»»å‹™å¤±æ•—
    log_error(e)
    # å¯é¸ï¼šåŸ·è¡Œè£œæ•‘é‚è¼¯
    write_error_record(e)
    raise  # é‡æ–°æ‹‹å‡ºä»¥æ¨™è¨˜ä»»å‹™å¤±æ•—

# æœ€ä½³å¯¦è¸ 2: ä½¿ç”¨å†ªç­‰æ€§è¨­è¨ˆ
# ç¢ºä¿é‡è¤‡åŸ·è¡Œä¸æœƒç”¢ç”Ÿé‡è¤‡è³‡æ–™
df.write.format("delta") \
    .mode("overwrite") \  # æˆ–ä½¿ç”¨ MERGE
    .option("replaceWhere", "date = '2024-01-01'") \
    .save("/mnt/data/table")
```

**Job UI é¡¯ç¤ºç¯„ä¾‹ï¼š**

```
Job Run #123
â”œâ”€ Task A: âŒ Failed (10:23:45 AM)
â”‚  â””â”€ Error: ZeroDivisionError
â”‚  â””â”€ Duration: 3m 45s
â”‚  â””â”€ âš ï¸ å¯èƒ½æœ‰éƒ¨åˆ†è®Šæ›´å·²æäº¤
â”‚
â”œâ”€ Task B: â­ï¸ Skipped
â”‚  â””â”€ Reason: Upstream task failed
â”‚
â””â”€ Task C: â­ï¸ Skipped
   â””â”€ Reason: Upstream task failed
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - No changes until all tasks complete

**éŒ¯èª¤åŸå› :** Databricks Workflows **ä¸æœƒç­‰æ‰€æœ‰ä»»å‹™å®Œæˆæ‰æäº¤è®Šæ›´**

**è©³ç´°åˆ†æ:**
- é€™å€‹é¸é …æš—ç¤ºæœ‰ã€Œtwo-phase commitã€æ©Ÿåˆ¶ï¼Œä½† Databricks Jobs ä¸¦éå¦‚æ­¤
- æ¯å€‹ä»»å‹™**ç¨ç«‹åŸ·è¡Œ**ï¼Œå¯«å…¥æ“ä½œæœƒç«‹å³æäº¤
- ä»»å‹™ä¹‹é–“æ²’æœ‰è·¨ä»»å‹™çš„äº‹å‹™é‚Šç•Œ

**èª¤è§£ä¾†æºï¼š**
å¯èƒ½æ··æ·†äº†ã€Œä¾è³´åœ–ã€èˆ‡ã€Œäº‹å‹™ç³»çµ±ã€ï¼š
- **ä¾è³´åœ–ï¼š** æ§åˆ¶åŸ·è¡Œé †åºï¼Œä¸ä¿è­‰åŸå­æ€§
- **äº‹å‹™ç³»çµ±ï¼š** ä¿è­‰ ACID ç‰¹æ€§ï¼Œå¯å›æ»¾

**å¯¦éš›è¡Œç‚ºï¼š**
```python
# Task A åŸ·è¡Œæµç¨‹
spark.sql("CREATE TABLE bronze.users ...").show()  # âœ… ç«‹å³æäº¤
spark.sql("INSERT INTO bronze.users ...").show()   # âœ… ç«‹å³æäº¤
# ç™¼ç”ŸéŒ¯èª¤ âŒ
# â†’ ä¸Šé¢çš„ CREATE å’Œ INSERT å·²ç¶“ç”Ÿæ•ˆï¼Œç„¡æ³•å›æ»¾
```

---

### é¸é … B - Tasks B and C will attempt to run; task A changes rolled back

**éŒ¯èª¤åŸå› :** æœ‰å…©å€‹éŒ¯èª¤ï¼š
1. Tasks B å’Œ C **ä¸æœƒåŸ·è¡Œ**ï¼ˆæœƒè¢«è·³éï¼‰
2. Task A çš„è®Šæ›´**ä¸æœƒè‡ªå‹•å›æ»¾**

**è©³ç´°åˆ†æ:**

**éŒ¯èª¤ 1 - ä¸‹æ¸¸ä»»å‹™åŸ·è¡Œè¡Œç‚ºï¼š**
- Databricks Workflows çš„é è¨­è¡Œç‚ºæ˜¯ï¼šç•¶ä¾è³´ä»»å‹™å¤±æ•—æ™‚ï¼Œ**è‡ªå‹•è·³éæ‰€æœ‰ä¸‹æ¸¸ä»»å‹™**
- é€™æ˜¯ã€Œå¿«é€Ÿå¤±æ•— (fail-fast)ã€ç­–ç•¥ï¼Œé¿å…æµªè²»è³‡æºåŸ·è¡Œæ³¨å®šå¤±æ•—çš„ä»»å‹™

**éŒ¯èª¤ 2 - å›æ»¾æ©Ÿåˆ¶ï¼š**
- Databricks **æ²’æœ‰è·¨ä»»å‹™çš„å›æ»¾æ©Ÿåˆ¶**
- å³ä½¿å–®å€‹ Delta Lake å¯«å…¥æ“ä½œæ˜¯åŸå­æ€§çš„ï¼Œä½†ä»»å‹™å¤±æ•—å¾Œ**ä¸æœƒè‡ªå‹•æ’¤éŠ·å·²æäº¤çš„è®Šæ›´**

**å°æ¯”æ­£ç¢ºè¡Œç‚ºï¼š**

| éšæ®µ | é¸é … B è²ç¨± | å¯¦éš›è¡Œç‚º |
|------|-----------|---------|
| Task A å¤±æ•—å¾Œ | B å’Œ C å˜—è©¦åŸ·è¡Œ | âŒ B å’Œ C è¢«è·³é |
| Task A è®Šæ›´ | è‡ªå‹•å›æ»¾ | âŒ ä¸æœƒå›æ»¾ |

---

### é¸é … C - All commits rolled back automatically (ä¾†æºæ¨™è¨»ç­”æ¡ˆ)

**éŒ¯èª¤åŸå› :** Databricks Workflows **æ²’æœ‰è‡ªå‹•å›æ»¾æ©Ÿåˆ¶**

**è©³ç´°åˆ†æ:**

**ç‚ºä½•æ­¤é¸é …éŒ¯èª¤ï¼š**

1. **ç„¡å…¨åŸŸäº‹å‹™é‚Šç•Œ**
   - Databricks Jobs ä¸åƒè³‡æ–™åº«äº‹å‹™é‚£æ¨£æœ‰ BEGIN/COMMIT/ROLLBACK
   - æ¯å€‹å¯«å…¥æ“ä½œï¼ˆå¦‚ `df.write`ï¼‰æ˜¯ç¨ç«‹æäº¤çš„

2. **Delta Lake çš„äº‹å‹™æ€§åƒ…é™æ–¼å–®æ¬¡å¯«å…¥**
   - Delta Lake ä¿è­‰**å–®æ¬¡å¯«å…¥æ“ä½œ**çš„åŸå­æ€§
   - ä½†**ä¸ä¿è­‰è·¨å¤šæ¬¡å¯«å…¥**çš„åŸå­æ€§

3. **å·¥ä½œæµç¨‹ â‰  äº‹å‹™ç³»çµ±**
   ```
   è³‡æ–™åº«äº‹å‹™ï¼š
   BEGIN TRANSACTION
      INSERT INTO table1 ...
      UPDATE table2 ...
      DELETE FROM table3 ...
   COMMIT -- å…¨éƒ¨æˆåŠŸæ‰æäº¤
   ROLLBACK -- å¤±æ•—å‰‡å…¨éƒ¨å›æ»¾
   
   Databricks Workflowï¼š
   Task A:
      df1.write.save(...)  # âœ… ç«‹å³æäº¤
      df2.write.save(...)  # âœ… ç«‹å³æäº¤
      raise Exception      # âŒ å¤±æ•—
   # â†’ å‰é¢çš„å¯«å…¥å·²ç”Ÿæ•ˆï¼Œç„¡æ³•å›æ»¾
   ```

**ç‚ºä½•ç¤¾ç¾¤èªç‚º C æ˜¯éŒ¯èª¤çš„ï¼š**

æ ¹æ“šå°ˆå®¶è¨è«–ï¼š
- **mouad_attaqi:** "Databricks does not automatically roll back changes"
- **aragorn_brego:** "Task failures don't trigger automatic rollback"
- å®˜æ–¹æ–‡ä»¶ä¹Ÿæ²’æœ‰æåŠè‡ªå‹•å›æ»¾åŠŸèƒ½

**å¯¦éš›æ¡ˆä¾‹ï¼š**

```python
# Task A å¤±æ•—ç¯„ä¾‹
# å¯«å…¥ 1: âœ… æˆåŠŸ
spark.sql("""
    INSERT INTO bronze.orders
    SELECT * FROM source_orders
    WHERE date = '2024-01-01'
""")

# å¯«å…¥ 2: âœ… æˆåŠŸ
spark.sql("""
    INSERT INTO bronze.customers
    SELECT * FROM source_customers
    WHERE date = '2024-01-01'
""")

# å¯«å…¥ 3: âŒ å¤±æ•—ï¼ˆä¾‹å¦‚è³‡æ–™æ ¼å¼éŒ¯èª¤ï¼‰
spark.sql("""
    INSERT INTO bronze.products
    SELECT * FROM invalid_source
""")

# çµæœï¼š
# - orders å’Œ customers è¡¨å·²æœ‰æ–°è³‡æ–™ âœ“
# - products è¡¨æ²’æœ‰æ–°è³‡æ–™ âœ“
# - å‰å…©å€‹å¯«å…¥ä¸æœƒå›æ»¾ âœ“
```

**æ˜“æ··æ·†é»:**
æ­¤é¸é …æœ€å®¹æ˜“èª¤é¸ï¼Œå› ç‚ºå¾ç›´è¦ºä¸Šã€Œå¤±æ•—æ‡‰è©²å›æ»¾ã€ï¼Œä½† Databricks Workflows ä¸æ˜¯é€™æ¨£è¨­è¨ˆçš„

---

### é¸é … E - Tasks B and C will be skipped; task A will not commit

**éŒ¯èª¤åŸå› :** å‰åŠéƒ¨æ­£ç¢ºï¼Œä½†å¾ŒåŠéƒ¨éŒ¯èª¤ - Task A **å¯èƒ½å·²ç¶“æäº¤äº†éƒ¨åˆ†è®Šæ›´**

**è©³ç´°åˆ†æ:**

**æ­£ç¢ºéƒ¨åˆ†ï¼š**
- âœ… "Tasks B and C will be skipped" - é€™æ˜¯å°çš„

**éŒ¯èª¤éƒ¨åˆ†ï¼š**
- âŒ "task A will not commit any changes because of stage failure" - é€™æ˜¯éŒ¯çš„
- ä»»å‹™å¤±æ•—**ä¸ä»£è¡¨æ²’æœ‰ä»»ä½•æäº¤**
- å¤±æ•—å¯èƒ½ç™¼ç”Ÿåœ¨åŸ·è¡Œéç¨‹ä¸­çš„ä»»ä½•éšæ®µ

**å°æ¯”é¸é … D å’Œ Eï¼š**

| é¸é … | é—œæ–¼ B/C | é—œæ–¼ Task A è®Šæ›´ | æ˜¯å¦æ­£ç¢º |
|------|---------|----------------|---------|
| D | æœƒè¢«è·³é âœ“ | å¯èƒ½å·²æäº¤éƒ¨åˆ†è®Šæ›´ âœ“ | âœ… å®Œå…¨æ­£ç¢º |
| E | æœƒè¢«è·³é âœ“ | ä¸æœƒæäº¤ä»»ä½•è®Šæ›´ âœ— | âŒ éƒ¨åˆ†éŒ¯èª¤ |

**ç‚ºä½• E éŒ¯èª¤çš„å¯¦ä¾‹ï¼š**

```python
# Task A notebook åŸ·è¡Œé †åº
# Step 1-5 åŸ·è¡ŒæˆåŠŸï¼ŒStep 6 å¤±æ•—

# Step 1: âœ… å¯«å…¥æˆåŠŸ
df1.write.format("delta").save("/bronze/table1")

# Step 2: âœ… å¯«å…¥æˆåŠŸ
df2.write.format("delta").save("/bronze/table2")

# Step 3: âœ… å¯«å…¥æˆåŠŸ
df3.write.format("delta").save("/bronze/table3")

# Step 4: âœ… åŸ·è¡ŒæˆåŠŸ
spark.sql("OPTIMIZE bronze.table1")

# Step 5: âœ… åŸ·è¡ŒæˆåŠŸ
spark.sql("ANALYZE TABLE bronze.table2 COMPUTE STATISTICS")

# Step 6: âŒ å¤±æ•—ï¼ˆä¾‹å¦‚è¨˜æ†¶é«”ä¸è¶³ï¼‰
huge_df = spark.read.parquet("/huge/dataset")  # OutOfMemoryError

# çµæœï¼š
# - é¸é … E èªªã€Œä¸æœƒæäº¤ä»»ä½•è®Šæ›´ã€ âŒ éŒ¯ï¼
# - å¯¦éš›ä¸Š table1, table2, table3 éƒ½æœ‰æ–°è³‡æ–™ âœ“
# - é¸é … D èªªã€Œå¯èƒ½å·²æäº¤éƒ¨åˆ†è®Šæ›´ã€ âœ… æ­£ç¢ºï¼
```

**é—œéµå·®ç•°ï¼š**
- é¸é … E å‡è¨­ï¼šå¤±æ•— = ç„¡ä»»ä½•è®Šæ›´ï¼ˆå¤ªçµ•å°ï¼‰
- é¸é … D æ­£ç¢ºï¼šå¤±æ•— = å¯èƒ½æœ‰éƒ¨åˆ†è®Šæ›´ï¼ˆç¬¦åˆå¯¦éš›ï¼‰

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### å£è¨£
**ã€ŒWorkflows ç·¨æ’éäº‹å‹™ï¼Œä¾è³´å¤±æ•—è·³ä¸‹æ¸¸ï¼Œå·²æäº¤çš„ä¸å›æ»¾ã€**

### é—œéµå°æ¯”è¡¨

| ç‰¹æ€§ | è³‡æ–™åº«äº‹å‹™ | Databricks Workflows |
|------|-----------|---------------------|
| åŸå­æ€§ç¯„åœ | BEGIN...COMMIT ä¹‹é–“ | åƒ…å–®æ¬¡å¯«å…¥æ“ä½œ |
| å¤±æ•—è™•ç† | å…¨éƒ¨å›æ»¾ (ROLLBACK) | âŒ ä¸è‡ªå‹•å›æ»¾ |
| ä¾è³´ä»»å‹™ | N/A | ä¾è³´å¤±æ•—å‰‡è·³é |
| é©ç”¨å ´æ™¯ | éŠ€è¡Œè½‰å¸³ã€è¨‚å–®è™•ç† | è³‡æ–™ç®¡ç·šç·¨æ’ |

### ä»»å‹™å¤±æ•—è¡Œç‚ºè¨˜æ†¶åœ–

```
Task A åŸ·è¡Œéç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: è®€å–è³‡æ–™       âœ…        â”‚
â”‚ Step 2: è½‰æ›è³‡æ–™       âœ…        â”‚
â”‚ Step 3: å¯«å…¥ Delta     âœ… æäº¤ï¼  â”‚
â”‚ Step 4: è¤‡é›œé‹ç®—       âŒ å¤±æ•—   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    çµæœåˆ¤æ–·ï¼š
    â”œâ”€ Task A: Failed âŒ
    â”œâ”€ Step 3 çš„å¯«å…¥: å·²æäº¤ âœ“ï¼ˆä¸æœƒå›æ»¾ï¼‰
    â”œâ”€ Task B: Skipped â­ï¸
    â””â”€ Task C: Skipped â­ï¸

æ­£ç¢ºç­”æ¡ˆï¼šD âœ…
ã€ŒTasks B and C will be skipped; 
 some logic may have been committedã€
```

### åˆ¤æ–·æŠ€å·§

**è­˜åˆ¥é—œéµå­—ï¼š**
- âŒ "rolled back automatically" â†’ å¤ªç†æƒ³åŒ–ï¼ŒDatabricks æ²’æœ‰é€™åŠŸèƒ½
- âŒ "no changes will be committed" â†’ å¤ªçµ•å°ï¼Œéƒ¨åˆ†è®Šæ›´å¯èƒ½å·²æäº¤
- âœ… "will be skipped" â†’ æ­£ç¢ºï¼Œä¾è³´å¤±æ•—å°è‡´ä¸‹æ¸¸è·³é
- âœ… "may have been committed" â†’ æ­£ç¢ºï¼Œå…è¨±éƒ¨åˆ†æäº¤çš„å¯èƒ½æ€§

**é¿å…èª¤é¸ C çš„æ€ç¶­ï¼š**

```
âŒ éŒ¯èª¤æ€ç¶­ï¼ˆå°è‡´é¸ Cï¼‰ï¼š
ã€Œå¤±æ•—äº†æ‡‰è©²è¦å›æ»¾ï¼Œé€™æ¨£æ‰å®‰å…¨ã€
â†’ é€™æ˜¯è³‡æ–™åº«æ€ç¶­ï¼Œä¸é©ç”¨æ–¼å·¥ä½œæµç¨‹

âœ… æ­£ç¢ºæ€ç¶­ï¼ˆé¸ Dï¼‰ï¼š
ã€ŒWorkflows æ˜¯ç·¨æ’å·¥å…·ï¼Œä¸æ˜¯äº‹å‹™ç³»çµ±ã€
ã€Œæ¯å€‹å¯«å…¥æ“ä½œç¨ç«‹æäº¤ã€
ã€Œå¤±æ•—åªæœƒå½±éŸ¿å¾ŒçºŒæ­¥é©Ÿçš„åŸ·è¡Œã€
```

### å¯¦å‹™æœ€ä½³å¯¦è¸

**è¨­è¨ˆå†ªç­‰æ€§ä»»å‹™ï¼š**

```python
# ä¸è‰¯è¨­è¨ˆ - é‡è¤‡åŸ·è¡Œæœƒç”¢ç”Ÿé‡è¤‡è³‡æ–™
df.write.format("delta").mode("append").save("/table")

# è‰¯å¥½è¨­è¨ˆ - å¯é‡è¤‡åŸ·è¡Œ
df.write.format("delta") \
    .mode("overwrite") \
    .option("replaceWhere", f"date = '{date}'") \
    .save("/table")

# æˆ–ä½¿ç”¨ MERGE
spark.sql(f"""
    MERGE INTO target t
    USING source s
    ON t.id = s.id AND t.date = '{date}'
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
""")
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

### å®˜æ–¹æ–‡ä»¶
- [Databricks Workflows - Task dependencies](https://docs.databricks.com/workflows/jobs/jobs-tasks.html#task-dependencies)
- [Databricks Workflows - Error handling](https://docs.databricks.com/workflows/jobs/jobs.html#error-handling)
- [Delta Lake - ACID guarantees](https://docs.databricks.com/delta/acid-guarantees.html)

### ç›¸é—œæ¦‚å¿µ
- Job å¤±æ•—é‡è©¦ç­–ç•¥ (Retry Policy)
- Job å‘Šè­¦é€šçŸ¥ (Alerting)
- å†ªç­‰æ€§è¨­è¨ˆæ¨¡å¼ (Idempotent Design)
- Delta Lake äº‹å‹™æ—¥èªŒ

### å¯¦å‹™æŠ€å·§

**1. ä»»å‹™å¤±æ•—è™•ç†ç­–ç•¥ï¼š**

```python
# åœ¨ Task Notebook ä¸­åŠ å…¥éŒ¯èª¤è™•ç†
dbutils.notebook.run(
    "/path/to/task",
    timeout_seconds=3600,
    arguments={"date": "2024-01-01"}
)

# è¨­å®šé‡è©¦ç­–ç•¥ï¼ˆåœ¨ Job UI é…ç½®ï¼‰
# - Max retries: 3
# - Retry interval: 5 minutes
# - Timeout: 1 hour
```

**2. Job é…ç½®ç¯„ä¾‹ (JSON):**

```json
{
  "name": "ETL Pipeline",
  "tasks": [
    {
      "task_key": "task_a",
      "notebook_task": {
        "notebook_path": "/Workflows/Task_A"
      },
      "max_retries": 2,
      "timeout_seconds": 3600
    },
    {
      "task_key": "task_b",
      "depends_on": [{"task_key": "task_a"}],
      "notebook_task": {
        "notebook_path": "/Workflows/Task_B"
      }
    },
    {
      "task_key": "task_c",
      "depends_on": [{"task_key": "task_a"}],
      "notebook_task": {
        "notebook_path": "/Workflows/Task_C"
      }
    }
  ],
  "email_notifications": {
    "on_failure": ["team@company.com"]
  }
}
```

**3. ç›£æ§èˆ‡å‘Šè­¦ï¼š**

```python
# ä½¿ç”¨ Databricks REST API æŸ¥è©¢ Job ç‹€æ…‹
import requests

response = requests.get(
    f"{databricks_host}/api/2.1/jobs/runs/get",
    headers={"Authorization": f"Bearer {token}"},
    params={"run_id": run_id}
)

run_status = response.json()["state"]["life_cycle_state"]
if run_status == "INTERNAL_ERROR":
    send_alert("Job failed - check for partial commits")
```

---

## ğŸ”— ç›¸é—œé¡Œç›®
- Q-XXX: Databricks Workflows é‡è©¦ç­–ç•¥
- Q-XXX: Delta Lake ACID äº‹å‹™ä¿è­‰
- Q-XXX: Job ä¾è³´åœ–è¨­è¨ˆæœ€ä½³å¯¦è¸
- Q-XXX: å†ªç­‰æ€§è³‡æ–™ç®¡ç·šè¨­è¨ˆ
