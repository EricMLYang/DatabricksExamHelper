# Question #106

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-106`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L3-Advanced`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A nightly batch job is configured to ingest all data files from a cloud object storage container where records are stored in a nested directory structure `YYYY/MM/DD`. The data for each date represents all records that were processed by the source system on that date, noting that **some records may be delayed as they await moderator approval**. 

Each entry represents a user review of a product and has the following schema:
```
user_id STRING, review_id BIGINT, product_id BIGINT, review_timestamp TIMESTAMP, review_text STRING
```

The ingestion job is configured to **append all data for the previous date** to a target table `reviews_raw` with an identical schema to the source system. The next step in the pipeline is a batch write to propagate all new records inserted into `reviews_raw` to a table where data is fully deduplicated, validated, and enriched.

Which solution **minimizes the compute costs** to propagate this batch of data?

### é¸é …

- **A.** Perform a batch read on the reviews_raw table and perform an insert-only merge using the natural composite key user_id, review_id, product_id, review_timestamp.
- **B.** Configure a Structured Streaming read against the reviews_raw table using the trigger once execution mode to process new records as a batch job.
- **C.** Use Delta Lake version history to get the difference between the latest version of reviews_raw and one version prior, then write these records to the next table.
- **D.** Filter all records in the reviews_raw table based on the review_timestamp; batch append those records produced in the last 48 hours.
- **E.** Reprocess all records in reviews_raw and overwrite the next table in the pipeline.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Delta-Lake`, `Streaming`, `Incremental-Processing`, `Cost-Optimization`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Version-History-Misuse`, `Streaming-vs-Batch`, `Merge-Complexity`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Performance & Cost Optimization

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B` (ç¤¾ç¾¤å…±è­˜) / `C` (ä¾†æºæ¨™è¨»ï¼Œæœ‰çˆ­è­°)

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** C
- **ç¤¾ç¾¤æŠ•ç¥¨ç­”æ¡ˆ:** A (55%) / B (45%)
- **æ·±å…¥æ´å¯Ÿ:** é€™é“é¡Œç›®å­˜åœ¨çˆ­è­°ï¼Œä¾†æºæ¨™è¨»ç­”æ¡ˆèˆ‡ç¤¾ç¾¤è§€é»ä¸åŒ

---

# é¡Œç›®è§£æ

---

## é¡Œç›®å›é¡§

### é¡Œç›®ç·¨è™Ÿèˆ‡é€£çµ
**é¡Œç›® ID:** `Q-106`
**é¡Œç›®é€£çµ:** [é»æ­¤è¿”å›é¡Œç›®](#question-106)

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B` (æ¨è–¦ç­”æ¡ˆ)

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Delta Lake Incremental Processing - Structured Streaming vs Version History
**çŸ¥è­˜é ˜åŸŸ:** Cost Optimization / Incremental Data Processing
**é—œéµæ¦‚å¿µ:**
- Structured Streaming çš„ `Trigger.Once` æ¨¡å¼
- Delta Lake ç‰ˆæœ¬æ­·å²ï¼ˆVersion Historyï¼‰çš„é©ç”¨å ´æ™¯
- Incremental Batch Processing çš„æœ€ä½³å¯¦å‹™
- Change Data Feed (CDF) vs Version History

### æ¬¡è¦è€ƒé»
- MERGE æ“ä½œçš„æˆæœ¬
- åŸºæ–¼æ™‚é–“æˆ³ç¯©é¸çš„å±€é™æ€§
- å®Œæ•´é‡æ–°è™•ç†çš„æˆæœ¬å•é¡Œ

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ B æ˜¯æœ€ä½³ç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†:**

**1. Structured Streaming + Trigger.Once çš„å„ªå‹¢ï¼š**

Structured Streaming åœ¨ Delta Lake ä¸Šæä¾› **åŸç”Ÿå¢é‡è™•ç†** èƒ½åŠ›ï¼š

```python
# é¸é … Bï¼šStructured Streaming with Trigger.Once
from pyspark.sql.streaming import Trigger

# è®€å– reviews_raw çš„å¢é‡è³‡æ–™
streaming_df = (spark.readStream
    .format("delta")
    .table("reviews_raw")
)

# åŸ·è¡Œè½‰æ›ï¼ˆå»é‡ã€é©—è­‰ã€è±å¯ŒåŒ–ï¼‰
enriched_df = (streaming_df
    .dropDuplicates(["user_id", "review_id", "product_id", "review_timestamp"])
    .filter("review_text IS NOT NULL AND length(review_text) > 10")
    .withColumn("processed_timestamp", current_timestamp())
)

# å¯«å…¥ç›®æ¨™è¡¨ï¼ˆTrigger.Once = æ‰¹æ¬¡åŸ·è¡Œï¼‰
(enriched_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", "/checkpoint/reviews_enriched")
    .trigger(once=True)  # â† é—œéµï¼šä»¥æ‰¹æ¬¡æ¨¡å¼åŸ·è¡Œï¼Œä½†ä¿ç•™ä¸²æµèªç¾©
    .table("reviews_enriched")
)
```

**âœ… ç‚ºä»€éº¼é€™æ˜¯æœ€ä½³æ–¹æ¡ˆï¼Ÿ**

| å„ªå‹¢ | èªªæ˜ |
|------|------|
| **è‡ªå‹•å¢é‡è¿½è¹¤** | Checkpoint è‡ªå‹•è¨˜éŒ„å·²è™•ç†çš„è³‡æ–™ï¼Œåªè®€å–æ–°å¢è¨˜éŒ„ |
| **æœ€å°è³‡æ–™æƒæ** | åªè®€å–è‡ªä¸Šæ¬¡åŸ·è¡Œå¾Œæ–°å¢çš„æª”æ¡ˆï¼ˆDelta Log è¿½è¹¤ï¼‰ |
| **æˆæœ¬æœ€ä½** | ä¸éœ€è¦å…¨è¡¨æƒæã€ä¸éœ€è¦ç‰ˆæœ¬æ¯”å°ã€ä¸éœ€è¦è¤‡é›œ MERGE |
| **å¯é‡è¤‡åŸ·è¡Œ** | Checkpoint ç¢ºä¿ exactly-once èªç¾©ï¼Œå¤±æ•—å¾Œå¯é‡è©¦ |
| **æ‰¹æ¬¡åŸ·è¡Œ** | `Trigger.Once` è®“ä¸²æµæŸ¥è©¢ä»¥æ‰¹æ¬¡æ¨¡å¼åŸ·è¡Œ |
| **ç¶­è­·ç°¡å–®** | ä¸éœ€è¦æ‰‹å‹•ç®¡ç† watermark æˆ–ç‰ˆæœ¬è™Ÿ |

**2. Structured Streaming çš„å¢é‡è™•ç†æ©Ÿåˆ¶ï¼š**

```
æ™‚é–“è»¸è¦–åœ–ï¼š

Day 1: reviews_raw æœ‰ 1000 ç­†
       â†“
       Trigger.Once åŸ·è¡Œ â†’ è™•ç† 1000 ç­† â†’ reviews_enriched
       Checkpoint è¨˜éŒ„: "å·²è™•ç†åˆ°ç‰ˆæœ¬ V1"

Day 2: reviews_raw æ–°å¢ 500 ç­†ï¼ˆç¸½å…± 1500 ç­†ï¼‰
       â†“
       Trigger.Once åŸ·è¡Œ â†’ åªè®€å–æ–°å¢ 500 ç­† â† æˆæœ¬æœ€ä½ï¼
       Checkpoint è¨˜éŒ„: "å·²è™•ç†åˆ°ç‰ˆæœ¬ V2"

Day 3: reviews_raw æ–°å¢ 300 ç­†ï¼ˆç¸½å…± 1800 ç­†ï¼‰
       â†“
       Trigger.Once åŸ·è¡Œ â†’ åªè®€å–æ–°å¢ 300 ç­†
       Checkpoint è¨˜éŒ„: "å·²è™•ç†åˆ°ç‰ˆæœ¬ V3"
```

**3. å®Œæ•´å¯¦ä½œç¯„ä¾‹ï¼š**

```python
# å®Œæ•´çš„å¢é‡è™•ç†ç®¡é“ï¼ˆé¸é … Bï¼‰

from pyspark.sql.functions import *
from pyspark.sql.streaming import Trigger

# Step 1: é…ç½® Structured Streaming è®€å–
streaming_df = (spark.readStream
    .format("delta")
    .table("reviews_raw")
)

# Step 2: è³‡æ–™å»é‡ï¼ˆè™•ç†å»¶é²åˆ°é”çš„é‡è¤‡è¨˜éŒ„ï¼‰
# æ³¨æ„ï¼šä½¿ç”¨ dropDuplicates åœ¨ä¸²æµä¸­æœƒç¶­è­·ç‹€æ…‹
deduplicated_df = streaming_df.dropDuplicates([
    "user_id", 
    "review_id", 
    "product_id", 
    "review_timestamp"
])

# Step 3: è³‡æ–™é©—è­‰
validated_df = deduplicated_df.filter(
    (col("user_id").isNotNull()) &
    (col("review_id").isNotNull()) &
    (col("review_text").isNotNull()) &
    (length(col("review_text")) >= 10)
)

# Step 4: è³‡æ–™è±å¯ŒåŒ–ï¼ˆå‡è¨­åŠ å…¥ç”¢å“è³‡è¨Šï¼‰
products_df = spark.table("products_dim")
enriched_df = validated_df.join(
    broadcast(products_df),
    "product_id",
    "left"
).select(
    "user_id",
    "review_id",
    "product_id",
    "review_timestamp",
    "review_text",
    "products_dim.product_name",
    "products_dim.category",
    current_timestamp().alias("processed_timestamp")
)

# Step 5: å¯«å…¥ç›®æ¨™è¡¨ï¼ˆTrigger.Onceï¼‰
query = (enriched_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", "/mnt/checkpoints/reviews_enriched")
    .trigger(once=True)  # â† æ‰¹æ¬¡åŸ·è¡Œæ¨¡å¼
    .table("reviews_enriched")
)

# ç­‰å¾…å®Œæˆ
query.awaitTermination()

print(f"è™•ç†å®Œæˆï¼åƒ…è™•ç†æ–°å¢çš„è¨˜éŒ„ï¼Œæˆæœ¬æœ€ä½ã€‚")
```

**4. èˆ‡å…¶ä»–æ–¹æ¡ˆçš„æˆæœ¬å°æ¯”ï¼š**

å‡è¨­ `reviews_raw` æœ‰ 1000 è¬ç­†æ­·å²è³‡æ–™ï¼Œæ¯æ—¥æ–°å¢ 10 è¬ç­†ï¼š

| æ–¹æ¡ˆ | æ¯æ—¥éœ€è¦æƒæçš„è³‡æ–™é‡ | ç›¸å°æˆæœ¬ |
|------|-------------------|---------|
| **B (Trigger.Once)** | 10 è¬ç­†ï¼ˆåƒ…æ–°å¢è³‡æ–™ï¼‰ | **1x (æœ€ä½)** |
| A (Batch + MERGE) | 1000 è¬ç­†ï¼ˆå…¨è¡¨æƒæä»¥ MERGEï¼‰ | 100x |
| C (Version History) | 10 è¬ç­†ï¼ˆç‰ˆæœ¬å·®ç•°ï¼‰ | 1xï¼Œä½†éœ€æ‰‹å‹•ç®¡ç† |
| D (Timestamp Filter) | è‡³å°‘æ•¸ç™¾è¬ç­†ï¼ˆ48å°æ™‚çª—å£ï¼‰ | 10-20x |
| E (Full Reprocess) | 1000 è¬ç­†ï¼ˆå…¨è¡¨ï¼‰ | 100x |

**ç¬¦åˆéœ€æ±‚:**

é¡Œç›®è¦æ±‚ï¼š
- âœ… **æœ€å°åŒ–è¨ˆç®—æˆæœ¬** - Trigger.Once åªè™•ç†æ–°å¢è³‡æ–™
- âœ… **è™•ç†æ–°è¨˜éŒ„** - è‡ªå‹•è¿½è¹¤å·²è™•ç†è¨˜éŒ„
- âœ… **å»é‡ã€é©—è­‰ã€è±å¯ŒåŒ–** - åœ¨ä¸²æµç®¡é“ä¸­å¯¦ç¾
- âœ… **è™•ç†å»¶é²è³‡æ–™** - å¯é…ç½® watermark è™•ç†å»¶é²è¨˜éŒ„

**5. Trigger.Once çš„å·¥ä½œåŸç†ï¼š**

```python
# Trigger.Once çš„è¡Œç‚º

# ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ˆå‡è¨­è¡¨ç‚ºç©ºï¼‰
query.trigger(once=True)
# â†’ è®€å– reviews_raw çš„æ‰€æœ‰è³‡æ–™ï¼ˆå› ç‚ºæ²’æœ‰ checkpointï¼‰
# â†’ è™•ç†ä¸¦å¯«å…¥ reviews_enriched
# â†’ åœ¨ checkpoint è¨˜éŒ„: offset = 100

# ç¬¬äºŒæ¬¡åŸ·è¡Œï¼ˆreviews_raw æ–°å¢è³‡æ–™ï¼‰
query.trigger(once=True)
# â†’ å¾ checkpoint è®€å– offset = 100
# â†’ åªè®€å– offset > 100 çš„æ–°è³‡æ–™ â† æˆæœ¬æœ€ä½ï¼
# â†’ è™•ç†ä¸¦å¯«å…¥ reviews_enriched
# â†’ æ›´æ–° checkpoint: offset = 150

# é—œéµç‰¹æ€§ï¼š
# 1. æ‰¹æ¬¡åŸ·è¡Œï¼ˆåŸ·è¡Œå®Œå°±åœæ­¢ï¼Œä¸æŒçºŒé‹è¡Œï¼‰
# 2. ä¿ç•™ä¸²æµèªç¾©ï¼ˆcheckpoint è¿½è¹¤é€²åº¦ï¼‰
# 3. å¢é‡è™•ç†ï¼ˆåªè™•ç†æ–°è³‡æ–™ï¼‰
# 4. Exactly-once ä¿è­‰ï¼ˆé¿å…é‡è¤‡è™•ç†ï¼‰
```

---

## ğŸ”„ é¸é … C åˆ†æï¼ˆä¾†æºæ¨™è¨»ç­”æ¡ˆï¼‰

### ç‚ºä»€éº¼ C æœ‰çˆ­è­°ï¼Ÿ

**é¸é … Cï¼šä½¿ç”¨ Delta Lake ç‰ˆæœ¬æ­·å²**

```python
# é¸é … C çš„å¯¦ä½œ
from delta.tables import DeltaTable

# å–å¾— reviews_raw çš„æœ€æ–°ç‰ˆæœ¬å’Œå‰ä¸€ç‰ˆæœ¬
delta_table = DeltaTable.forName(spark, "reviews_raw")
current_version = delta_table.history(1).first()["version"]
previous_version = current_version - 1

# è®€å–å…©å€‹ç‰ˆæœ¬çš„è³‡æ–™
current_df = spark.read.format("delta").option("versionAsOf", current_version).table("reviews_raw")
previous_df = spark.read.format("delta").option("versionAsOf", previous_version).table("reviews_raw")

# æ‰¾å‡ºå·®ç•°ï¼ˆæ–°å¢çš„è¨˜éŒ„ï¼‰
new_records = current_df.exceptAll(previous_df)

# è™•ç†æ–°è¨˜éŒ„ï¼ˆå»é‡ã€é©—è­‰ã€è±å¯ŒåŒ–ï¼‰
enriched_df = (new_records
    .dropDuplicates(["user_id", "review_id", "product_id", "review_timestamp"])
    .filter("review_text IS NOT NULL")
    # ... å…¶ä»–è½‰æ›
)

# å¯«å…¥ç›®æ¨™è¡¨
enriched_df.write.format("delta").mode("append").saveAsTable("reviews_enriched")
```

**â“ ç‚ºä»€éº¼é€™å€‹æ–¹æ¡ˆæœ‰å•é¡Œï¼Ÿ**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **âŒ ä¸ä¿è­‰åªæœ‰ä¸€å€‹ç‰ˆæœ¬å·®ç•°** | å¦‚æœ nightly job å¤±æ•—é‡è©¦ï¼Œå¯èƒ½ç”¢ç”Ÿå¤šå€‹ç‰ˆæœ¬ |
| **âŒ éœ€è¦è®€å–å…©å€‹å®Œæ•´ç‰ˆæœ¬** | `current_df` å’Œ `previous_df` éƒ½éœ€è¦å®Œæ•´æƒæ |
| **âŒ exceptAll æˆæœ¬é«˜** | éœ€è¦å°‡å…©å€‹ç‰ˆæœ¬çš„æ‰€æœ‰è³‡æ–™è¼‰å…¥è¨˜æ†¶é«”æ¯”å° |
| **âŒ ç„¡æ³•è™•ç†ä¸¦ç™¼å¯«å…¥** | å¦‚æœæœ‰å¤šå€‹ä½œæ¥­å¯«å…¥ï¼Œç‰ˆæœ¬ç®¡ç†æœƒè¤‡é›œåŒ– |
| **âŒ æ‰‹å‹•ç®¡ç†è¤‡é›œ** | éœ€è¦è¿½è¹¤ç‰ˆæœ¬è™Ÿã€è™•ç†éŒ¯èª¤ç­‰ |

**å¯¦éš›æˆæœ¬å°æ¯”ï¼š**

```
å‡è¨­ reviews_raw æœ‰ 1000 è¬ç­†ï¼Œæ¯æ—¥æ–°å¢ 10 è¬ç­†

é¸é … B (Trigger.Once):
- è®€å–è³‡æ–™é‡: 10 è¬ç­†ï¼ˆåƒ…æ–°å¢ï¼‰
- è¨˜æ†¶é«”ä½¿ç”¨: æœ€å°
- I/O æˆæœ¬: æœ€ä½

é¸é … C (Version History):
- è®€å–è³‡æ–™é‡: 1000 è¬ + 1000 è¬ = 2000 è¬ç­†ï¼ˆå…©å€‹ç‰ˆæœ¬ï¼‰
- exceptAll æ“ä½œ: éœ€è¦å°‡ 2000 è¬ç­†è¼‰å…¥è¨˜æ†¶é«”æ¯”å°
- I/O æˆæœ¬: é«˜å‡º 200 å€ï¼
```

**âœ… é¸é … C çš„é©ç”¨å ´æ™¯ï¼ˆä¸é©åˆæœ¬é¡Œï¼‰ï¼š**

Version History é©åˆä»¥ä¸‹æƒ…æ³ï¼š
1. **Ad-hoc åˆ†æ** - å¶çˆ¾éœ€è¦æ¯”å°ç‰ˆæœ¬å·®ç•°
2. **å¯©è¨ˆè¿½è¹¤** - æŸ¥çœ‹ç‰¹å®šæ™‚é–“é»çš„è³‡æ–™ç‹€æ…‹
3. **éŒ¯èª¤æ¢å¾©** - å›æº¯åˆ°å…ˆå‰çš„æ­£ç¢ºç‰ˆæœ¬
4. **ä¸€æ¬¡æ€§è³‡æ–™ä¿®æ­£** - æ¯”è¼ƒä¿®æ­£å‰å¾Œçš„å·®ç•°

ä½†**ä¸é©åˆ**ä½œç‚ºå®šæœŸå¢é‡è™•ç†çš„æ–¹æ¡ˆï¼

---

## âŒ å…¶ä»–éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - "Batch read + insert-only merge"

**ç‚ºä»€éº¼æˆæœ¬è¼ƒé«˜ï¼Ÿ**

```python
# é¸é … A çš„å¯¦ä½œ
from delta.tables import DeltaTable

# 1. å…¨è¡¨è®€å– reviews_raw
source_df = spark.table("reviews_raw")

# 2. åŸ·è¡Œ MERGE æ“ä½œ
target_table = DeltaTable.forName(spark, "reviews_enriched")

target_table.alias("target").merge(
    source_df.alias("source"),
    """target.user_id = source.user_id AND 
       target.review_id = source.review_id AND 
       target.product_id = source.product_id AND 
       target.review_timestamp = source.review_timestamp"""
).whenNotMatchedInsertAll().execute()
```

**âŒ æˆæœ¬å•é¡Œï¼š**

| å•é¡Œ | å½±éŸ¿ |
|------|------|
| **å…¨è¡¨æƒæ** | æ¯æ¬¡éƒ½è®€å– reviews_raw çš„æ‰€æœ‰è³‡æ–™ |
| **MERGE é–‹éŠ·** | MERGE éœ€è¦æƒæç›®æ¨™è¡¨ä¾†æª¢æŸ¥é‡è¤‡ |
| **ç„¡å¢é‡è¿½è¹¤** | ç„¡æ³•çŸ¥é“å“ªäº›æ˜¯æ–°è¨˜éŒ„ï¼Œå¿…é ˆå…¨éƒ¨æª¢æŸ¥ |
| **æˆæœ¬éš¨è³‡æ–™å¢é•·** | è³‡æ–™è¶Šå¤šï¼Œæˆæœ¬è¶Šé«˜ï¼ˆO(n)ï¼‰ |

**æˆæœ¬å°æ¯”ï¼š**

```
ç¬¬ 1 å¤©: reviews_raw æœ‰ 10 è¬ç­†
- é¸é … A: æƒæ 10 è¬ç­†
- é¸é … B: æƒæ 10 è¬ç­†
- æˆæœ¬ç›¸ç•¶

ç¬¬ 30 å¤©: reviews_raw æœ‰ 300 è¬ç­†
- é¸é … A: æƒæ 300 è¬ç­†ï¼ˆå…¨è¡¨ï¼‰ â† æˆæœ¬é«˜
- é¸é … B: æƒæ 10 è¬ç­†ï¼ˆåƒ…æ–°å¢ï¼‰ â† æˆæœ¬ä½

ç¬¬ 365 å¤©: reviews_raw æœ‰ 3650 è¬ç­†
- é¸é … A: æƒæ 3650 è¬ç­† â† æˆæœ¬æ¥µé«˜ï¼
- é¸é … B: æƒæ 10 è¬ç­† â† æˆæœ¬æ†å®š
```

**âœ… é¸é … A çš„é©ç”¨å ´æ™¯ï¼ˆä¸é©åˆæœ¬é¡Œï¼‰ï¼š**
- CDC (Change Data Capture) è³‡æ–™æºï¼ˆæœ¬èº«å·²æ¨™è¨»æ–°å¢/æ›´æ–°/åˆªé™¤ï¼‰
- éœ€è¦ UPSERT èªç¾©ï¼ˆæ›´æ–°ç¾æœ‰è¨˜éŒ„ï¼‰
- è³‡æ–™é‡è¼ƒå°ä¸”å¢é•·ç·©æ…¢

---

### é¸é … D - "Filter by timestamp (last 48 hours)"

**ç‚ºä»€éº¼ä¸å¯é ä¸”æˆæœ¬é«˜ï¼Ÿ**

```python
# é¸é … D çš„å¯¦ä½œ
from pyspark.sql.functions import current_timestamp, expr

# ç¯©é¸éå» 48 å°æ™‚çš„è¨˜éŒ„
recent_reviews = spark.table("reviews_raw").filter(
    expr("review_timestamp >= current_timestamp() - INTERVAL 48 HOURS")
)

# è™•ç†ä¸¦å¯«å…¥
enriched_df = recent_reviews.dropDuplicates(...).filter(...)
enriched_df.write.mode("append").saveAsTable("reviews_enriched")
```

**âŒ å•é¡Œåˆ†æï¼š**

1. **âŒ æœƒé‡è¤‡è™•ç†è³‡æ–™ï¼š**
```
Day 1, 22:00: è™•ç† review_timestamp = Day 1, 21:00 çš„è¨˜éŒ„ âœ…
Day 2, 22:00: åˆè™•ç† review_timestamp = Day 1, 21:00 çš„è¨˜éŒ„ âŒ é‡è¤‡ï¼
```

2. **âŒ ç„¡æ³•è™•ç†å»¶é²è³‡æ–™ï¼š**
```
é¡Œç›®æ˜ç¢ºèªªï¼šã€Œsome records may be delayed as they await moderator approvalã€

å¦‚æœä¸€ç­†è¨˜éŒ„ï¼š
- review_timestamp = 2026-01-10 10:00
- å¯¦éš›åˆ°é”æ™‚é–“ = 2026-01-15 10:00ï¼ˆå»¶é² 5 å¤©ï¼‰

48 å°æ™‚çª—å£æœƒéŒ¯éé€™ç­†è¨˜éŒ„ï¼
```

3. **âŒ éœ€è¦å…¨è¡¨æƒææ™‚é–“æˆ³ï¼š**
```python
# å³ä½¿æœ‰æ™‚é–“æˆ³ç´¢å¼•ï¼Œä»éœ€æƒæå¤§é‡è³‡æ–™
# 48 å°æ™‚ = 2 å¤© Ã— 10 è¬ç­†/å¤© = 20 è¬ç­†
# é å¤šæ–¼çœŸæ­£çš„æ–°å¢è³‡æ–™ï¼ˆå¯èƒ½åªæœ‰ 10 è¬ç­†ï¼‰
```

4. **âŒ ç„¡æ³•ä¿è­‰ exactly-onceï¼š**
- å¦‚æœä½œæ¥­å¤±æ•—é‡è©¦ï¼Œæœƒå†æ¬¡è™•ç†ç›¸åŒçš„ 48 å°æ™‚çª—å£
- å°è‡´ç›®æ¨™è¡¨å‡ºç¾é‡è¤‡è³‡æ–™

---

### é¸é … E - "Full reprocess and overwrite"

**ç‚ºä»€éº¼æˆæœ¬æœ€é«˜ï¼Ÿ**

```python
# é¸é … E çš„å¯¦ä½œï¼ˆæœ€ç³Ÿç³•çš„æ–¹æ¡ˆï¼‰
# æ¯æ¬¡éƒ½é‡æ–°è™•ç†æ‰€æœ‰è³‡æ–™
all_reviews = spark.table("reviews_raw")

enriched_df = (all_reviews
    .dropDuplicates(["user_id", "review_id", "product_id", "review_timestamp"])
    .filter("review_text IS NOT NULL")
    # ... å…¶ä»–è½‰æ›
)

# è¦†å¯«æ•´å€‹ç›®æ¨™è¡¨
enriched_df.write.mode("overwrite").saveAsTable("reviews_enriched")
```

**âŒ å•é¡Œï¼š**

| å•é¡Œ | å½±éŸ¿ |
|------|------|
| **æ¯æ¬¡å…¨è¡¨æƒæ** | å³ä½¿åªæœ‰ 1% çš„æ–°è³‡æ–™ï¼Œä¹Ÿè¦è™•ç† 100% |
| **è¦†å¯«é¢¨éšª** | å¦‚æœä½œæ¥­å¤±æ•—ï¼Œæœƒéºå¤±å·²è™•ç†çš„è³‡æ–™ |
| **ä¸‹æ¸¸å½±éŸ¿** | è¦†å¯«æœƒè§¸ç™¼ä¸‹æ¸¸è¡¨çš„å®Œæ•´é‡æ–°è¨ˆç®— |
| **æˆæœ¬ç·šæ€§å¢é•·** | æˆæœ¬éš¨ç´¯ç©è³‡æ–™é‡å¢é•·ï¼Œç„¡ä¸Šé™ |

**æˆæœ¬ç½é›£ç¯„ä¾‹ï¼š**

```
Day 1: 10 è¬ç­† â†’ è™•ç† 10 è¬ç­†
Day 30: 300 è¬ç­† â†’ è™•ç† 300 è¬ç­†ï¼ˆåƒ… 10 è¬ç­†æ˜¯æ–°çš„ï¼‰
Day 365: 3650 è¬ç­† â†’ è™•ç† 3650 è¬ç­†ï¼ˆåƒ… 10 è¬ç­†æ˜¯æ–°çš„ï¼‰

æµªè²»çš„è¨ˆç®—è³‡æºï¼š
Day 365: 3650 è¬ - 10 è¬ = 3640 è¬ç­†æ˜¯ç„¡æ•ˆè™•ç†
æµªè²»ç‡ = 3640 / 3650 = 99.7%ï¼
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£è¨˜æ†¶

**ã€Œå¢é‡è™•ç†é¸ Streamingï¼ŒTrigger Once ç•¶æ‰¹æ¬¡ç”¨ã€**

```python
readStream + Trigger.Once = æœ€ä½æˆæœ¬çš„å¢é‡æ‰¹æ¬¡è™•ç†
```

**ã€ŒCheckpoint è¨˜é€²åº¦ï¼Œåªè®€æ–°è³‡æ–™ã€**
- Checkpoint è‡ªå‹•è¿½è¹¤å·²è™•ç†çš„è³‡æ–™
- ä¸éœ€è¦æ‰‹å‹•ç®¡ç†ç‰ˆæœ¬æˆ–æ™‚é–“æˆ³

### æ¦‚å¿µå°æ¯”è¡¨

| æ–¹æ¡ˆ | è³‡æ–™æƒæé‡ | å¢é‡è¿½è¹¤ | æˆæœ¬ | é©ç”¨å ´æ™¯ |
|------|-----------|---------|------|---------|
| **Trigger.Once** | åƒ…æ–°å¢è³‡æ–™ | âœ… è‡ªå‹• | æœ€ä½ | **å®šæœŸå¢é‡è™•ç†ï¼ˆæ¨è–¦ï¼‰** |
| Version History | å…©å€‹å®Œæ•´ç‰ˆæœ¬ | âŒ æ‰‹å‹• | é«˜ | Ad-hoc åˆ†æã€å¯©è¨ˆ |
| Batch + MERGE | å…¨è¡¨ | âŒ ç„¡ | é«˜ | CDCã€UPSERT éœ€æ±‚ |
| Timestamp Filter | æ™‚é–“çª—å£å…§æ‰€æœ‰è³‡æ–™ | âŒ ç„¡ | ä¸­é«˜ | æ™‚é–“æ•æ„Ÿã€å·²çŸ¥å»¶é²ä¸Šé™ |
| Full Reprocess | å…¨è¡¨ | âŒ ç„¡ | æœ€é«˜ | ä¸€æ¬¡æ€§é‡å»ºã€é‚è¼¯è®Šæ›´ |

### æ±ºç­–æ¨¹

```
éœ€è¦å¢é‡è™•ç†ï¼Ÿ
  â”œâ”€ æ˜¯ â†’ æœ‰æ˜ç¢ºçš„æ–°å¢äº‹ä»¶æˆ–ç‰ˆæœ¬ï¼Ÿ
  â”‚      â”œâ”€ æ˜¯ â†’ ä½¿ç”¨ Structured Streaming + Trigger.Once âœ… (é¸é … B)
  â”‚      â””â”€ å¦ â†’ è€ƒæ…® Timestamp Filter (é¸é … Dï¼Œæœ‰é¢¨éšª)
  â”‚
  â””â”€ å¦ â†’ éœ€è¦å®Œæ•´é‡æ–°è¨ˆç®—ï¼Ÿ
         â”œâ”€ æ˜¯ â†’ Full Reprocess (é¸é … Eï¼Œæˆæœ¬é«˜)
         â””â”€ å¦ â†’ ä½¿ç”¨ MERGE (é¸é … Aï¼Œæˆæœ¬ä¸­ç­‰)
```

### å¯¦å‹™è¨˜æ†¶é»

**1. Trigger.Once çš„æ ¸å¿ƒåƒ¹å€¼ï¼š**
```python
# æ‰¹æ¬¡çš„ä¾¿åˆ©æ€§ + ä¸²æµçš„å¢é‡èƒ½åŠ›
.trigger(once=True)
# â†‘ åŸ·è¡Œå®Œå°±åœæ­¢ï¼ˆæ‰¹æ¬¡ï¼‰
# â†‘ Checkpoint è¿½è¹¤é€²åº¦ï¼ˆä¸²æµï¼‰
# â†‘ åªè™•ç†æ–°è³‡æ–™ï¼ˆå¢é‡ï¼‰
```

**2. é¿å…å¸¸è¦‹éŒ¯èª¤ï¼š**
- âŒ ä¸è¦ç”¨ Version History åšå®šæœŸå¢é‡ï¼ˆæˆæœ¬é«˜ï¼‰
- âŒ ä¸è¦ç”¨ Timestamp Filterï¼ˆæœƒé‡è¤‡è™•ç†ï¼‰
- âŒ ä¸è¦æ¯æ¬¡éƒ½ Full Reprocessï¼ˆæµªè²»è³‡æºï¼‰
- âœ… ä½¿ç”¨ Trigger.Once ç²å¾—æœ€ä½³æˆæœ¬æ•ˆç›Š

**3. æˆæœ¬å„ªåŒ–é—œéµæŒ‡æ¨™ï¼š**
```
å„ªåŒ–ç›®æ¨™ = æœ€å°åŒ–æƒæè³‡æ–™é‡

Trigger.Once æƒæé‡ = æ–°å¢è³‡æ–™é‡ï¼ˆæœ€å„ªï¼‰
å…¶ä»–æ–¹æ¡ˆæƒæé‡ â‰¥ å…¨è¡¨å¤§å°ï¼ˆæ¬¡å„ªæˆ–æœ€å·®ï¼‰
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶èˆ‡åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡ä»¶é€£çµ

1. **Structured Streaming - Trigger.Once:**
   - [Trigger Documentation](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#triggers)
   - èªªæ˜ `Trigger.Once` çš„æ‰¹æ¬¡åŸ·è¡Œæ¨¡å¼

2. **Delta Lake - Incremental Processing:**
   - [Table Streaming Reads and Writes](https://docs.databricks.com/delta/delta-streaming.html)
   - Delta å¢é‡è™•ç†çš„æœ€ä½³å¯¦å‹™

3. **Delta Lake - Change Data Feed:**
   - [Change Data Feed](https://docs.databricks.com/delta/delta-change-data-feed.html)
   - æ›´é€²éšçš„å¢é‡è¿½è¹¤æ–¹æ¡ˆï¼ˆæ¯” Version History æ›´å¥½ï¼‰

4. **Structured Streaming - Checkpointing:**
   - [Checkpointing Guide](https://docs.databricks.com/structured-streaming/checkpointing.html)
   - Checkpoint çš„å·¥ä½œåŸç†èˆ‡æœ€ä½³å¯¦å‹™

### å»¶ä¼¸å­¸ç¿’è³‡æº

- **Change Data Feed vs Version Historyï¼š** é¸æ“‡æ­£ç¢ºçš„å¢é‡æ–¹æ¡ˆ
- **Trigger æ¨¡å¼æ¯”è¼ƒï¼š** Continuous / ProcessingTime / Once
- **æˆæœ¬å„ªåŒ–ç­–ç•¥ï¼š** å¢é‡è™•ç† vs å…¨é‡è™•ç†çš„å–æ¨

---

## ğŸ’¡ è£œå……èªªæ˜

### å¯¦å‹™æ‡‰ç”¨å ´æ™¯

**1. å®Œæ•´çš„ Medallion Architecture å¯¦ä½œï¼ˆBronze â†’ Silverï¼‰ï¼š**

```python
# Bronze â†’ Silver: ä½¿ç”¨ Trigger.Once åšå¢é‡è™•ç†
from pyspark.sql.functions import *
from pyspark.sql.streaming import Trigger

def bronze_to_silver_incremental():
    """
    å¾ Bronze å±¤å¢é‡è™•ç†è³‡æ–™åˆ° Silver å±¤
    ä½¿ç”¨ Trigger.Once å¯¦ç¾æ‰¹æ¬¡å¢é‡è™•ç†
    """
    
    # è®€å– Bronze å±¤ï¼ˆreviews_rawï¼‰
    bronze_stream = (spark.readStream
        .format("delta")
        .table("reviews_raw")
    )
    
    # è³‡æ–™æ¸…æ´—èˆ‡é©—è­‰
    silver_df = (bronze_stream
        # å»é‡ï¼ˆè™•ç†å»¶é²è³‡æ–™ï¼‰
        .dropDuplicates([
            "user_id", 
            "review_id", 
            "product_id", 
            "review_timestamp"
        ])
        
        # è³‡æ–™é©—è­‰
        .filter(
            (col("user_id").isNotNull()) &
            (col("review_id") > 0) &
            (col("product_id") > 0) &
            (col("review_text").isNotNull()) &
            (length(col("review_text")) >= 10) &
            (col("review_timestamp") <= current_timestamp())
        )
        
        # è³‡æ–™æ¨™æº–åŒ–
        .withColumn("review_text_clean", 
                   lower(trim(col("review_text"))))
        
        # æ–°å¢ç¨½æ ¸æ¬„ä½
        .withColumn("ingestion_timestamp", current_timestamp())
        .withColumn("data_source", lit("mobile_app"))
    )
    
    # å¯«å…¥ Silver å±¤ï¼ˆTrigger.Onceï¼‰
    query = (silver_df.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", "/mnt/checkpoints/silver_reviews")
        .option("mergeSchema", "true")  # å…è¨± schema æ¼”é€²
        .trigger(once=True)
        .table("reviews_silver")
    )
    
    query.awaitTermination()
    print(f"Silver å±¤æ›´æ–°å®Œæˆï¼Œåƒ…è™•ç†æ–°å¢è¨˜éŒ„")

# åœ¨ nightly job ä¸­å‘¼å«
bronze_to_silver_incremental()
```

**2. è™•ç†å»¶é²è³‡æ–™ï¼ˆLate Arriving Dataï¼‰ï¼š**

```python
# é…ç½® watermark è™•ç†å»¶é²è³‡æ–™
def process_with_watermark():
    """
    ä½¿ç”¨ watermark è™•ç†å»¶é²åˆ°é”çš„è³‡æ–™
    é¡Œç›®æåˆ°: "some records may be delayed as they await moderator approval"
    """
    
    bronze_stream = (spark.readStream
        .format("delta")
        .table("reviews_raw")
    )
    
    # è¨­å®š watermarkï¼ˆå®¹å¿ 7 å¤©çš„å»¶é²ï¼‰
    with_watermark = bronze_stream.withWatermark(
        "review_timestamp", 
        "7 days"  # å‡è¨­å¯©æ ¸æœ€å¤šå»¶é² 7 å¤©
    )
    
    # åœ¨ watermark å…§é€²è¡Œå»é‡
    deduplicated = (with_watermark
        .dropDuplicates([
            "user_id", 
            "review_id", 
            "product_id", 
            "review_timestamp"
        ])
    )
    
    # å¯«å…¥ï¼ˆTrigger.Onceï¼‰
    query = (deduplicated.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", "/mnt/checkpoints/reviews_with_watermark")
        .trigger(once=True)
        .table("reviews_silver")
    )
    
    query.awaitTermination()
```

**3. ä½¿ç”¨ Change Data Feedï¼ˆæ›´å¥½çš„æ›¿ä»£æ–¹æ¡ˆï¼‰ï¼š**

```python
# å•Ÿç”¨ Change Data Feedï¼ˆæ¯” Version History æ›´å¥½ï¼‰
spark.sql("""
    ALTER TABLE reviews_raw 
    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")

# è®€å–è®Šæ›´è³‡æ–™
def process_with_cdf():
    """
    ä½¿ç”¨ Change Data Feed è¿½è¹¤è®Šæ›´
    æ¯” Version History æ›´é«˜æ•ˆ
    """
    
    # è®€å–è‡ªä¸Šæ¬¡è™•ç†ä»¥ä¾†çš„è®Šæ›´
    changes_df = (spark.read
        .format("delta")
        .option("readChangeFeed", "true")
        .option("startingVersion", get_last_processed_version())  # å¾æª¢æŸ¥é»è®€å–
        .table("reviews_raw")
    )
    
    # åªè™•ç†æ–°å¢çš„è¨˜éŒ„ï¼ˆINSERTï¼‰
    new_records = changes_df.filter("_change_type = 'insert'")
    
    # è™•ç†ä¸¦å¯«å…¥
    enriched_df = new_records.dropDuplicates(...).filter(...)
    enriched_df.write.mode("append").saveAsTable("reviews_enriched")
    
    # æ›´æ–°æª¢æŸ¥é»
    update_last_processed_version(changes_df.agg(max("_commit_version")).collect()[0][0])
```

**4. ç›£æ§èˆ‡æˆæœ¬è¿½è¹¤ï¼š**

```python
# ç›£æ§å¢é‡è™•ç†çš„æ•ˆç‡
def monitor_incremental_processing():
    """
    è¿½è¹¤å¢é‡è™•ç†çš„æˆæœ¬æ•ˆç›Š
    """
    from delta.tables import DeltaTable
    
    # æŸ¥çœ‹æ¯æ¬¡åŸ·è¡Œè™•ç†çš„è³‡æ–™é‡
    delta_table = DeltaTable.forName(spark, "reviews_silver")
    
    history_df = delta_table.history(10)  # æœ€è¿‘ 10 æ¬¡æ“ä½œ
    
    # åˆ†ææ¯æ¬¡è™•ç†çš„è³‡æ–™é‡
    processing_stats = history_df.select(
        col("timestamp"),
        col("operation"),
        col("operationMetrics.numOutputRows").alias("rows_processed"),
        col("operationMetrics.numOutputBytes").alias("bytes_processed")
    ).filter("operation = 'STREAMING UPDATE'")
    
    processing_stats.show()
    
    # è¨ˆç®—å¹³å‡è™•ç†é‡ï¼ˆé©—è­‰æ˜¯å¦ç‚ºå¢é‡ï¼‰
    avg_rows = processing_stats.agg(avg("rows_processed")).collect()[0][0]
    total_rows = spark.table("reviews_raw").count()
    
    incremental_ratio = avg_rows / total_rows
    print(f"å¢é‡è™•ç†æ¯”ä¾‹: {incremental_ratio:.2%}")
    
    if incremental_ratio > 0.1:  # å¦‚æœæ¯æ¬¡è™•ç†è¶…é 10% çš„è³‡æ–™
        print("âš ï¸ è­¦å‘Šï¼šå¯èƒ½æœªæ­£ç¢ºå¯¦ç¾å¢é‡è™•ç†ï¼")
    else:
        print("âœ… å¢é‡è™•ç†é‹ä½œæ­£å¸¸ï¼Œæˆæœ¬æœ€å„ªåŒ–")
```

**5. å®Œæ•´çš„ Databricks Job é…ç½®ï¼š**

```python
# Databricks Notebook: nightly_review_processing.py

# ä»»å‹™é…ç½®
dbutils.widgets.text("checkpoint_path", "/mnt/checkpoints/reviews_processing")
dbutils.widgets.text("source_table", "reviews_raw")
dbutils.widgets.text("target_table", "reviews_enriched")

checkpoint_path = dbutils.widgets.get("checkpoint_path")
source_table = dbutils.widgets.get("source_table")
target_table = dbutils.widgets.get("target_table")

# ä¸»è¦è™•ç†é‚è¼¯
from pyspark.sql.functions import *
from pyspark.sql.streaming import Trigger
import time

start_time = time.time()

# å¢é‡è®€å–
streaming_df = (spark.readStream
    .format("delta")
    .table(source_table)
)

# è½‰æ›
enriched_df = (streaming_df
    .dropDuplicates(["user_id", "review_id", "product_id", "review_timestamp"])
    .filter("review_text IS NOT NULL AND length(review_text) >= 10")
    .withColumn("processed_timestamp", current_timestamp())
    .withColumn("processing_date", current_date())
)

# å¯«å…¥ï¼ˆTrigger.Onceï¼‰
query = (enriched_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", checkpoint_path)
    .trigger(once=True)
    .table(target_table)
)

# ç­‰å¾…å®Œæˆ
query.awaitTermination()

# è¨ˆç®—åŸ·è¡Œæ™‚é–“
execution_time = time.time() - start_time

# è¨˜éŒ„æŒ‡æ¨™
processed_rows = spark.sql(f"""
    SELECT COUNT(*) as cnt 
    FROM {target_table} 
    WHERE processing_date = current_date()
""").collect()[0]["cnt"]

print(f"""
åŸ·è¡Œæ‘˜è¦:
- è™•ç†æ™‚é–“: {execution_time:.2f} ç§’
- è™•ç†è¨˜éŒ„æ•¸: {processed_rows:,}
- å¹³å‡é€Ÿåº¦: {processed_rows/execution_time:.0f} ç­†/ç§’
- æˆæœ¬å„ªåŒ–: âœ… åƒ…è™•ç†å¢é‡è³‡æ–™
""")

# ç™¼é€æˆåŠŸé€šçŸ¥
dbutils.notebook.exit(f"SUCCESS: Processed {processed_rows} records in {execution_time:.2f}s")
```

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

**çœ‹åˆ°ã€Œminimize compute costsã€+ ã€Œå¢é‡è™•ç†ã€é¡Œç›®ï¼Œå„ªå…ˆé †åºï¼š**

1. âœ… **ç¬¬ä¸€é¸æ“‡ï¼šStructured Streaming + Trigger.Once**
   - é—œéµå­—ï¼š`readStream`, `writeStream`, `trigger(once=True)`, `checkpoint`

2. âœ… **ç¬¬äºŒé¸æ“‡ï¼šChange Data Feed (CDF)**
   - é—œéµå­—ï¼š`readChangeFeed`, `_change_type`, `startingVersion`

3. âš ï¸ **è¬¹æ…é¸æ“‡ï¼šMERGE**
   - é©ç”¨æ–¼ï¼šCDC è³‡æ–™æºã€éœ€è¦ UPSERT
   - ä¸é©ç”¨æ–¼ï¼šç´” append å ´æ™¯

4. âŒ **é¿å…é¸æ“‡ï¼šVersion History**
   - é©ç”¨æ–¼ï¼šAd-hoc åˆ†æã€å¯©è¨ˆ
   - ä¸é©ç”¨æ–¼ï¼šå®šæœŸå¢é‡è™•ç†

5. âŒ **çµ•å°é¿å…ï¼šFull Reprocess**
   - åƒ…ç”¨æ–¼ï¼šé‚è¼¯å¤§æ”¹ã€ä¸€æ¬¡æ€§é‡å»º

### é™·é˜±è­˜åˆ¥

**æœ¬é¡Œçš„é™·é˜±è¨­è¨ˆï¼š**

- **é¸é … Cï¼ˆä¾†æºç­”æ¡ˆï¼‰** - çœ‹ä¼¼é«˜ç´šï¼ˆVersion Historyï¼‰ï¼Œå¯¦éš›æˆæœ¬é«˜ï¼ˆéœ€è®€å…©å€‹ç‰ˆæœ¬ï¼‰
- **é¸é … A** - çœ‹ä¼¼æ­£ç¢ºï¼ˆMERGE å»é‡ï¼‰ï¼Œå¯¦éš›ç„¡å¢é‡è¿½è¹¤ï¼ˆå…¨è¡¨æƒæï¼‰
- **é¸é … D** - çœ‹ä¼¼åˆç†ï¼ˆæ™‚é–“ç¯©é¸ï¼‰ï¼Œå¯¦éš›æœƒé‡è¤‡è™•ç†ä¸”éºæ¼å»¶é²è³‡æ–™
- **é¸é … E** - æœ€æ˜é¡¯éŒ¯èª¤ï¼ˆFull Reprocessï¼‰
- **é¸é … B** - âœ… æœ€ä½³æ–¹æ¡ˆï¼ˆTrigger.Once å¢é‡è™•ç†ï¼‰

**å¿«é€Ÿæ’é™¤æŠ€å·§ï¼š**

| é¸é … | å¿«é€Ÿè­˜åˆ¥ | æ’é™¤åŸå›  |
|------|---------|---------|
| E | "reprocess all" + "overwrite" | å…¨é‡è™•ç†ï¼Œæˆæœ¬æœ€é«˜ |
| D | "last 48 hours" | æœƒé‡è¤‡è™•ç†ï¼Œç„¡æ³•è™•ç†å»¶é² |
| A | "batch read" + ç„¡å¢é‡æ©Ÿåˆ¶ | å…¨è¡¨æƒæï¼Œæˆæœ¬éš¨è³‡æ–™å¢é•· |
| C | "version history" + "difference" | éœ€è®€å…©å€‹ç‰ˆæœ¬ï¼Œæˆæœ¬é«˜ |
| B | "Streaming" + "trigger once" | âœ… å¢é‡ + æ‰¹æ¬¡ï¼Œæˆæœ¬æœ€ä½ |

---

## âš ï¸ ç­”æ¡ˆçˆ­è­°ç¸½çµ

### ç‚ºä»€éº¼ B å„ªæ–¼ Cï¼Ÿ

| å°æ¯”é … | é¸é … B (Trigger.Once) | é¸é … C (Version History) |
|--------|----------------------|-------------------------|
| **è³‡æ–™æƒæé‡** | åƒ…æ–°å¢è³‡æ–™ | å…©å€‹å®Œæ•´ç‰ˆæœ¬ |
| **å¯¦ä½œè¤‡é›œåº¦** | ç°¡å–®ï¼ˆSpark åŸç”Ÿï¼‰ | è¤‡é›œï¼ˆæ‰‹å‹•ç®¡ç†ç‰ˆæœ¬ï¼‰ |
| **å¯é æ€§** | é«˜ï¼ˆCheckpoint ä¿è­‰ï¼‰ | ä¸­ï¼ˆéœ€è™•ç†ä¸¦ç™¼ã€é‡è©¦ï¼‰ |
| **æˆæœ¬** | æœ€ä½ | é«˜ï¼ˆ2x å…¨è¡¨æƒæï¼‰ |
| **ç¶­è­·æ€§** | å„ªï¼ˆè‡ªå‹•åŒ–ï¼‰ | å·®ï¼ˆæ‰‹å‹•è¿½è¹¤ï¼‰ |
| **é©ç”¨å ´æ™¯** | âœ… å®šæœŸå¢é‡è™•ç† | âŒ Ad-hoc åˆ†æ |

### æœ€çµ‚å»ºè­°

**æ¨è–¦ç­”æ¡ˆï¼šB (Structured Streaming + Trigger.Once)**

ç†ç”±ï¼š
1. âœ… ç¬¦åˆé¡Œç›®è¦æ±‚ï¼ˆæœ€å°åŒ–è¨ˆç®—æˆæœ¬ï¼‰
2. âœ… æ¥­ç•Œæœ€ä½³å¯¦å‹™ï¼ˆDatabricks å®˜æ–¹æ¨è–¦ï¼‰
3. âœ… å¯æ“´å±•æ€§æœ€ä½³ï¼ˆæˆæœ¬ä¸éš¨è³‡æ–™é‡å¢é•·ï¼‰
4. âœ… å¯¦ä½œç°¡å–®ä¸”å¯é 

**å¦‚æœè€ƒè©¦é‡åˆ°æ­¤é¡Œï¼š**
- å„ªå…ˆé¸æ“‡ **B**ï¼ˆç¤¾ç¾¤å…±è­˜ + æŠ€è¡“æ­£ç¢ºï¼‰
- å¦‚æœæ²’æœ‰ Bï¼Œè€ƒæ…® **A**ï¼ˆæ¬¡å„ªæ–¹æ¡ˆï¼‰
- ç†è§£ **C çš„å±€é™æ€§**ï¼ˆä¾†æºç­”æ¡ˆä½†æœ‰çˆ­è­°ï¼‰

---

**[è¿”å›é¡Œç›®](#question-106)**
 