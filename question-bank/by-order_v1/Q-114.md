# Question #114

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-114`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A data team's Structured Streaming job is configured to calculate **running aggregates** for item sales to update a downstream marketing dashboard. The marketing team has introduced a new promotion, and they would like to add a **new field** to track the number of times this promotion code is used for each item. 

A junior data engineer suggests updating the existing query as follows. Note that proposed changes are in **bold**.

**Original query:**

```python
df.groupBy("item")
  .agg(count("item").alias("total_count"),
       mean("sale_price").alias("avg_price"))
  .writeStream
  .outputMode("complete")
  .option("checkpointLocation", "/item_agg/_checkpoint")
  .start("/item_agg")
```

**Proposed query:**

```python
df.groupBy("item")
  .agg(count("item").alias("total_count"),
       mean("sale_price").alias("avg_price"),
       count("promo_code = 'NEW_MEMBER'").alias("new_member_promo"))  # æ–°å¢
  .writeStream
  .outputMode("complete")
  .option('mergeSchema', 'true')  # æ–°å¢
  .option("checkpointLocation", "/item_agg/_checkpoint")
  .start("/item_agg")
```

Which step must also be completed to put the proposed query into production?

### é¸é …

- **A.** Specify a new checkpoint location
- **B.** Remove `.option('mergeSchema', 'true')` from the streaming write
- **C.** Increase the shuffle partitions to account for additional aggregates
- **D.** Run `REFRESH TABLE delta.'/item_agg'`

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Structured-Streaming`, `Checkpoint`, `Aggregation`, `Schema-Evolution`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Schema-Evolution-Streaming`, `Checkpoint-State-Management`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Streaming

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `A`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** A
- **ç¤¾ç¾¤å…±è­˜:** A (100%)

### ç¤¾ç¾¤è¨è«–é‡é»
- **Deb9753:** ç•¶æ›´æ–° Streaming Job çš„ schemaï¼ˆç‰¹åˆ¥æ˜¯æ¶‰åŠç‹€æ…‹å­˜å„²çš„èšåˆæŸ¥è©¢ï¼‰æ™‚ï¼ŒæŒ‡å®šä¸€å€‹æ–°çš„ checkpoint location æ˜¯å¿…è¦çš„ï¼Œé€™æ¨£å¯ä»¥ç¢ºä¿æŸ¥è©¢ä½¿ç”¨æ–°æ¶æ§‹é‡æ–°é–‹å§‹ï¼Œé¿å…èˆŠ checkpoint çš„ç‹€æ…‹èˆ‡æ–° schema ä¸ç›¸å®¹çš„å•é¡Œ
- **MDWPartners:** æ¯å€‹æŸ¥è©¢å¿…é ˆæœ‰ä¸åŒçš„ checkpoint locationï¼Œå¤šå€‹æŸ¥è©¢ä¸æ‡‰å…±ç”¨åŒä¸€ä½ç½®

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming Checkpoint & State Management  
**é—œéµæ¦‚å¿µ:** Aggregation State Evolutionã€Checkpoint ä¸å¯è®Šæ€§

**é¡Œç›®é—œéµå­—ï¼š**
- **Running aggregates** (ç‹€æ…‹å­˜å„²èšåˆ)
- **Add a new field** (Schema è®Šæ›´)
- **outputMode("complete")** (å®Œæ•´è¼¸å‡ºæ¨¡å¼éœ€è¦ç‹€æ…‹)
- **checkpointLocation** (ç›¸åŒçš„ checkpoint è·¯å¾‘)

**è€ƒç¶±å°æ‡‰:**
- Part 2: ç¶­é‹èˆ‡è‡ªå‹•åŒ– â†’ Streaming Job ç®¡ç†
- Part 4: æˆæœ¬èˆ‡æ•ˆèƒ½å„ªåŒ– â†’ Checkpoint ç­–ç•¥

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A (Specify a new checkpoint location) æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

Structured Streaming çš„ **checkpoint** å„²å­˜äº†æŸ¥è©¢çš„**ç‹€æ…‹è³‡è¨Š**ï¼š

```
/item_agg/_checkpoint/
â”œâ”€â”€ metadata                    # æŸ¥è©¢é…ç½®ï¼ˆå« schemaï¼‰
â”œâ”€â”€ offsets/                    # å·²è™•ç†çš„è³‡æ–™åç§»é‡
â”œâ”€â”€ state/                      # èšåˆç‹€æ…‹ï¼ˆæ¯å€‹ item çš„ç´¯è¨ˆå€¼ï¼‰
â”‚   â”œâ”€â”€ 0/                      # Partition 0 çš„ç‹€æ…‹
â”‚   â”‚   â””â”€â”€ 1.delta             # Version 1 çš„ç‹€æ…‹æª”æ¡ˆ
â”‚   â”‚       â”œâ”€â”€ item=A, total_count=100, avg_price=50.0  â† èˆŠ schema
â”‚   â”‚       â””â”€â”€ item=B, total_count=200, avg_price=75.0
â””â”€â”€ commits/                    # å·²æäº¤çš„ batch
```

**å•é¡Œæ‰€åœ¨ï¼š**

| é …ç›® | èˆŠ Schema | æ–° Schema | è¡çª |
|------|-----------|-----------|------|
| **State æ¬„ä½** | `total_count`, `avg_price` | `total_count`, `avg_price`, **`new_member_promo`** | âŒ Schema ä¸åŒ¹é… |
| **Checkpoint metadata** | è¨˜éŒ„ 2 å€‹èšåˆæ¬„ä½ | éœ€è¦ 3 å€‹èšåˆæ¬„ä½ | âŒ ç„¡æ³•è®€å–èˆŠç‹€æ…‹ |

**æ­£ç¢ºåšæ³•ï¼š**

```python
df.groupBy("item")
  .agg(count("item").alias("total_count"),
       mean("sale_price").alias("avg_price"),
       count("promo_code = 'NEW_MEMBER'").alias("new_member_promo"))
  .writeStream
  .outputMode("complete")
  .option("checkpointLocation", "/item_agg/_checkpoint_v2")  # âœ… æ–°è·¯å¾‘
  .start("/item_agg")
```

**ç‚ºä»€éº¼éœ€è¦æ–° checkpointï¼Ÿ**

| åŸå›  | èªªæ˜ |
|------|------|
| **State ä¸å¯è®Š** | èˆŠ checkpoint çš„ state ç„¡æ³•è‡ªå‹•é·ç§»åˆ°æ–° schema |
| **Metadata å›ºå®š** | Checkpoint çš„ metadata è¨˜éŒ„äº†åŸå§‹ schema |
| **é‡æ–°åˆå§‹åŒ–** | æ–° checkpoint = å¾é ­é–‹å§‹ç´¯ç©ç‹€æ…‹ï¼ˆæˆ–å¾ç‰¹å®š offset é‡æ’­ï¼‰ |
| **é¿å…è³‡æ–™æå£** | é˜²æ­¢ schema ä¸åŒ¹é…å°è‡´æŸ¥è©¢å¤±æ•— |

**å¯¦å‹™æµç¨‹ï¼š**

```python
# æ­¥é©Ÿ 1ï¼šåœæ­¢èˆŠæŸ¥è©¢
old_query.stop()

# æ­¥é©Ÿ 2ï¼šå‚™ä»½èˆŠ checkpointï¼ˆé¸æ“‡æ€§ï¼‰
# dbutils.fs.cp("/item_agg/_checkpoint", "/item_agg/_checkpoint_backup", recurse=True)

# æ­¥é©Ÿ 3ï¼šä½¿ç”¨æ–° checkpoint å•Ÿå‹•æ–°æŸ¥è©¢
new_query = (
  df.groupBy("item")
    .agg(count("item").alias("total_count"),
         mean("sale_price").alias("avg_price"),
         count("promo_code = 'NEW_MEMBER'").alias("new_member_promo"))
    .writeStream
    .outputMode("complete")
    .option("checkpointLocation", "/item_agg/_checkpoint_v2")
    .start("/item_agg")
)
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### B - Remove `.option('mergeSchema', 'true')`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … B èªç‚º mergeSchema æ˜¯å•é¡Œæ ¹æº
.option('mergeSchema', 'true')  # âŒ èª¤ä»¥ç‚ºé€™å€‹é¸é …æœ‰å®³
```

**éŒ¯èª¤ç†ç”±ï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **mergeSchema æ˜¯é‡å° sink** | é€™å€‹é¸é …ç”¨æ–¼**è¼¸å‡ºè³‡æ–™è¡¨** schema æ¼”åŒ–ï¼Œèˆ‡ checkpoint ç„¡é—œ |
| **å…è¨±æ–°å¢æ¬„ä½** | è®“ç›®æ¨™è³‡æ–™è¡¨æ¥å—æ–°æ¬„ä½ï¼ˆä¾‹å¦‚ `new_member_promo`ï¼‰ |
| **ä¸å½±éŸ¿ checkpoint state** | Checkpoint çš„ state schema ç”±**èšåˆé‚è¼¯**æ±ºå®šï¼Œè€Œé sink é¸é … |

**çœŸç›¸ï¼š**

```python
# mergeSchema çš„çœŸæ­£ä½œç”¨
.option('mergeSchema', 'true')  # âœ… è®“ç›®æ¨™è³‡æ–™è¡¨å¯ä»¥æ–°å¢æ¬„ä½
# ç¯„ä¾‹ï¼šè¼¸å‡ºè³‡æ–™è¡¨å¾ 2 æ¬„è®Š 3 æ¬„
# èˆŠï¼šitem | total_count | avg_price
# æ–°ï¼šitem | total_count | avg_price | new_member_promo
```

---

### C - Increase the shuffle partitions

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … C å»ºè­°èª¿æ•´ shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", "400")
```

**âŒ éŒ¯èª¤é»ï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **èˆ‡ Schema è®Šæ›´ç„¡é—œ** | Shuffle partitions æ˜¯æ•ˆèƒ½å„ªåŒ–åƒæ•¸ |
| **ä¸è§£æ±º checkpoint è¡çª** | å³ä½¿å¢åŠ  partitionsï¼ŒèˆŠ checkpoint ä¾ç„¶ç„¡æ³•è®€å– |
| **éåº¦å„ªåŒ–** | æ–°å¢ä¸€å€‹æ¬„ä½ä¸éœ€è¦å¤§å¹…å¢åŠ  partitions |

**æ¦‚å¿µæ··æ·†ï¼š**

- **Shuffle partitions** â†’ æ§åˆ¶èšåˆé‹ç®—çš„ä¸¦è¡Œåº¦
- **Checkpoint state** â†’ å„²å­˜èšåˆä¸­é–“çµæœçš„æŒä¹…åŒ–æ©Ÿåˆ¶

å…©è€…ç¨ç«‹é‹ä½œï¼Œèª¿æ•´ shuffle partitions ä¸æœƒä¿®å¾© schema ä¸åŒ¹é…å•é¡Œã€‚

---

### D - Run `REFRESH TABLE`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```sql
-- é¸é … D
REFRESH TABLE delta.'/item_agg'
```

**âŒ éŒ¯èª¤é»ï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **é‡å°éœæ…‹è³‡æ–™è¡¨** | `REFRESH TABLE` ç”¨æ–¼æ›´æ–°éœæ…‹è³‡æ–™è¡¨çš„ metadata cache |
| **ä¸å½±éŸ¿ Streaming æŸ¥è©¢** | Streaming æŸ¥è©¢æ˜¯**æŒçºŒé‹è¡Œ**çš„ï¼Œä¸ä¾è³´ cache |
| **ç„¡æ³•ä¿®å¾© checkpoint** | ä¸æœƒæ”¹è®Š checkpoint çš„ state schema |

**REFRESH TABLE çš„çœŸæ­£ç”¨é€”ï¼š**

```sql
-- ä½¿ç”¨å ´æ™¯ï¼šå¤–éƒ¨ä¿®æ”¹è³‡æ–™è¡¨ï¼ˆä¾‹å¦‚é€éå…¶ä»–å·¥å…·ï¼‰
-- éœ€è¦é€šçŸ¥ Spark catalog é‡æ–°æƒæè³‡æ–™è¡¨çµæ§‹
REFRESH TABLE sales;  -- æ›´æ–° metadata cache

-- âŒ ä¸é©ç”¨æ–¼æ­¤é¡Œï¼š
-- 1. Streaming æŸ¥è©¢ä¸éœ€è¦ REFRESH
-- 2. å•é¡Œåœ¨ checkpoint stateï¼Œä¸åœ¨è³‡æ–™è¡¨ metadata
```

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡å°æ¯”

### å£è¨£è¨˜æ†¶
**ã€Œèšåˆ Schema è®Šï¼ŒCheckpoint å¿…é ˆæ›ã€**

### é—œéµæ±ºç­–æ¨¹

```
Streaming æŸ¥è©¢ä¿®æ”¹äº†ä»€éº¼ï¼Ÿ
â”‚
â”œâ”€ åªæ”¹ filter/è½‰æ›é‚è¼¯ â†’ å¯ä»¥ç¹¼çºŒç”¨èˆŠ checkpoint
â”‚
â”œâ”€ åªæ”¹ sink è³‡æ–™è¡¨ schema â†’ ç”¨ mergeSchema=trueï¼Œç„¡éœ€æ› checkpoint
â”‚
â””â”€ æ”¹äº†èšåˆæ¬„ä½ï¼ˆæ–°å¢/åˆªé™¤/ä¿®æ”¹ï¼‰
   â””â”€ âœ… å¿…é ˆæŒ‡å®šæ–° checkpoint location
```

### å°æ¯”è¡¨ï¼šSchema Evolution ç­–ç•¥

| è®Šæ›´é¡å‹ | èˆŠ Checkpoint å¯ç”¨ï¼Ÿ | è§£æ±ºæ–¹æ¡ˆ |
|----------|---------------------|----------|
| **æ–°å¢ filter æ¢ä»¶** | âœ… å¯ä»¥ | ä¿æŒåŸ checkpoint |
| **ä¿®æ”¹è¼¸å‡ºæ ¼å¼** | âœ… å¯ä»¥ | ä¿æŒåŸ checkpoint |
| **è¼¸å‡ºè³‡æ–™è¡¨æ–°å¢æ¬„ä½** | âœ… å¯ä»¥ | åŠ ä¸Š `mergeSchema=true` |
| **èšåˆæ¬„ä½æ–°å¢/åˆªé™¤** | âŒ ä¸å¯ä»¥ | **å¿…é ˆæ–° checkpoint** |
| **ä¿®æ”¹ groupBy key** | âŒ ä¸å¯ä»¥ | **å¿…é ˆæ–° checkpoint** |
| **ä¿®æ”¹ outputMode** | âŒ ä¸å¯ä»¥ | **å¿…é ˆæ–° checkpoint** |

### Checkpoint vs MergeSchema

| é¸é … | ä½œç”¨å°è±¡ | ç”¨é€” | ä½•æ™‚ä½¿ç”¨ |
|------|----------|------|----------|
| **checkpointLocation** | Streaming æŸ¥è©¢ç‹€æ…‹ | å„²å­˜ offsets + state + metadata | æ–°æŸ¥è©¢æˆ– state schema è®Šæ›´æ™‚ |
| **mergeSchema** | è¼¸å‡ºè³‡æ–™è¡¨ | å…è¨± sink è³‡æ–™è¡¨ schema æ¼”åŒ– | è¼¸å‡ºæ¬„ä½å¢åŠ æ™‚ |

---

## ğŸ’¡ å¯¦å‹™è£œå……

### ç•¶ä½ éœ€è¦ä¿ç•™æ­·å²è³‡æ–™æ™‚æ€éº¼è¾¦ï¼Ÿ

**æ–¹æ³• 1ï¼šé‡æ’­ï¼ˆReplayï¼‰èˆŠè³‡æ–™**

```python
# å¾ç‰¹å®š offset é–‹å§‹é‡æ’­
new_query = (
  spark.readStream
    .format("delta")
    .option("startingVersion", "0")  # âœ… å¾ç‰ˆæœ¬ 0 é‡æ–°è™•ç†
    .load("/source")
    .groupBy("item")
    .agg(...)
    .writeStream
    .option("checkpointLocation", "/item_agg/_checkpoint_v2")
    .start("/item_agg")
)
```

**æ–¹æ³• 2ï¼šé·ç§»æ­·å²ç‹€æ…‹ï¼ˆé€²éšï¼‰**

```python
# è®€å–èˆŠè³‡æ–™è¡¨çš„æ­·å²è³‡æ–™ï¼Œæ‰‹å‹•åˆå§‹åŒ–æ–°ç‹€æ…‹
historical_state = spark.read.table("item_agg")

# åœ¨æ–°æŸ¥è©¢ä¸­è™•ç†æ–°è³‡æ–™æ™‚ï¼Œå…ˆè¼‰å…¥æ­·å²ç‹€æ…‹
# ä½¿ç”¨ merge æˆ– insert é‚è¼¯ç¢ºä¿é€£çºŒæ€§
```

**æ–¹æ³• 3ï¼šä½¿ç”¨ foreachBatch è‡ªå®šç¾©é‚è¼¯**

```python
def upsert_with_history(microBatchDF, batchId):
    # è®€å–èˆŠè³‡æ–™
    old_data = spark.read.table("item_agg")
    
    # åˆä½µæ–°èˆŠè³‡æ–™
    merged = old_data.union(microBatchDF).groupBy("item").agg(...)
    
    # å¯«å…¥
    merged.write.format("delta").mode("overwrite").save("/item_agg")

df.writeStream.foreachBatch(upsert_with_history).start()
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶åƒè€ƒ

1. **Structured Streaming Checkpoint:**
   - [Databricks Docs - Checkpoint Location](https://docs.databricks.com/structured-streaming/production.html#checkpoint-location)

2. **Schema Evolution in Streaming:**
   - [Delta Lake - Schema Evolution](https://docs.delta.io/latest/delta-streaming.html#schema-evolution)

3. **Stateful Operations:**
   - [Apache Spark Docs - Stateful Operations](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stateful-operations)

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

**é‡åˆ° Streaming + Schema è®Šæ›´é¡Œç›®æ™‚ï¼š**

1. âœ… **å…ˆçœ‹æ˜¯å¦æ¶‰åŠ stateï¼ˆèšåˆ/join/windowï¼‰** â†’ éœ€è¦æ–° checkpoint
2. âœ… **è‹¥åªæ”¹è¼¸å‡ºæ ¼å¼** â†’ ç”¨ mergeSchemaï¼Œä¸éœ€æ–° checkpoint
3. âœ… **è‹¥ä¸ç¢ºå®š** â†’ ä¿å®ˆèµ·è¦‹é¸æ“‡æ–° checkpointï¼ˆä¸æœƒå‡ºéŒ¯ï¼‰

**é—œéµåˆ¤æ–·ï¼š**

| å•é¡Œ | åˆ¤æ–· |
|------|------|
| é¡Œç›®æœ‰æåˆ° **aggregation/groupBy/window**ï¼Ÿ | â†’ å¯èƒ½éœ€è¦æ–° checkpoint |
| æœ‰æåˆ° **new field/new column**ï¼Ÿ | â†’ éœ€è¦æª¢æŸ¥æ˜¯ source/sink/state çš„å“ªå€‹éƒ¨åˆ† |
| é¸é …æœ‰ **checkpointLocation** å’Œ **mergeSchema**ï¼Ÿ | â†’ checkpoint è™•ç† stateï¼ŒmergeSchema è™•ç† sink |

**é™·é˜±è­¦ç¤ºï¼š**
- âš ï¸ ä¸è¦è¢« `mergeSchema=true` èª¤å°ï¼Œä»¥ç‚ºé€™å€‹é¸é …ã€Œæœ‰å•é¡Œã€
- âš ï¸ ä¸è¦ä»¥ç‚ºã€Œåªæ˜¯æ–°å¢ä¸€å€‹æ¬„ä½ã€å¾ˆç°¡å–®ï¼Œèšåˆ state çš„ schema è®Šæ›´éå¸¸åš´æ ¼
- âš ï¸ `REFRESH TABLE` æ˜¯é‡å°éœæ…‹è³‡æ–™è¡¨ï¼Œèˆ‡ Streaming ç„¡é—œ