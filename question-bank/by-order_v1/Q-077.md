# é¡Œç›® Q-077

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-077`

### ä¾†æº
**ä¾†æº:** Mock Exam (ç¬¬ä¸‰æ–¹æ¨¡æ“¬è©¦é¡Œ)

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

### æ¨™ç±¤
**Topics:** `Auto-Loader`, `Streaming`, `Delta-Schema-Evolution`  
**Traps:** `Syntax-Confusion`, `Parameter-Order`  
**Level:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

**Question #77** focuses on leveraging **Databricks Auto Loader** to create a helper function for near real-time data ingestion and automatic schema evolution.

In order to facilitate **near real-time workloads**, a data engineer is creating a helper function to leverage the schema detection and evolution functionality of **Databricks Auto Loader**. The desired function will automatically detect the schema of the source directly, incrementally process JSON files as they arrive in a source directory, and **automatically evolve the schema** of the table when new fields are detected.

The function is displayed below with a blank:

```python
def auto_load_json(source_path: str,
                   checkpoint_path: str,
                   target_table_path: str):

    (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .option("cloudFiles.schemaLocation", checkpoint_path)
        .load(source_path)
        <BLANK>
    )
```

Which response correctly fills in the blank to meet the specified requirements?

### é¸é …

**A.** `.writeStream.option("mergeschema", True).start(target_table_path)`

**B.** `.writeStream.option("checkpointLocation", checkpoint_path).option("mergeschema", True).trigger(once=True).start(target_table_path)`

**C.** `.write.option("checkpointLocation", checkpoint_path).option("mergeSchema", True).outputMode("append").save(target_table_path)`

**D.** `.write.option("mergeSchema", True).mode("append").save(target_table_path)`

**E.** `.writeStream.option("checkpointLocation", checkpoint_path).option("mergeSchema", True).start(target_table_path)`

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `E`

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Databricks Auto Loader (cloudFiles) + Structured Streaming  
**çŸ¥è­˜é ˜åŸŸ:** æ ¸å¿ƒé–‹ç™¼èˆ‡è½‰æ› - ä¸²æµè³‡æ–™è™•ç†  
**é—œéµæ¦‚å¿µ:** near real-time streamingã€schema evolutionã€checkpoint management

### æ¬¡è¦è€ƒé»
- Structured Streaming çš„ writeStream vs write å·®ç•°
- checkpointLocation åœ¨ä¸²æµä¸­çš„å¿…è¦æ€§
- mergeSchema åƒæ•¸çš„æ­£ç¢ºä½¿ç”¨

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼é¸æ“‡ E

**Option E æ­£ç¢ºçš„åŸå› ï¼š**

1. **Near Real-Time éœ€æ±‚**: é¡Œç›®æ˜ç¢ºè¦æ±‚ã€Œnear real-time workloadsã€ï¼Œé€™éœ€è¦ä½¿ç”¨ **`.writeStream`** é€²è¡Œä¸²æµè™•ç†ï¼Œè€Œéæ‰¹æ¬¡è™•ç†çš„ `.write`

2. **å®¹éŒ¯æ€§è¦æ±‚**: ä»»ä½•ä¸²æµæ“ä½œéƒ½éœ€è¦ **`checkpointLocation`** ä¾†è¿½è¹¤é€²åº¦ä¸¦ç¢ºä¿ç®¡é“èƒ½å¾æ•…éšœä¸­æ¢å¾©ã€‚åœ¨ Auto Loader ä¸­ï¼Œå¸¸è¦‹åšæ³•æ˜¯è®“ `cloudFiles.schemaLocation` å’Œ `checkpointLocation` ä½¿ç”¨ç›¸åŒè·¯å¾‘ä»¥ä¾¿æ–¼ schema ç®¡ç†

3. **Schema Evolution**: ç‚ºäº†æ»¿è¶³ã€Œautomatically evolve the schema when new fields are detectedã€çš„è¦æ±‚ï¼Œå¿…é ˆè¨­å®š **`mergeSchema`** é¸é …ç‚º `True`

4. **é€£çºŒè™•ç†**: `.start(target_table_path)` å•Ÿå‹•é€£çºŒçš„ä¸²æµè™•ç†ï¼Œç¬¦åˆè¿‘ä¹å³æ™‚çš„éœ€æ±‚

### æŠ€è¡“ç´°ç¯€

é€™æ˜¯ä¸€å€‹æœ€ç²¾ç°¡çš„æ‹†è§£ï¼Œå¹«åŠ©ä½ å¿«é€ŸæŒæ¡é€™æ®µç¨‹å¼ç¢¼çš„é‚è¼¯ï¼š

#### 1. æ ¸å¿ƒç¨‹å¼ç¢¼çµæ§‹

```python
(spark.readStream                    # ã€è®€å–ç«¯ã€‘å•Ÿå‹•ä¸²æµè®€å–
    .format("cloudFiles")            # 1. è²æ˜ä½¿ç”¨ Auto Loader
    .option("cloudFiles.format", "json") 
    .option("cloudFiles.schemaLocation", path) 
    .load(source_path) 
    .writeStream                     # ã€å¯«å…¥ç«¯ã€‘å•Ÿå‹•ä¸²æµå¯«å…¥
    .option("checkpointLocation", path) 
    .option("mergeSchema", "true")   # 2. å…è¨±è¡¨æ ¼çµæ§‹è‡ªå‹•æ“´å±•
    .start(target_table_path) 
)

```

---

#### 2. åƒæ•¸åˆ†é¡èˆ‡æ„ç¾©

| é¡åˆ¥ | åƒæ•¸åç¨± (Key) | åƒæ•¸å€¼ (Value) | ä½œç”¨èªªæ˜ |
| --- | --- | --- | --- |
| **Auto Loader** | `cloudFiles.format` | `"json"` | å‘Šè¨´ç³»çµ±ï¼šåŸå§‹æª”æ¡ˆæ˜¯ **JSON** æ ¼å¼ã€‚ |
| **Auto Loader** | `cloudFiles.schemaLocation` | `path` | **è‡ªå‹•åµæ¸¬çµæ§‹**ï¼šæŠŠç™¼ç¾çš„æ¬„ä½è³‡è¨Šè¨˜åœ¨é€™è£¡ã€‚ |
| **Streaming** | `checkpointLocation` | `path` | **é€²åº¦ç´€éŒ„**ï¼šè¨˜ä½è®€åˆ°å“ªä¸€å€‹æª”æ¡ˆï¼Œç•¶æ©Ÿé‡å•Ÿä¸æ¼æŠ“ã€‚ |
| **Delta Lake** | `mergeSchema` | `"true"` | **çµæ§‹æ¼”è®Š**ï¼šç™¼ç¾æ–°æ¬„ä½æ™‚ï¼Œè‡ªå‹•åŠ é€²ç›®æ¨™è¡¨ã€‚ |

---

#### 3. ä¸€å¥è©±ç¸½çµå…©è€…é—œä¿‚

> **ã€ŒAuto Loader æ˜¯ä¸€å€‹å°ˆç‚º Structured Streaming æ‰“é€ çš„ã€æ™ºæ…§å‹æ„Ÿæ‡‰å…¥å£ã€ï¼Œè² è²¬è‡ªå‹•åµæ¸¬æª”æ¡ˆä¸¦è®€å–ï¼Œå†äº¤ç”± Streaming å¼•æ“é€²è¡ŒæŒçºŒä¸æ–·çš„æ¬é‹èˆ‡å¯«å…¥ã€‚ã€**

---

**æŒæ¡äº†é€™äº›æ ¸å¿ƒåƒæ•¸å¾Œï¼Œå¦‚æœæ‚¨æƒ³é‡å°ä¸åŒæ ¼å¼ï¼ˆä¾‹å¦‚ CSVï¼‰é€²è¡Œè¨­å®šï¼Œé€šå¸¸åªéœ€è¦ä¿®æ”¹ `cloudFiles.format` ä¸¦åŠ å…¥åˆ†éš”ç¬¦è™Ÿè¨­å®šå³å¯ã€‚éœ€è¦æˆ‘ç¤ºç¯„ CSV ç‰ˆçš„å¯«æ³•å—ï¼Ÿ**
---

## âŒ éŒ¯èª¤é¸é …åˆ†æ

### Option A - ç¼ºå°‘ checkpointLocation
```python
.writeStream.option("mergeschema", True).start(target_table_path)
```
**å•é¡Œï¼š**
- ç¼ºå°‘ `checkpointLocation`ï¼Œä¸²æµæœƒå¤±æ•—
- åƒæ•¸åç¨±éŒ¯èª¤ï¼š`mergeschema` â†’ æ­£ç¢ºæ‡‰ç‚º `mergeSchema`

### Option B - trigger(once=True)
```python
.writeStream.option("checkpointLocation", checkpoint_path).option("mergeschema", True).trigger(once=True).start(target_table_path)
```
**å•é¡Œï¼š**
- `trigger(once=True)` ç”¨æ–¼ã€Œæ‰¹æ¬¡æ¨¡å¼ã€è™•ç†ä¸²æµè³‡æ–™ï¼Œä¸ç¬¦åˆã€Œnear real-timeã€éœ€æ±‚
- åƒæ•¸åç¨±éŒ¯èª¤ï¼š`mergeschema` â†’ æ­£ç¢ºæ‡‰ç‚º `mergeSchema`

### Option C & D - ä½¿ç”¨ .write
```python
# Option C
.write.option("checkpointLocation", checkpoint_path).option("mergeSchema", True).outputMode("append").save(target_table_path)

# Option D  
.write.option("mergeSchema", True).mode("append").save(target_table_path)
```
**å•é¡Œï¼š**
- ä½¿ç”¨ `.write` é€²è¡Œæ‰¹æ¬¡è™•ç†ï¼Œä¸ç¬¦åˆã€Œnear real-timeã€éœ€æ±‚
- Option C: `checkpointLocation` ä¸é©ç”¨æ–¼æ‰¹æ¬¡å¯«å…¥
- Option D: ç¼ºå°‘å¿…è¦çš„ checkpoint æ©Ÿåˆ¶

---

## ğŸ§  è¨˜æ†¶æŠ€å·§

### Auto Loader ä¸²æµè™•ç†å£è¨£
```
Auto Loader ä¸²æµä¸‰è¦ç´ ï¼š
1. writeStream (ä¸æ˜¯ write)
2. checkpointLocation (å¿…éœ€ï¼)  
3. mergeSchema (å¤§å°å¯«è¦å°)
```

### å¸¸è¦‹é™·é˜±å°æ¯”è¡¨

| é™·é˜±é¡å‹ | éŒ¯èª¤å¯«æ³• | æ­£ç¢ºå¯«æ³• | å‚™è¨» |
|---------|---------|---------|------|
| è™•ç†æ¨¡å¼ | `.write` | `.writeStream` | Near real-time éœ€è¦ä¸²æµ |
| Schema åƒæ•¸ | `mergeschema` | `mergeSchema` | å¤§å°å¯«æ•æ„Ÿ |
| Checkpoint | çœç•¥ checkpointLocation | å¿…é ˆåŒ…å« | ä¸²æµå®¹éŒ¯å¿…éœ€ |
| Trigger æ¨¡å¼ | `once=True` | çœç•¥ (é€£çºŒè™•ç†) | Once æ˜¯æ‰¹æ¬¡æ¨¡å¼ |

---

## ğŸ“š å»¶ä¼¸å­¸ç¿’

### Auto Loader æœ€ä½³å¯¦è¸

1. **Schema ä½ç½®ç®¡ç†**
```python
# å»ºè­°è®“ schemaLocation å’Œ checkpointLocation ä½¿ç”¨ç›¸åŒè·¯å¾‘
.option("cloudFiles.schemaLocation", checkpoint_path)
.option("checkpointLocation", checkpoint_path)
```

2. **æª”æ¡ˆæ ¼å¼æ”¯æ´**
```python
# Auto Loader æ”¯æ´å¤šç¨®æ ¼å¼
.option("cloudFiles.format", "json")    # JSON
.option("cloudFiles.format", "parquet") # Parquet  
.option("cloudFiles.format", "csv")     # CSV
```

3. **Schema Evolution æ§åˆ¶**
```python
# æ›´ç´°ç·»çš„ schema æ§åˆ¶
.option("mergeSchema", True)                    # è‡ªå‹•åˆä½µæ–°æ¬„ä½
.option("cloudFiles.schemaEvolutionMode", "addNewColumns")  # åªæ·»åŠ æ–°æ¬„ä½
```

### å®˜æ–¹æ–‡ä»¶é€£çµ

- [Databricks Auto Loader å®˜æ–¹æ–‡ä»¶](https://docs.databricks.com/ingestion/auto-loader/index.html)
- [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Schema Evolution in Delta Lake](https://docs.delta.io/latest/delta-batch.html#schema-evolution)