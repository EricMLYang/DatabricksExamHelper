# Question #118

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-118`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A member of the data engineering team has submitted a short notebook that they wish to **schedule as part of a larger data pipeline**. Assume that the commands provided below produce the **logically correct results** when run as presented.

**Notebook Commands:**

- **Cmd 1:** `rawDF = spark.table("raw_data")`
- **Cmd 2:** `rawDF.printSchema()`
- **Cmd 3:** `flattenedDF = rawDF.select("*", "values.*")`
- **Cmd 4:** `finalDF = flattenedDF.drop("values")`
- **Cmd 5:** `display(finalDF)`
- **Cmd 6:** `finalDF.write.mode("append").saveAsTable("flat_data")`

Which command should be **removed** from the notebook before scheduling it as a job?

### é¸é …

- **A.** Cmd 2 (`rawDF.printSchema()`)
- **B.** Cmd 3 (`flattenedDF = rawDF.select("*", "values.*")`)
- **C.** Cmd 4 (`finalDF = flattenedDF.drop("values")`)
- **D.** Cmd 5 (`display(finalDF)`)

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Job-Scheduling`, `Spark-Actions`, `Notebook-Optimization`, `Production-Best-Practices`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Display-in-Production`, `Interactive-vs-Batch`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Operations

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** D
- **ç¤¾ç¾¤å…±è­˜:** D (100%)

### ç¤¾ç¾¤è¨è«–é‡é»
- **ç­”æ¡ˆè§£æ:** `display()` å‡½æ•¸ä¸»è¦ç”¨æ–¼é–‹ç™¼éšæ®µçš„äº’å‹•å¼æ•¸æ“šè¦–è¦ºåŒ–ã€‚åœ¨è‡ªå‹•åŒ–æ’ç¨‹çš„ç”Ÿç”¢ç’°å¢ƒä½œæ¥­ï¼ˆProduction Jobï¼‰ä¸­ï¼Œ`display()` æœƒè§¸ç™¼ Action å¼·åˆ¶ Spark åŸ·è¡Œè¨ˆç®—ä¸¦å°‡çµæœå›å‚³è‡³ Driver é€²è¡Œæ¸²æŸ“ï¼Œé€™ä¸åƒ…å°æ–¼éäº’å‹•å¼ä½œæ¥­æ˜¯å¤šé¤˜çš„ï¼Œé‚„æœƒé€ æˆä¸å¿…è¦çš„é‹ç®—è³‡æºæµªè²»èˆ‡æ½›åœ¨çš„æ•ˆèƒ½ç“¶é ¸
- **é—œè¯é¡Œç›®:** æ­¤é¡Œèˆ‡ Question 61 å…§å®¹å®Œå…¨ç›¸åŒã€‚åœ¨ Question 61 çš„ç¤¾ç¾¤è¨è«–ä¸­ï¼Œç”¨æˆ¶æŒ‡å‡º `display()` æ˜¯äº’å‹•å¼åŠŸèƒ½ï¼Œä¸æ‡‰å­˜åœ¨æ–¼ç”Ÿç”¢ä½œæ¥­ä¸­ï¼Œæ‡‰äºˆä»¥ç§»é™¤æˆ–è¨»è§£æ‰ã€‚é›–ç„¶ `printSchema()` (Cmd 2) ä¹Ÿæœƒè¼¸å‡ºè³‡è¨Šåˆ° Logï¼Œä½†å®ƒåƒ…æ¶‰åŠ Metadata æ“ä½œï¼Œé–‹éŠ·é å°æ–¼æ¶‰åŠå¯¦éš›è³‡æ–™ç§»å‹•çš„ `display()`

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Spark Actions vs Transformations & Production Job Optimization  
**é—œéµæ¦‚å¿µ:** äº’å‹•å¼æŒ‡ä»¤ vs æ‰¹æ¬¡è™•ç†ã€è³‡æºæµªè²»

**é¡Œç›®é—œéµå­—ï¼š**
- **Schedule as part of a larger data pipeline** (æ’ç¨‹ç‚ºè‡ªå‹•åŒ–ä½œæ¥­)
- **Which command should be removed** (å“ªå€‹æŒ‡ä»¤æ‡‰è©²ç§»é™¤)
- **display(finalDF)** (äº’å‹•å¼é¡¯ç¤ºæŒ‡ä»¤)

**è€ƒç¶±å°æ‡‰:**
- Part 2: ç¶­é‹èˆ‡è‡ªå‹•åŒ– â†’ Job Scheduling & Optimization
- Part 4: æˆæœ¬èˆ‡æ•ˆèƒ½å„ªåŒ– â†’ é¿å…ä¸å¿…è¦çš„é‹ç®—

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D (display(finalDF)) æ‡‰è©²è¢«ç§»é™¤ï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

**display() çš„è¨­è¨ˆç›®çš„ï¼š**

```python
# Cmd 5: display(finalDF)  â† äº’å‹•å¼é¡¯ç¤ºï¼Œåƒ…é©ç”¨æ–¼é–‹ç™¼ç’°å¢ƒ
```

| ç‰¹æ€§ | èªªæ˜ |
|------|------|
| **ç”¨é€”** | åœ¨ Notebook ä¸­ä»¥è¡¨æ ¼/åœ–è¡¨å½¢å¼é¡¯ç¤ºè³‡æ–™ |
| **ç’°å¢ƒ** | åƒ…é©ç”¨æ–¼**äº’å‹•å¼ Notebook** ç’°å¢ƒ |
| **åŸ·è¡Œæ–¹å¼** | è§¸ç™¼ **Action**ï¼Œæ”¶é›†è³‡æ–™åˆ° Driver |
| **é è¨­è¡Œç‚º** | æ”¶é›†å‰ 1000 rows åˆ° Driver é€²è¡Œæ¸²æŸ“ |

**åœ¨ Production Job ä¸­çš„å•é¡Œï¼š**

```
æ’ç¨‹ä½œæ¥­åŸ·è¡Œæµç¨‹ï¼ˆç„¡ä½¿ç”¨è€…ä»‹é¢ï¼‰ï¼š

Cmd 1: è®€å–è³‡æ–™ âœ…
Cmd 2: é¡¯ç¤º Schema âš ï¸ (è¼¸å‡ºåˆ° logï¼Œé–‹éŠ·å°)
Cmd 3: è½‰æ›è³‡æ–™ âœ…
Cmd 4: åˆªé™¤æ¬„ä½ âœ…
Cmd 5: display(finalDF) âŒ è§¸ç™¼ä¸å¿…è¦çš„è¨ˆç®—
         â†“
         æ”¶é›† 1000 rows åˆ° Driver
         â†“
         å˜—è©¦æ¸²æŸ“ï¼ˆä½†ç„¡äº’å‹•ä»‹é¢ï¼‰
         â†“
         è³‡æºæµªè²»ã€ç„¡ä»»ä½•å¯¦éš›ç”¨é€”
Cmd 6: å¯«å…¥è³‡æ–™è¡¨ âœ…
```

**ç‚ºä»€éº¼ display() æœ‰å•é¡Œï¼Ÿ**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **è§¸ç™¼ Action** | å¼·åˆ¶ Spark åŸ·è¡Œæ•´å€‹è¨ˆç®—æµç¨‹ |
| **è³‡æ–™å‚³è¼¸** | å°‡è³‡æ–™å¾ Executors å‚³é€åˆ° Driver |
| **è¨˜æ†¶é«”æ¶ˆè€—** | Driver éœ€è¦æš«å­˜æ”¶é›†çš„è³‡æ–™ |
| **é‡è¤‡è¨ˆç®—** | Cmd 6 æœƒå†æ¬¡åŸ·è¡Œç›¸åŒçš„è¨ˆç®— |
| **ç„¡å¯¦éš›ç”¨é€”** | æ’ç¨‹ä½œæ¥­æ²’æœ‰ UIï¼Œæ¸²æŸ“çµæœç„¡æ³•è¢«æŸ¥çœ‹ |

**æ€§èƒ½å½±éŸ¿åˆ†æï¼š**

```python
# åŸ·è¡Œè¨ˆç•«ï¼ˆå« displayï¼‰ï¼š
rawDF = spark.table("raw_data")         # Transformation
flattenedDF = rawDF.select("*", "values.*")  # Transformation
finalDF = flattenedDF.drop("values")    # Transformation

display(finalDF)  # â† Action #1: è§¸ç™¼åŸ·è¡Œä¸¦æ”¶é›† 1000 rows
# Spark åŸ·è¡Œï¼šScan â†’ Select â†’ Drop â†’ Collect(1000)

finalDF.write.mode("append").saveAsTable("flat_data")  # â† Action #2
# Spark å†æ¬¡åŸ·è¡Œï¼šScan â†’ Select â†’ Drop â†’ Writeï¼ˆå…¨éƒ¨è³‡æ–™ï¼‰
# âŒ é‡è¤‡è¨ˆç®—ï¼ï¼ˆé™¤éæœ‰ cachingï¼‰
```

**æ­£ç¢ºåšæ³•ï¼ˆç§»é™¤ displayï¼‰ï¼š**

```python
# åŸ·è¡Œè¨ˆç•«ï¼ˆä¸å« displayï¼‰ï¼š
rawDF = spark.table("raw_data")
flattenedDF = rawDF.select("*", "values.*")
finalDF = flattenedDF.drop("values")

# display(finalDF)  â† è¨»è§£æ‰æˆ–ç§»é™¤

finalDF.write.mode("append").saveAsTable("flat_data")  # â† å”¯ä¸€çš„ Action
# Spark åŸ·è¡Œä¸€æ¬¡ï¼šScan â†’ Select â†’ Drop â†’ Write
# âœ… é«˜æ•ˆï¼
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A - Cmd 2: printSchema()

**ç‚ºä»€éº¼ä¸æ‡‰è©²ç§»é™¤ï¼Ÿ**

```python
# Cmd 2: rawDF.printSchema()
```

**âœ… ä¿ç•™çš„ç†ç”±ï¼š**

| åŸå›  | èªªæ˜ |
|------|------|
| **Metadata æ“ä½œ** | åªè®€å– schema metadataï¼Œä¸æ¶‰åŠè³‡æ–™æƒæ |
| **é–‹éŠ·æ¥µå°** | ä¸è§¸ç™¼ full scanï¼Œåªæª¢æŸ¥è³‡æ–™çµæ§‹ |
| **é™¤éŒ¯åƒ¹å€¼** | åœ¨ç”Ÿç”¢ç’°å¢ƒæœ‰åŠ©æ–¼é©—è­‰è³‡æ–™çµæ§‹ |
| **æ—¥èªŒè¨˜éŒ„** | è¼¸å‡ºåˆ° logï¼Œæ–¹ä¾¿è¿½è¹¤èˆ‡é™¤éŒ¯ |

**å¯¦éš›è¡Œç‚ºï¼š**

```python
# printSchema() çš„å…§éƒ¨è¡Œç‚º
rawDF.printSchema()
# è¼¸å‡ºåˆ° stdout/log:
# root
#  |-- id: integer (nullable = true)
#  |-- name: string (nullable = true)
#  |-- values: struct (nullable = true)
#  |    |-- value1: string (nullable = true)
#  |    |-- value2: integer (nullable = true)

# âœ… æ²’æœ‰è³‡æ–™ç§»å‹•
# âœ… æ²’æœ‰ full scan
# âœ… åƒ…è®€å– metadata
```

**å¯¦å‹™åƒ¹å€¼ï¼š**

```python
# åœ¨ç”Ÿç”¢ä½œæ¥­ä¸­ä¿ç•™ printSchema() çš„å¥½è™•
try:
    rawDF = spark.table("raw_data")
    rawDF.printSchema()  # â† è¨˜éŒ„åˆ° log
    # ... è™•ç†é‚è¼¯
except Exception as e:
    # è‹¥ç™¼ç”ŸéŒ¯èª¤ï¼Œlog ä¸­æœ‰ schema è³‡è¨Šå¯ä¾›åˆ†æ
    print(f"Error: {e}")
```

---

### B - Cmd 3: select()

**ç‚ºä»€éº¼ä¸æ‡‰è©²ç§»é™¤ï¼Ÿ**

```python
# Cmd 3: flattenedDF = rawDF.select("*", "values.*")
```

**âœ… ä¿ç•™çš„ç†ç”±ï¼š**

| åŸå›  | èªªæ˜ |
|------|------|
| **æ ¸å¿ƒé‚è¼¯** | å°‡å·¢ç‹€çµæ§‹ `values` æ¬„ä½å±•é–‹ï¼ˆflattenï¼‰ |
| **è³‡æ–™è½‰æ›** | å¿…è¦çš„ ETL æ­¥é©Ÿ |
| **Transformation** | ä¸è§¸ç™¼è¨ˆç®—ï¼Œåªå®šç¾©åŸ·è¡Œè¨ˆç•« |

**åŠŸèƒ½èªªæ˜ï¼š**

```python
# åŸå§‹è³‡æ–™çµæ§‹
# +----+------+-------------------+
# | id | name | values            |
# +----+------+-------------------+
# | 1  | A    | {value1: X, ...}  |
# +----+------+-------------------+

# åŸ·è¡Œ select("*", "values.*") å¾Œ
# +----+------+-------------------+--------+--------+
# | id | name | values            | value1 | value2 |
# +----+------+-------------------+--------+--------+
# | 1  | A    | {value1: X, ...}  | X      | 100    |
# +----+------+-------------------+--------+--------+

# âœ… é€™æ˜¯å¿…è¦çš„è³‡æ–™è½‰æ›æ­¥é©Ÿ
```

---

### C - Cmd 4: drop()

**ç‚ºä»€éº¼ä¸æ‡‰è©²ç§»é™¤ï¼Ÿ**

```python
# Cmd 4: finalDF = flattenedDF.drop("values")
```

**âœ… ä¿ç•™çš„ç†ç”±ï¼š**

| åŸå›  | èªªæ˜ |
|------|------|
| **æ¸…ç†è³‡æ–™** | ç§»é™¤å·²å±•é–‹çš„åŸå§‹å·¢ç‹€æ¬„ä½ |
| **é¿å…é‡è¤‡** | å±•é–‹å¾Œçš„ `values` æ¬„ä½å·²ç„¡ç”¨è™• |
| **æœ€ä½³å¯¦å‹™** | ä¿æŒè³‡æ–™è¡¨æ¬„ä½ä¹¾æ·¨ |

**åŠŸèƒ½èªªæ˜ï¼š**

```python
# Cmd 3 åŸ·è¡Œå¾Œ
# +----+------+-------------------+--------+--------+
# | id | name | values (å†—é¤˜)     | value1 | value2 |
# +----+------+-------------------+--------+--------+

# Cmd 4 åŸ·è¡Œå¾Œ
# +----+------+--------+--------+
# | id | name | value1 | value2 |
# +----+------+--------+--------+
# âœ… ç§»é™¤å†—é¤˜çš„å·¢ç‹€çµæ§‹æ¬„ä½
```

---

### D (æ­£ç¢ºç­”æ¡ˆ) - display()

**ç‚ºä»€éº¼å¿…é ˆç§»é™¤ï¼Ÿ**

```python
# Cmd 5: display(finalDF)  â† æ‡‰è©²ç§»é™¤
```

**âŒ å¿…é ˆç§»é™¤çš„ç†ç”±ï¼š**

| å•é¡Œ | å½±éŸ¿ |
|------|------|
| **è§¸ç™¼ Action** | å¼·åˆ¶åŸ·è¡Œè¨ˆç®— |
| **è³‡æ–™æ”¶é›†** | å‚³é€è³‡æ–™åˆ° Driverï¼ˆé è¨­ 1000 rowsï¼‰ |
| **é‡è¤‡è¨ˆç®—** | Cmd 6 æœƒå†æ¬¡åŸ·è¡Œç›¸åŒçš„è½‰æ› |
| **è³‡æºæµªè²»** | CPUã€è¨˜æ†¶é«”ã€ç¶²è·¯é »å¯¬ |
| **ç„¡å¯¦éš›ç”¨é€”** | æ’ç¨‹ä½œæ¥­æ²’æœ‰äº’å‹•ä»‹é¢ |

**æ€§èƒ½å°æ¯”ï¼š**

```python
# âŒ ä¿ç•™ display() çš„åŸ·è¡Œæ™‚é–“
# å‡è¨­è³‡æ–™è¡¨æœ‰ 1,000,000 rows
display(finalDF)  # è¨ˆç®—ä¸¦æ”¶é›† 1000 rows â†’ 2 ç§’
finalDF.write...  # å†æ¬¡è¨ˆç®— 1,000,000 rows â†’ 30 ç§’
# ç¸½è¨ˆï¼š32 ç§’

# âœ… ç§»é™¤ display() çš„åŸ·è¡Œæ™‚é–“
finalDF.write...  # è¨ˆç®— 1,000,000 rows ä¸€æ¬¡ â†’ 30 ç§’
# ç¸½è¨ˆï¼š30 ç§’ï¼ˆçœä¸‹ 2 ç§’ + è¨˜æ†¶é«”ï¼‰
```

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡å°æ¯”

### å£è¨£è¨˜æ†¶
**ã€Œæ’ç¨‹ä½œæ¥­ç„¡äººçœ‹ï¼Œdisplay æŒ‡ä»¤è¦ç§»å®Œã€**  
**ã€Œäº’å‹•é–‹ç™¼ç”¨ displayï¼Œç”Ÿç”¢ä½œæ¥­åˆ¥ä¾è³´ã€**

### äº’å‹•å¼ vs ç”Ÿç”¢ç’°å¢ƒæŒ‡ä»¤å°æ¯”

| æŒ‡ä»¤ | é¡å‹ | é–‹ç™¼ç’°å¢ƒ | ç”Ÿç”¢ç’°å¢ƒ | åŸå›  |
|------|------|----------|----------|------|
| **display()** | Action | âœ… ä½¿ç”¨ | âŒ ç§»é™¤ | äº’å‹•å¼é¡¯ç¤ºï¼Œç„¡ UI ç„¡æ„ç¾© |
| **show()** | Action | âœ… ä½¿ç”¨ | âš ï¸ è¬¹æ… | è¼¸å‡ºåˆ° logï¼Œå°‘é‡è³‡æ–™å¯æ¥å— |
| **printSchema()** | Metadata | âœ… ä½¿ç”¨ | âœ… ä¿ç•™ | è¨˜éŒ„ schemaï¼Œæœ‰åŠ©é™¤éŒ¯ |
| **count()** | Action | âœ… ä½¿ç”¨ | âš ï¸ è¬¹æ… | è§¸ç™¼ full scanï¼Œåƒ…åœ¨éœ€è¦æ™‚ä½¿ç”¨ |
| **cache()** | Action | âœ… ä½¿ç”¨ | âœ… ä¿ç•™ | é¿å…é‡è¤‡è¨ˆç®— |
| **write()** | Action | âœ… ä½¿ç”¨ | âœ… å¿…è¦ | æŒä¹…åŒ–è³‡æ–™ |

### Spark Actions vs Transformations

| é¡åˆ¥ | ç¯„ä¾‹ | åŸ·è¡Œæ™‚æ©Ÿ | æ˜¯å¦å¯åœ¨ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨ |
|------|------|----------|---------------------|
| **Transformations** | select(), filter(), drop() | Lazyï¼ˆå»¶é²åŸ·è¡Œï¼‰ | âœ… æ ¸å¿ƒé‚è¼¯ |
| **Actions (æœ‰ç”¨)** | write(), saveAsTable() | ç«‹å³åŸ·è¡Œ | âœ… æŒä¹…åŒ–è³‡æ–™ |
| **Actions (é™¤éŒ¯)** | show(), count() | ç«‹å³åŸ·è¡Œ | âš ï¸ åƒ…åœ¨å¿…è¦æ™‚ |
| **Actions (äº’å‹•)** | display(), take() | ç«‹å³åŸ·è¡Œ | âŒ åƒ…é–‹ç™¼ç’°å¢ƒ |

### æ‡‰è©²ç§»é™¤/ä¿ç•™çš„æ±ºç­–æ¨¹

```
é€™å€‹æŒ‡ä»¤åœ¨æ’ç¨‹ä½œæ¥­ä¸­æœ‰å¯¦éš›ç”¨é€”å—ï¼Ÿ
â”‚
â”œâ”€ æ˜¯æ ¸å¿ƒè³‡æ–™è½‰æ›ï¼Ÿ
â”‚  â””â”€ âœ… ä¿ç•™ (select, filter, drop, join...)
â”‚
â”œâ”€ æ˜¯è³‡æ–™æŒä¹…åŒ–ï¼Ÿ
â”‚  â””â”€ âœ… ä¿ç•™ (write, saveAsTable, insertInto...)
â”‚
â”œâ”€ æ˜¯é™¤éŒ¯/ç›£æ§ï¼Ÿ
â”‚  â”œâ”€ Metadata æ“ä½œ (printSchema) â†’ âœ… ä¿ç•™
â”‚  â”œâ”€ è¼•é‡æ—¥èªŒ (logger.info) â†’ âœ… ä¿ç•™
â”‚  â””â”€ è³‡æ–™æ”¶é›† (display, take) â†’ âŒ ç§»é™¤
â”‚
â””â”€ æ˜¯äº’å‹•å¼åŠŸèƒ½ï¼Ÿ
   â””â”€ âŒ ç§»é™¤ (display, widgets...)
```

---

## ğŸ’¡ å¯¦å‹™æ‡‰ç”¨

### é–‹ç™¼åˆ°ç”Ÿç”¢çš„è½‰æ›æµç¨‹

**Step 1: é–‹ç™¼éšæ®µï¼ˆå«äº’å‹•æŒ‡ä»¤ï¼‰**

```python
# Development Notebook
rawDF = spark.table("raw_data")
rawDF.printSchema()  # â† æª¢æŸ¥çµæ§‹

# æª¢æŸ¥åŸå§‹è³‡æ–™
display(rawDF.limit(10))  # â† äº’å‹•å¼æŸ¥çœ‹

flattenedDF = rawDF.select("*", "values.*")
display(flattenedDF.limit(10))  # â† é©—è­‰å±•é–‹çµæœ

finalDF = flattenedDF.drop("values")
display(finalDF)  # â† ç¢ºèªæœ€çµ‚çµæœ

finalDF.write.mode("append").saveAsTable("flat_data")
```

**Step 2: æº–å‚™ç”Ÿç”¢ç‰ˆæœ¬ï¼ˆç§»é™¤äº’å‹•æŒ‡ä»¤ï¼‰**

```python
# Production Notebook
rawDF = spark.table("raw_data")
rawDF.printSchema()  # âœ… ä¿ç•™ï¼šè¨˜éŒ„åˆ° log

# display(rawDF.limit(10))  â† âŒ è¨»è§£æ‰

flattenedDF = rawDF.select("*", "values.*")
# display(flattenedDF.limit(10))  â† âŒ è¨»è§£æ‰

finalDF = flattenedDF.drop("values")
# display(finalDF)  â† âŒ è¨»è§£æ‰æˆ–ç§»é™¤

finalDF.write.mode("append").saveAsTable("flat_data")
```

**Step 3: åŠ å…¥ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦å‹™**

```python
# Production Notebook with Best Practices
import logging

logging.info("Starting data pipeline")

# è®€å–è³‡æ–™
rawDF = spark.table("raw_data")
rawDF.printSchema()  # è¨˜éŒ„ schema
logging.info(f"Loaded raw data")

# è½‰æ›è³‡æ–™
flattenedDF = rawDF.select("*", "values.*")
finalDF = flattenedDF.drop("values")

# è³‡æ–™å“è³ªæª¢æŸ¥ï¼ˆè¨˜éŒ„åˆ° logï¼‰
row_count = finalDF.count()  # âš ï¸ é€™æ˜¯ Actionï¼Œä½†æœ‰å¿…è¦
logging.info(f"Processing {row_count} rows")

# æŒä¹…åŒ–
finalDF.write.mode("append").saveAsTable("flat_data")
logging.info("Data written successfully")
```

### ä½¿ç”¨æ¢ä»¶æ¨™è¨˜å€åˆ†ç’°å¢ƒ

```python
# æ™ºèƒ½åŒ–è™•ç†ï¼šæ ¹æ“šç’°å¢ƒæ±ºå®šæ˜¯å¦ display
import os

IS_PRODUCTION = os.getenv("ENV") == "production"

rawDF = spark.table("raw_data")

# é–‹ç™¼ç’°å¢ƒé¡¯ç¤ºï¼Œç”Ÿç”¢ç’°å¢ƒè·³é
if not IS_PRODUCTION:
    display(rawDF.limit(10))

flattenedDF = rawDF.select("*", "values.*")
finalDF = flattenedDF.drop("values")

if not IS_PRODUCTION:
    display(finalDF)

finalDF.write.mode("append").saveAsTable("flat_data")
```

### ä½¿ç”¨ Logger å–ä»£ display

```python
# âœ… æ›´å¥½çš„åšæ³•ï¼šä½¿ç”¨ logger
import logging

logger = logging.getLogger(__name__)

rawDF = spark.table("raw_data")

# å–ä»£ display(rawDF)
logger.info(f"Raw data schema: {rawDF.schema}")
logger.info(f"Raw data sample: {rawDF.limit(5).toPandas().to_dict()}")

flattenedDF = rawDF.select("*", "values.*")
finalDF = flattenedDF.drop("values")

# å–ä»£ display(finalDF)
logger.info(f"Final data count: {finalDF.count()}")
logger.info(f"Final data columns: {finalDF.columns}")

finalDF.write.mode("append").saveAsTable("flat_data")
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶åƒè€ƒ

1. **Databricks display() function:**
   - [Databricks Docs - Display Function](https://docs.databricks.com/notebooks/visualizations/index.html#display-function)

2. **Production Best Practices:**
   - [Databricks Docs - Production Best Practices](https://docs.databricks.com/notebooks/best-practices.html)

3. **Spark Actions vs Transformations:**
   - [Apache Spark Docs - RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

**é‡åˆ°ã€Œå“ªå€‹æŒ‡ä»¤æ‡‰è©²ç§»é™¤ã€é¡Œç›®æ™‚ï¼š**

1. âœ… **è­˜åˆ¥äº’å‹•å¼æŒ‡ä»¤** â†’ display(), widgets, dbutils.notebook.exit()
2. âœ… **æª¢æŸ¥æ˜¯å¦æœ‰å¯¦éš›ç”¨é€”** â†’ åœ¨ç„¡ UI çš„ç’°å¢ƒä¸­èƒ½å¦ç”¢ç”Ÿåƒ¹å€¼
3. âœ… **è©•ä¼°æ€§èƒ½å½±éŸ¿** â†’ æ˜¯å¦è§¸ç™¼ä¸å¿…è¦çš„ Action

**å¿«é€Ÿåˆ¤æ–·æ¨™æº–ï¼š**

| æŒ‡ä»¤ç‰¹å¾µ | åˆ¤æ–· |
|---------|------|
| åŒ…å« `display()` | âŒ ç”Ÿç”¢ç’°å¢ƒæ‡‰ç§»é™¤ |
| åŒ…å« `show()` ä¸”è³‡æ–™é‡å¤§ | âš ï¸ è¬¹æ…ä½¿ç”¨ |
| åŒ…å« `printSchema()` | âœ… å¯ä¿ç•™ï¼ˆmetadata æ“ä½œï¼‰ |
| æ ¸å¿ƒè½‰æ›é‚è¼¯ (select/filter/join) | âœ… å¿…é ˆä¿ç•™ |
| è³‡æ–™æŒä¹…åŒ– (write/saveAsTable) | âœ… å¿…é ˆä¿ç•™ |

**é™·é˜±è­¦ç¤ºï¼š**
- âš ï¸ ä¸è¦è¢« `printSchema()` èª¤å°ï¼Œå®ƒçš„é–‹éŠ·å¾ˆå°
- âš ï¸ `display()` çœ‹èµ·ä¾†ç„¡å®³ï¼Œä½†æœƒè§¸ç™¼å®Œæ•´è¨ˆç®—
- âš ï¸ é¡Œç›®å¼·èª¿ã€Œschedule as a jobã€â†’ é—œéµæç¤ºç‚ºè‡ªå‹•åŒ–ç’°å¢ƒ

**è¨˜æ†¶æŠ€å·§ï¼š**
- å‡¡æ˜¯éœ€è¦ã€Œäººçœ¼æŸ¥çœ‹ã€çš„æŒ‡ä»¤ â†’ ä¸é©åˆç”Ÿç”¢ç’°å¢ƒ
- å‡¡æ˜¯ã€Œäº’å‹•å¼ã€åŠŸèƒ½ â†’ åœ¨æ’ç¨‹ä½œæ¥­ä¸­ç„¡ç”¨æ­¦ä¹‹åœ°
