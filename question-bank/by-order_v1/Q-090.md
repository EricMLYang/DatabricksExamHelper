# Q-090

## é¡Œç›®è³‡è¨Š

**ID:** `Q-090`

**ä¾†æº:** Mock Exam

**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

Which statement regarding Spark configuration on the Databricks platform is true?

### é¸é …

- **A.** The Databricks REST API can be used to modify the Spark configuration properties for an interactive cluster without interrupting jobs currently running on the cluster.
- **B.** Spark configurations set within a notebook will affect all SparkSessions attached to the same interactive cluster.
- **C.** Spark configuration properties can only be set for an interactive cluster by creating a global init script.
- **D.** Spark configuration properties set for an interactive cluster with the Clusters UI will impact all notebooks attached to that cluster.
- **E.** When the same Spark configuration property is set for an interactive cluster and a notebook attached to that cluster, the notebook setting will always be ignored.

---

## æ¨™ç±¤ç³»çµ±

**Topics:** `Cluster-Management`, `Spark-Configuration`, `Databricks-Platform`

**Traps:** `Configuration-Scope`, `Priority-Confusion`

**Domain:** `ç¶­é‹èˆ‡è‡ªå‹•åŒ– (Operations & Automation)`

---

## ç­”æ¡ˆèˆ‡åˆ†æ

### æ­£ç¢ºç­”æ¡ˆ

**æ­£è§£:** `D`

**ç¤¾ç¾¤æŠ•ç¥¨:** D (83%)
**ä¾†æºæ¨™è¨»:** D

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Spark Configuration Management on Databricks
**çŸ¥è­˜é ˜åŸŸ:** ç¶­é‹èˆ‡è‡ªå‹•åŒ– - å¢é›†é…ç½®ç®¡ç†
**é—œéµæ¦‚å¿µ:** 
- Cluster-level vs Notebook-level configuration
- Configuration ä½œç”¨ç¯„åœ (Scope)
- Databricks Cluster UI é…ç½®è¡Œç‚º

### æ¬¡è¦è€ƒé»
- SparkSession èˆ‡ cluster çš„é—œä¿‚
- Configuration å„ªå…ˆé †åº
- Cluster é‡å•Ÿèˆ‡é…ç½®è®Šæ›´

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æŠ€è¡“åŸç†:**

Databricks çš„ Spark Configuration å±¤ç´šæ¶æ§‹ï¼š

```
å±¤ç´š 1: Cluster-level Configuration (Cluster UI)
   â†“ å½±éŸ¿ç¯„åœï¼šæ‰€æœ‰é€£æ¥åˆ°æ­¤ cluster çš„ notebooks
   â†“
å±¤ç´š 2: Notebook-level Configuration (spark.conf.set)
   â†“ å½±éŸ¿ç¯„åœï¼šåƒ…ç•¶å‰ notebook çš„ SparkSession
```

**Cluster UI è¨­å®šçš„å½±éŸ¿ç¯„åœï¼š**

åœ¨ Databricks Cluster UI ä¸­è¨­å®šçš„ Spark configurationï¼š
1. **åœ¨ cluster å•Ÿå‹•æ™‚è¼‰å…¥**
2. **æ‡‰ç”¨æ–¼æ•´å€‹ cluster**
3. **å½±éŸ¿æ‰€æœ‰é™„åŠ çš„ notebooks**
4. **å»ºç«‹ cluster çš„åŸºç¤é…ç½®**

**ç¬¦åˆéœ€æ±‚:**

```
Interactive Cluster æ¶æ§‹ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Interactive Cluster (my-cluster)   â”‚
â”‚  Spark Configs (åœ¨ Cluster UI è¨­å®š):  â”‚
â”‚  - spark.sql.shuffle.partitions=200 â”‚
â”‚  - spark.executor.memory=4g         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“ å½±éŸ¿ç¯„åœ
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“        â†“        â†“
 Notebook1 Notebook2 Notebook3 Notebook4
  (å…¨éƒ¨éƒ½ä½¿ç”¨ç›¸åŒçš„ cluster-level configs)
```

**å¯¦å‹™æ‡‰ç”¨:**

**1. åœ¨ Cluster UI è¨­å®š Spark configurationï¼š**

```
Databricks Workspace â†’ Compute â†’ [Your Cluster] â†’ Edit

Advanced Options â†’ Spark Config:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ spark.sql.shuffle.partitions 200        â”‚
â”‚ spark.executor.memory 4g                â”‚
â”‚ spark.executor.cores 4                  â”‚
â”‚ spark.sql.adaptive.enabled true         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

é€™äº›è¨­å®šæœƒå½±éŸ¿**æ‰€æœ‰é€£æ¥åˆ°æ­¤ cluster çš„ notebooks**ã€‚

**2. é©—è­‰ cluster-level configurationï¼š**

```python
# åœ¨ä»»ä½•é™„åŠ åˆ°æ­¤ cluster çš„ notebook ä¸­åŸ·è¡Œ
print(spark.conf.get("spark.sql.shuffle.partitions"))
# è¼¸å‡º: 200 (ä¾†è‡ª cluster configuration)

print(spark.conf.get("spark.executor.memory"))
# è¼¸å‡º: 4g (ä¾†è‡ª cluster configuration)
```

**3. å¤šå€‹ notebooks å…±äº«ç›¸åŒçš„ cluster configurationï¼š**

```python
# Notebook A
print(f"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}")
# è¼¸å‡º: 200

# Notebook B (é€£æ¥åˆ°åŒä¸€ cluster)
print(f"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}")
# è¼¸å‡º: 200 (ç›¸åŒçš„å€¼ï¼Œå› ç‚ºä¾†è‡ª cluster configuration)

# Notebook C (é€£æ¥åˆ°åŒä¸€ cluster)
print(f"Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}")
# è¼¸å‡º: 200 (ç›¸åŒçš„å€¼)
```

**ç‚ºä½•é€™æ¨£è¨­è¨ˆï¼š**

1. **ä¸€è‡´æ€§ï¼š** ç¢ºä¿æ‰€æœ‰åœ¨åŒä¸€ cluster ä¸Šé‹è¡Œçš„ä»£ç¢¼æœ‰ç›¸åŒçš„åŸ·è¡Œç’°å¢ƒ
2. **æ•ˆèƒ½ç®¡ç†ï¼š** Cluster ç®¡ç†å“¡å¯ä»¥çµ±ä¸€è¨­å®šæœ€ä½³åŒ–åƒæ•¸
3. **è³‡æºæ§åˆ¶ï¼š** çµ±ä¸€ç®¡ç†è¨˜æ†¶é«”ã€æ ¸å¿ƒç­‰è³‡æºé…ç½®

**é…ç½®ç”Ÿæ•ˆæ™‚æ©Ÿï¼š**

```
è¨­å®šè®Šæ›´æµç¨‹ï¼š
1. åœ¨ Cluster UI ä¿®æ”¹ Spark configuration
2. é»æ“Š "Confirm"
3. Cluster éœ€è¦é‡å•Ÿæ‰èƒ½å¥—ç”¨æ–°é…ç½®
4. é‡å•Ÿå¾Œï¼Œæ‰€æœ‰ notebooks éƒ½ä½¿ç”¨æ–°é…ç½®
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - REST API can modify without interrupting

**éŒ¯èª¤åŸå› :** Spark configuration ä¿®æ”¹**éœ€è¦é‡å•Ÿ cluster**ï¼Œæœƒä¸­æ–·æ­£åœ¨é‹è¡Œçš„ä»»å‹™

**è©³ç´°åˆ†æ:**

**Spark configuration çš„ä¸å¯è®Šæ€§ (Immutability)ï¼š**
- Spark configuration åœ¨ SparkContext å»ºç«‹æ™‚è¼‰å…¥
- **ç„¡æ³•åœ¨ runtime å‹•æ…‹ä¿®æ”¹** cluster-level configuration
- ä¿®æ”¹ cluster configuration å¿…é ˆé‡å•Ÿ cluster

**REST API ä¿®æ”¹ cluster configuration çš„å¯¦éš›è¡Œç‚ºï¼š**

```python
# ä½¿ç”¨ REST API æ›´æ–° cluster configuration
import requests

# 1. ç·¨è¼¯ cluster configuration
response = requests.post(
    f"{databricks_host}/api/2.0/clusters/edit",
    headers={"Authorization": f"Bearer {token}"},
    json={
        "cluster_id": "1234-567890-abcd",
        "spark_conf": {
            "spark.sql.shuffle.partitions": "400"  # ä¿®æ”¹é…ç½®
        }
    }
)

# 2. é…ç½®å·²æ›´æ–°ï¼Œä½†éœ€è¦é‡å•Ÿæ‰èƒ½ç”Ÿæ•ˆ
# 3. æ­£åœ¨é‹è¡Œçš„ jobs æœƒè¢«ä¸­æ–·
```

**æ­£ç¢ºæµç¨‹ï¼š**

```
ä¿®æ”¹ cluster configuration (é€é UI æˆ– REST API)
    â†“
Cluster å¿…é ˆé‡å•Ÿ
    â†“
æ­£åœ¨é‹è¡Œçš„ jobs/notebooks æœƒè¢«ä¸­æ–· âœ“
    â†“
é‡å•Ÿå¾Œæ–°é…ç½®ç”Ÿæ•ˆ
```

**æ˜“æ··æ·†é»:**
- å¯èƒ½èª¤ä»¥ç‚º REST API æœ‰ã€Œç†±æ›´æ–°ã€åŠŸèƒ½
- å¯¦éš›ä¸Šä»»ä½• cluster-level configuration è®Šæ›´éƒ½éœ€è¦é‡å•Ÿ

---

### é¸é … B - Notebook settings affect all SparkSessions on cluster

**éŒ¯èª¤åŸå› :** Notebook-level configuration **åªå½±éŸ¿ç•¶å‰ notebook**ï¼Œä¸å½±éŸ¿å…¶ä»– notebooks

**è©³ç´°åˆ†æ:**

**Configuration çš„éš”é›¢æ€§ï¼š**

```python
# Notebook A
spark.conf.set("spark.sql.shuffle.partitions", "100")
print(spark.conf.get("spark.sql.shuffle.partitions"))
# è¼¸å‡º: 100

# Notebook B (é€£æ¥åˆ°åŒä¸€ cluster)
print(spark.conf.get("spark.sql.shuffle.partitions"))
# è¼¸å‡º: 200 (cluster defaultï¼Œæœªå— Notebook A å½±éŸ¿)
```

**ä½œç”¨ç¯„åœå°æ¯”ï¼š**

```
Cluster UI è¨­å®š:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cluster Config   â”‚
â”‚   partitions=200   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ å½±éŸ¿æ‰€æœ‰ notebooks
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ N1 â”‚ N2 â”‚ N3 â”‚ N4 â”‚ â†’ å…¨éƒ¨éƒ½æ˜¯ 200
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

Notebook å…§è¨­å®š:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Notebook A       â”‚
â”‚   spark.conf.set   â”‚
â”‚   partitions=100   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ åªå½±éŸ¿è‡ªå·±
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚ N1 â”‚ N2 â”‚ N3 â”‚ N4 â”‚
â”‚100 â”‚200 â”‚200 â”‚200 â”‚ â†’ åªæœ‰ N1 æ˜¯ 100
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
```

**ç‚ºä½•é€™æ¨£è¨­è¨ˆï¼š**
- å…è¨±ä¸åŒ notebooks æ ¹æ“šéœ€æ±‚è‡ªè¨‚é…ç½®
- ä¸æœƒäº’ç›¸å¹²æ“¾
- æä¾›å½ˆæ€§çš„é…ç½®ç®¡ç†

---

### é¸é … C - Can only be set via global init script

**éŒ¯èª¤åŸå› :** Spark configuration æœ‰**å¤šç¨®è¨­å®šæ–¹å¼**ï¼Œä¸é™æ–¼ init script

**è©³ç´°åˆ†æ:**

**Spark Configuration çš„æ‰€æœ‰è¨­å®šæ–¹å¼ï¼š**

**1. Cluster UI (æœ€å¸¸ç”¨) âœ“**
```
Compute â†’ Edit Cluster â†’ Advanced Options â†’ Spark Config
```

**2. Notebook å…§ç¨‹å¼ç¢¼ âœ“**
```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
```

**3. Init Script âœ“**
```bash
#!/bin/bash
echo "spark.sql.shuffle.partitions 300" >> /databricks/spark/conf/spark-defaults.conf
```

**4. REST API âœ“**
```python
requests.post(f"{host}/api/2.0/clusters/create", json={
    "spark_conf": {"spark.executor.memory": "8g"}
})
```

**5. Cluster Policies âœ“**
```json
{
  "spark_conf.spark.sql.shuffle.partitions": {
    "type": "fixed",
    "value": "200"
  }
}
```

**Init Script çš„é©ç”¨å ´æ™¯ï¼š**
- éœ€è¦åŸ·è¡Œè¤‡é›œçš„ç³»çµ±å±¤ç´šé…ç½®
- å®‰è£é¡å¤–çš„ç¨‹å¼åº«æˆ–å·¥å…·
- ä¿®æ”¹éœ€è¦ root æ¬Šé™çš„è¨­å®š
- **ä¸æ˜¯å”¯ä¸€çš„é…ç½®æ–¹å¼**

**æ˜“æ··æ·†é»:**
é¸é …ä½¿ç”¨ã€Œonlyã€é€™å€‹çµ•å°è©ï¼Œä»»ä½•æœ‰ã€Œonlyã€çš„é¸é …éƒ½è¦ç‰¹åˆ¥å°å¿ƒ

---

### é¸é … E - Notebook setting will always be ignored

**éŒ¯èª¤åŸå› :** Notebook-level configuration **ä¸æœƒè¢«å¿½ç•¥**ï¼Œåè€Œæœƒ**è¦†è“‹ cluster-level** è¨­å®š

**è©³ç´°åˆ†æ:**

**Configuration å„ªå…ˆé †åºï¼ˆç”±é«˜åˆ°ä½ï¼‰ï¼š**

```
å„ªå…ˆé †åº 1: Notebook-level (spark.conf.set)  â† æœ€é«˜å„ªå…ˆ
å„ªå…ˆé †åº 2: Cluster-level (Cluster UI)
å„ªå…ˆé †åº 3: Spark Default
```

**å¯¦éš›è¡Œç‚ºé©—è­‰ï¼š**

```python
# å‡è¨­ Cluster UI è¨­å®šäº†ï¼š
# spark.sql.shuffle.partitions = 200

# åœ¨ Notebook ä¸­ï¼š
print(f"åˆå§‹å€¼: {spark.conf.get('spark.sql.shuffle.partitions')}")
# è¼¸å‡º: åˆå§‹å€¼: 200 (ä¾†è‡ª cluster)

# åœ¨ Notebook ä¸­è¦†å¯«ï¼š
spark.conf.set("spark.sql.shuffle.partitions", "100")
print(f"è¦†å¯«å¾Œ: {spark.conf.get('spark.sql.shuffle.partitions')}")
# è¼¸å‡º: è¦†å¯«å¾Œ: 100 (notebook setting ç”Ÿæ•ˆï¼)

# åŸ·è¡ŒæŸ¥è©¢æ™‚ä½¿ç”¨ 100 å€‹ partitions
df = spark.range(1000000).repartition(10)
df.write.mode("overwrite").saveAsTable("test_table")
# ä½¿ç”¨ 100 å€‹ shuffle partitions (notebook setting)
```

**å°æ¯”æ­£ç¢ºè¡Œç‚ºï¼š**

| é…ç½®ä¾†æº | æœƒè¢«å¿½ç•¥å—ï¼Ÿ | å„ªå…ˆé †åº | å½±éŸ¿ç¯„åœ |
|---------|-----------|---------|---------|
| Notebook-level | âŒ å¦ï¼Œæœƒç”Ÿæ•ˆ | æœ€é«˜ | ç•¶å‰ notebook |
| Cluster-level | âŒ å¦ï¼Œæœƒç”Ÿæ•ˆ | ä¸­ç­‰ | æ‰€æœ‰ notebooks |
| Spark Default | âŒ å¦ï¼Œæœƒç”Ÿæ•ˆ | æœ€ä½ | ç³»çµ±é è¨­ |

**ç‚ºä½•é¸é … E éŒ¯èª¤ï¼š**
- èªªã€Œalways be ignoredã€å®Œå…¨ç›¸å
- å¯¦éš›ä¸Š notebook setting æœ‰**æœ€é«˜å„ªå…ˆæ¬Š**
- é€™æ˜¯ Spark çš„æ¨™æº–è¡Œç‚º

**é‡è¦æ³¨æ„äº‹é …ï¼š**

æŸäº›é…ç½®**ç„¡æ³•åœ¨ notebook ä¸­è¦†å¯«**ï¼ˆä¾‹å¦‚ executor memoryï¼‰ï¼Œä½†å¤§éƒ¨åˆ†é…ç½®å¯ä»¥ï¼š

```python
# å¯ä»¥è¦†å¯«çš„é…ç½®ï¼š
spark.conf.set("spark.sql.shuffle.partitions", "50")  âœ…
spark.conf.set("spark.sql.adaptive.enabled", "true")  âœ…
spark.conf.set("spark.sql.files.maxPartitionBytes", "64MB")  âœ…

# ç„¡æ³•åœ¨ runtime ä¿®æ”¹çš„é…ç½®ï¼š
spark.conf.set("spark.executor.memory", "8g")  âŒ 
# éŒ¯èª¤: Cannot modify spark.executor.memory at runtime
```

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### å£è¨£
**ã€ŒCluster é…ç½®å½±éŸ¿å…¨éƒ¨ï¼ŒNotebook é…ç½®åªå½±éŸ¿è‡ªå·±ã€**

### é—œéµå°æ¯”è¡¨

| é…ç½®æ–¹å¼ | è¨­å®šä½ç½® | å½±éŸ¿ç¯„åœ | å„ªå…ˆé †åº | éœ€è¦é‡å•Ÿ |
|---------|---------|---------|---------|---------|
| **Cluster UI** | Compute â†’ Edit | æ‰€æœ‰ notebooks âœ“ | ä¸­ | æ˜¯ |
| **Notebook code** | spark.conf.set() | ç•¶å‰ notebook | é«˜ | å¦ |
| **Init Script** | Cluster â†’ Init Scripts | æ‰€æœ‰ notebooks | ä¸­ | æ˜¯ |
| **REST API** | API call | æ‰€æœ‰ notebooks | ä¸­ | æ˜¯ |

### Configuration ä½œç”¨ç¯„åœè¦–è¦ºåŒ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Interactive Cluster                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Cluster-level Configuration       â”‚   â”‚
â”‚  â”‚   (Set in Cluster UI)               â”‚   â”‚
â”‚  â”‚   spark.sql.shuffle.partitions=200  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â†“ å¥—ç”¨åˆ°æ‰€æœ‰ notebooks         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Notebook1 â”‚Notebook2 â”‚Notebook3     â”‚   â”‚
â”‚  â”‚  (200)   â”‚  (200)   â”‚  (200)       â”‚   â”‚
â”‚  â”‚          â”‚          â”‚ â†“ å¯è¦†å¯«      â”‚   â”‚
â”‚  â”‚          â”‚          â”‚spark.conf.setâ”‚   â”‚
â”‚  â”‚          â”‚          â”‚  (100)       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

çµè«–ï¼š
- Cluster UI é…ç½®å½±éŸ¿æ‰€æœ‰ notebooks âœ… (é¸é … D æ­£ç¢º)
- Notebook å¯ä»¥è¦†å¯« cluster é…ç½® âœ“
- Notebook é…ç½®ä¸å½±éŸ¿å…¶ä»– notebooks âœ“
```

### åˆ¤æ–·æŠ€å·§

**è­˜åˆ¥é—œéµå­—ï¼š**
- âœ… "impact all notebooks attached to that cluster" â†’ æ­£ç¢ºæè¿° cluster-level config
- âŒ "without interrupting" â†’ éŒ¯èª¤ï¼Œé…ç½®è®Šæ›´éœ€è¦é‡å•Ÿ
- âŒ "affect all SparkSessions" â†’ éŒ¯èª¤ï¼Œnotebook config åªå½±éŸ¿è‡ªå·±
- âŒ "only be set" â†’ éŒ¯èª¤ï¼Œã€Œonlyã€é€šå¸¸æ˜¯é™·é˜±
- âŒ "always be ignored" â†’ éŒ¯èª¤ï¼Œã€Œalwaysã€å¤ªçµ•å°

**å¿«é€Ÿæ’é™¤æ³•ï¼š**

```
é¸é …åŒ…å«ã€Œonlyã€æˆ–ã€Œalwaysã€ï¼Ÿ
    â†“ æ˜¯
  å¯èƒ½æ˜¯é™·é˜±ï¼Œä»”ç´°æª¢æŸ¥

é¸é …æåˆ°ã€Œwithout interruptingã€ä¿®æ”¹ cluster configï¼Ÿ
    â†“ æ˜¯
  éŒ¯èª¤ï¼cluster config ä¿®æ”¹éœ€è¦é‡å•Ÿ

é¸é …èªª notebook config å½±éŸ¿å…¶ä»– notebooksï¼Ÿ
    â†“ æ˜¯
  éŒ¯èª¤ï¼notebook config åªå½±éŸ¿è‡ªå·±

é¸é …èªª cluster UI config å½±éŸ¿æ‰€æœ‰ notebooksï¼Ÿ
    â†“ æ˜¯
  æ­£ç¢ºï¼âœ… é¸ D
```

### å¯¦å‹™æœ€ä½³å¯¦è¸

**1. ä½•æ™‚ä½¿ç”¨ Cluster-level Configurationï¼š**
```
é©ç”¨å ´æ™¯ï¼š
âœ“ éœ€è¦ç¢ºä¿æ‰€æœ‰ notebooks ä½¿ç”¨ä¸€è‡´çš„é…ç½®
âœ“ æ•ˆèƒ½å„ªåŒ–è¨­å®šï¼ˆè¨˜æ†¶é«”ã€æ ¸å¿ƒæ•¸ï¼‰
âœ“ åœ˜éšŠå…±äº«çš„æ¨™æº–é…ç½®
âœ“ é•·æœŸç©©å®šçš„é…ç½®

ç¯„ä¾‹ï¼š
- spark.executor.memory: 4g
- spark.executor.cores: 4
- spark.sql.adaptive.enabled: true
```

**2. ä½•æ™‚ä½¿ç”¨ Notebook-level Configurationï¼š**
```
é©ç”¨å ´æ™¯ï¼š
âœ“ ç‰¹å®š notebook éœ€è¦ä¸åŒçš„é…ç½®
âœ“ æš«æ™‚æ€§çš„æ¸¬è©¦é…ç½®
âœ“ é‡å°ç‰¹å®šå·¥ä½œè² è¼‰çš„å„ªåŒ–
âœ“ ä¸å½±éŸ¿å…¶ä»–ä½¿ç”¨è€…

ç¯„ä¾‹ï¼š
spark.conf.set("spark.sql.shuffle.partitions", "50")  # å°è³‡æ–™é›†
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "50MB")
```

**3. Configuration è¨­å®šå»ºè­°ï¼š**

```python
# åœ¨ Notebook é–‹é ­è¨­å®šå¸¸ç”¨é…ç½®
# æ–¹ä¾¿è¿½è¹¤å’Œç‰ˆæœ¬æ§åˆ¶

# é‡å°å¤§è³‡æ–™é›†
spark.conf.set("spark.sql.shuffle.partitions", "400")
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# é‡å°å°è³‡æ–™é›†
spark.conf.set("spark.sql.shuffle.partitions", "50")
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "100MB")

# é™¤éŒ¯æ¨¡å¼
spark.conf.set("spark.sql.adaptive.enabled", "false")
spark.conf.set("spark.sql.shuffle.partitions", "10")
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

### å®˜æ–¹æ–‡ä»¶
- [Databricks - Cluster configuration](https://docs.databricks.com/clusters/configure.html)
- [Databricks - Spark configuration](https://docs.databricks.com/clusters/configure.html#spark-configuration)
- [Apache Spark - Configuration](https://spark.apache.org/docs/latest/configuration.html)

### ç›¸é—œæ¦‚å¿µ
- SparkSession vs SparkContext
- Cluster Policies ç®¡ç†é…ç½®
- Init Scripts é€²éšæ‡‰ç”¨
- Dynamic Resource Allocation

### å¯¦å‹™ç¯„ä¾‹

**1. æŸ¥çœ‹æ‰€æœ‰ Spark Configurationï¼š**

```python
# æ–¹æ³• 1: æŸ¥çœ‹æ‰€æœ‰é…ç½®
all_configs = spark.sparkContext.getConf().getAll()
for key, value in sorted(all_configs):
    print(f"{key} = {value}")

# æ–¹æ³• 2: æŸ¥çœ‹ç‰¹å®šé…ç½®
print(spark.conf.get("spark.sql.shuffle.partitions"))
print(spark.conf.get("spark.executor.memory"))
print(spark.conf.get("spark.executor.cores"))
```

**2. æ¢ä»¶å¼é…ç½®è¨­å®šï¼š**

```python
# æ ¹æ“šè³‡æ–™å¤§å°å‹•æ…‹èª¿æ•´
data_size_gb = 100

if data_size_gb < 10:
    spark.conf.set("spark.sql.shuffle.partitions", "50")
elif data_size_gb < 100:
    spark.conf.set("spark.sql.shuffle.partitions", "200")
else:
    spark.conf.set("spark.sql.shuffle.partitions", "400")

print(f"Using {spark.conf.get('spark.sql.shuffle.partitions')} partitions")
```

**3. æ•ˆèƒ½å„ªåŒ–é…ç½®çµ„åˆï¼š**

```python
# é€šç”¨æ•ˆèƒ½å„ªåŒ–é…ç½®
performance_configs = {
    "spark.sql.adaptive.enabled": "true",
    "spark.sql.adaptive.coalescePartitions.enabled": "true",
    "spark.sql.adaptive.skewJoin.enabled": "true",
    "spark.sql.files.maxPartitionBytes": "128MB",
    "spark.sql.shuffle.partitions": "200"
}

for key, value in performance_configs.items():
    spark.conf.set(key, value)
    print(f"Set {key} = {value}")
```

**4. ä½¿ç”¨ REST API æ›´æ–° cluster configurationï¼š**

```python
import requests
import json

databricks_host = "https://your-workspace.cloud.databricks.com"
token = "your-token"

# ç·¨è¼¯ cluster configuration
cluster_config = {
    "cluster_id": "1234-567890-abcdefgh",
    "spark_conf": {
        "spark.sql.shuffle.partitions": "400",
        "spark.executor.memory": "8g",
        "spark.executor.cores": "4"
    }
}

response = requests.post(
    f"{databricks_host}/api/2.0/clusters/edit",
    headers={
        "Authorization": f"Bearer {token}",
        "Content-Type": "application/json"
    },
    json=cluster_config
)

if response.status_code == 200:
    print("Cluster configuration updated successfully")
    print("Note: Restart cluster to apply changes")
else:
    print(f"Error: {response.text}")
```

---

## ğŸ”— ç›¸é—œé¡Œç›®
- Q-XXX: Cluster å•Ÿå‹•èˆ‡é‡å•Ÿè¡Œç‚º
- Q-XXX: Init Scripts åŸ·è¡Œé †åº
- Q-XXX: Spark Configuration æ•ˆèƒ½èª¿æ ¡
- Q-XXX: Cluster Policies æœ€ä½³å¯¦è¸
