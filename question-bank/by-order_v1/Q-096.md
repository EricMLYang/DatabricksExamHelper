# Question #96

---

## 題目資訊

### 題目編號
**ID:** `Q-096`

### 來源
**來源:** Mock Exam / Community Contributed

### 難度等級
**難度:** `L2-Intermediate`

---

## 題目內容

### 題幹

A junior data engineer is migrating a workload from a relational database system to the Databricks Lakehouse. The source system uses a star schema, leveraging foreign key constraints and multi-table inserts to validate records on write.

Which consideration will impact the decisions made by the engineer while migrating this workload?

### 選項

- **A.** Databricks only allows foreign key constraints on hashed identifiers, which avoid collisions in highly-parallel writes.
- **B.** Databricks supports Spark SQL and JDBC; all logic can be directly migrated from the source system without refactoring.
- **C.** Committing to multiple tables simultaneously requires taking out multiple table locks and can lead to a state of deadlock.
- **D.** All Delta Lake transactions are ACID compliant against a single table, and Databricks does not enforce foreign key constraints.
- **E.** Foreign keys must reference a primary key field; multi-table inserts must leverage Delta Lake's upsert functionality.

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Delta-Lake`, `Data-Modeling`, `ETL-Patterns`

### Trap Tags (陷阱類型標籤)
**Traps:** `Feature-Assumption`, `Concept-Confusion`

### Knowledge Domain (知識領域)
**Domain:** Data Engineering / Lakehouse Architecture

---

## 答案與來源

### 正確答案
**正解:** `D`

### 答案來源
- **來源標註答案:** D
- **社群投票答案:** D (100%)

---

# 題目解析

---

## 題目回顧

### 題目編號與連結
**題目 ID:** `Q-096`
**題目連結:** [點此返回題目](#question-96)

### 正確答案
**正解:** `D`

---

## 📍 考點識別

### 主要考點
**核心技術:** Delta Lake ACID Transactions & Schema Constraints
**知識領域:** Lakehouse Architecture / Data Migration Patterns
**關鍵概念:**
- Delta Lake 的 ACID 事務保證範圍（單表級別）
- Databricks 不強制執行外鍵約束（referential integrity）
- 從 RDBMS 遷移到 Lakehouse 的架構差異

### 次要考點
- Star Schema 資料建模
- 傳統資料庫的約束機制 vs Lakehouse 的資料驗證策略

---

## ✅ 正解說明

### 為什麼 D 是正確的？

**技術原理:**

Delta Lake 提供 **ACID 事務保證**，但這些保證是 **針對單一表格（single table）** 的：
- **Atomicity（原子性）:** 單一表格的寫入操作要麼全部成功，要麼全部失敗
- **Consistency（一致性）:** 單表內的資料符合定義的約束條件
- **Isolation（隔離性）:** 並行操作不會互相干擾
- **Durability（持久性）:** 已提交的變更會永久保存

然而，Databricks **不強制執行外鍵約束（foreign key constraints）**：
- 可以在 DDL 中 **宣告** 外鍵，但僅作為 metadata 使用
- 系統 **不會在寫入時驗證** 外鍵參照完整性（referential integrity）
- 不會阻止寫入違反外鍵約束的資料

**符合需求:**

題目問的是「遷移過程中需要考慮的因素」，工程師必須知道：

1. **無法依賴系統自動驗證外鍵** - 需要在應用層或 ETL 邏輯中實作參照完整性檢查
2. **無跨表事務保證** - 無法像傳統 RDBMS 那樣在多表間執行原子性操作
3. **需要重新設計驗證邏輯** - 原本依賴資料庫層級的約束，現在必須透過其他機制實現

**實務應用:**

```sql
-- ❌ Databricks 中可以宣告，但不會強制執行
CREATE TABLE orders (
  order_id INT PRIMARY KEY,
  customer_id INT,
  FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
);

-- ⚠️ 即使 customers 表中不存在該 customer_id，下列插入仍會成功
INSERT INTO orders VALUES (1, 999);

-- ✅ 正確做法：在 ETL 中先驗證
spark.sql("""
  SELECT o.*
  FROM orders o
  LEFT JOIN customers c ON o.customer_id = c.customer_id
  WHERE c.customer_id IS NULL
""")
-- 檢查是否有孤兒記錄（orphan records）
```

**遷移策略建議:**
- 使用 `Delta Lake Constraints` (CHECK constraints) 驗證欄位值
- 在 ETL pipeline 中實作參照完整性檢查
- 使用 `MERGE` 操作確保關聯資料的一致性
- 考慮使用 Delta Live Tables 的 Expectations 進行資料品質管理

---

## ❌ 錯誤選項排除

### 選項 A - "Databricks only allows foreign key constraints on hashed identifiers..."

**錯誤原因:** 完全錯誤的臆測功能

**詳細分析:**
- Databricks **根本不強制執行** 外鍵約束，更不用說「只能用在 hashed identifiers 上」
- 沒有「避免高並行寫入碰撞」的特殊限制
- 這個選項是杜撰出來的功能描述

**易混淆點:**
可能讓人聯想到「分散式系統需要特殊的 ID 策略」，但這與外鍵約束的執行機制無關。

---

### 選項 B - "Databricks supports Spark SQL and JDBC; all logic can be directly migrated..."

**錯誤原因:** 過度樂觀的假設，忽略架構差異

**詳細分析:**
- 雖然 Databricks 支援 Spark SQL 和 JDBC，但 **無法直接遷移** 所有邏輯：
  - **外鍵約束不會被強制執行** - 需要重新實作驗證邏輯
  - **Multi-table transactions 的語義不同** - RDBMS 的跨表事務需要重新設計
  - **Stored procedures / triggers 不支援** - 需要轉換為 Spark jobs 或 Delta Live Tables
- 語法相似 ≠ 功能完全相容

**易混淆點:**
「支援 SQL」讓人誤以為可以無痛遷移，但 Lakehouse 與 RDBMS 的底層架構差異很大。

---

### 選項 C - "Committing to multiple tables simultaneously requires taking out multiple table locks..."

**錯誤原因:** 將 RDBMS 的鎖定機制套用到 Databricks

**詳細分析:**
- Delta Lake 使用 **optimistic concurrency control（樂觀並行控制）**，而非傳統的 **table-level locking**
- **不會發生 deadlock（死鎖）** - 因為寫入時不需要預先取得表鎖
- 衝突處理機制：使用 transaction log 檢查版本，衝突時重試（retry）

**易混淆點:**
傳統資料庫工程師可能習慣「表鎖 + 死鎖」的思維模式，但這不適用於 Delta Lake。

---

### 選項 E - "Foreign keys must reference a primary key field; multi-table inserts must leverage Delta Lake's upsert..."

**錯誤原因:** 混合了部分正確與錯誤的概念

**詳細分析:**
- **前半句** 是傳統 RDBMS 的規則，但 Databricks 不強制執行外鍵
- **後半句** 提到 MERGE (upsert) 是正確的功能，但與「multi-table inserts」沒有必然關聯
- MERGE 操作是 **針對單一表格** 的 upsert，無法跨表原子性插入

**易混淆點:**
看起來很專業，結合了「外鍵規則 + MERGE 功能」，但邏輯上不連貫。

---

## 🧠 記憶法與對比

### 記憶口訣
**「Delta ACID 單表保，外鍵宣告不強制」**
- **單表保**: ACID 事務保證僅限單一表格
- **不強制**: 外鍵可以宣告（DDL），但系統不會驗證

### 關鍵對比表

| 特性 | RDBMS (如 MySQL, PostgreSQL) | Databricks Delta Lake |
|------|------------------------------|----------------------|
| **外鍵約束** | ✅ 強制執行 | ❌ 僅作為 metadata，不強制 |
| **ACID 範圍** | ✅ 支援跨表事務 | ⚠️ 僅單表級別 |
| **約束類型** | Primary Key, Foreign Key, Unique | CHECK Constraints, NOT NULL |
| **驗證時機** | 寫入時自動驗證 | 需在應用層或 ETL 中實作 |
| **鎖定機制** | Pessimistic locking (悲觀鎖) | Optimistic concurrency (樂觀並行) |

### 實務決策樹

```
遷移 RDBMS 到 Databricks 時：
├─ 外鍵約束？
│  └─ 不會自動強制 → 需在 ETL 中實作檢查邏輯
├─ 多表事務？
│  └─ 不支援 → 需重新設計為單表操作 + 應用層協調
└─ 資料驗證？
   └─ 使用 Delta Constraints、Expectations 或自定義邏輯
```

---

## 📚 延伸學習

### 官方文件
- [Delta Lake Constraints](https://docs.databricks.com/delta/constraints.html)
- [Delta Lake ACID Guarantees](https://docs.databricks.com/delta/index.html#delta-lake-acid-guarantees)
- [Migrate from Data Warehouse to Databricks](https://docs.databricks.com/migration/index.html)

### 相關題目
- Q-XXX: Delta Lake Constraints 的使用方式
- Q-XXX: MERGE 操作的語法與最佳實踐
- Q-XXX: Delta Live Tables Expectations 的資料品質管理

---

## 🎯 重點提醒

> **核心觀念:** Databricks Lakehouse 不是傳統的 RDBMS，外鍵約束不會被強制執行，ACID 事務保證僅限單一表格。遷移時必須重新設計資料驗證與參照完整性檢查邏輯。

**考試陷阱:**
- ❌ 誤以為 Databricks 會自動驗證外鍵
- ❌ 假設可以直接遷移 RDBMS 的所有功能
- ❌ 套用 RDBMS 的鎖定機制思維

**正確思維:**
- ✅ Delta Lake = ACID within single table
- ✅ 外鍵 = metadata only（僅供查詢優化器參考）
- ✅ 需在 ETL / application 層實作驗證邏輯
