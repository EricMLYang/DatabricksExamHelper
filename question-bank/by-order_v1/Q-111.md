# Question #111

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-111`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L1-Basic`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

Which describes a method of installing a Python package **scoped at the notebook level** to **all nodes** in the currently active cluster?

### é¸é …

- **A.** Run `source env/bin/activate` in a notebook setup script
- **B.** Use `%conda install` in a notebook cell
- **C.** Use `%pip install` in a notebook cell
- **D.** Use `%sh pip install` in a notebook cell
- **E.** Install libraries from PyPI using the cluster UI

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Python`, `Package-Management`, `Notebook`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Magic-Command-Confusion`, `Scope-Misunderstanding`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Development

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C` (ç¤¾ç¾¤å…±è­˜ 100%)

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** D (éŒ¯èª¤)
- **ç¤¾ç¾¤å…±è­˜:** C (100% æŠ•ç¥¨)
- **å¯¦å‹™é©—è­‰:** C æ˜¯æ­£ç¢ºç­”æ¡ˆ

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Databricks Notebook Package Management  
**é—œéµæ¦‚å¿µ:** `%pip` magic commandã€Notebook-scoped librariesã€All nodes installation

**é¡Œç›®é—œéµå­—ï¼š**
- **Notebook level** (notebook å±¤ç´šä½œç”¨åŸŸ)
- **All nodes** (æ‰€æœ‰ç¯€é»ï¼ŒåŒ…å« driver å’Œ workers)
- **Currently active cluster** (ç•¶å‰åŸ·è¡Œä¸­çš„ cluster)

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ C (%pip install) æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

```python
# âœ… æ­£ç¢ºæ–¹å¼ï¼šä½¿ç”¨ %pip install
%pip install pandas==2.0.0

# ç‰¹æ€§ï¼š
# 1. Notebook-scopedï¼ˆåªå½±éŸ¿ç•¶å‰ notebookï¼‰
# 2. å®‰è£åˆ°æ‰€æœ‰ç¯€é»ï¼ˆdriver + all workersï¼‰
# 3. ç«‹å³ç”Ÿæ•ˆ
# 4. ä¸å½±éŸ¿å…¶ä»– notebook
```

**%pip çš„æ ¸å¿ƒå„ªå‹¢ï¼š**

| ç‰¹æ€§ | èªªæ˜ |
|------|------|
| **Notebook-scoped** | åªåœ¨ç•¶å‰ notebook æœ‰æ•ˆï¼Œä¸å½±éŸ¿ cluster å…¨åŸŸ |
| **All nodes** | è‡ªå‹•å®‰è£åˆ° driver å’Œæ‰€æœ‰ worker ç¯€é» |
| **å³æ™‚ç”Ÿæ•ˆ** | å®‰è£å¾Œç«‹å³å¯ç”¨ï¼Œç„¡éœ€é‡å•Ÿ |
| **éš”é›¢æ€§** | ä¸åŒ notebook å¯ä½¿ç”¨ä¸åŒç‰ˆæœ¬å¥—ä»¶ |
| **æ¨è–¦æ–¹å¼** | Databricks å®˜æ–¹æ¨è–¦çš„æœ€ä½³å¯¦å‹™ |

**å·¥ä½œåŸç†ï¼š**

```
ä½¿ç”¨è€…åŸ·è¡Œï¼š%pip install requests

Databricks è¡Œç‚ºï¼š
  â†“
1. åœ¨ Driver ç¯€é»åŸ·è¡Œ pip install requests
  â†“
2. åŒæ™‚åœ¨æ‰€æœ‰ Worker ç¯€é»åŸ·è¡Œ pip install requests
  â†“
3. æ›´æ–° notebook çš„ Python ç’°å¢ƒ
  â†“
4. å®Œæˆï¼ˆæ‰€æœ‰ç¯€é»éƒ½æœ‰ requests å¥—ä»¶ï¼‰
```

**å¯¦éš›ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
# Notebook Cell 1: å®‰è£å¥—ä»¶
%pip install pandas==2.0.0 numpy scikit-learn

# Notebook Cell 2: ç«‹å³ä½¿ç”¨
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

df = pd.DataFrame({'x': [1, 2, 3], 'y': [2, 4, 6]})
print(df)  # âœ… æ­£å¸¸é‹ä½œ

# åœ¨ workers ä¸ŠåŸ·è¡Œçš„ UDF ä¹Ÿèƒ½ä½¿ç”¨
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf("double")
def multiply_by_two(s: pd.Series) -> pd.Series:
    return s * 2  # âœ… workers ä¹Ÿæœ‰ pandas

spark.range(10).select(multiply_by_two("id")).show()
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A - `source env/bin/activate`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```bash
# é¸é … A
source env/bin/activate  # Unix shell å‘½ä»¤
```

**âŒ å•é¡Œï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **ä¸æ˜¯ Python å®‰è£** | é€™åªæ˜¯å•Ÿå‹•è™›æ“¬ç’°å¢ƒï¼Œä¸å®‰è£å¥—ä»¶ |
| **èªæ³•éŒ¯èª¤** | Databricks notebook ä¸æ”¯æ´é€™ç¨® shell èªæ³• |
| **ç„¡æ³•è·¨ç¯€é»** | å³ä½¿èƒ½åŸ·è¡Œï¼Œä¹Ÿåªå½±éŸ¿ driver ç¯€é» |
| **ä¸ç¬¦éœ€æ±‚** | é¡Œç›®å•çš„æ˜¯ã€Œå®‰è£å¥—ä»¶ã€ï¼Œä¸æ˜¯ã€Œå•Ÿå‹•ç’°å¢ƒã€|

### B - `%conda install`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … B
%conda install pandas
```

**âŒ å•é¡Œï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **ä¸æ”¯æ´** | Databricks ä¸æ”¯æ´ `%conda` magic command |
| **åƒ… Anaconda** | conda æ˜¯ Anaconda å°ˆç”¨ï¼ŒDatabricks ä½¿ç”¨ pip |
| **æœƒå ±éŒ¯** | åŸ·è¡Œæœƒé¡¯ç¤º "Unknown magic command" |

**Databricks æ”¯æ´çš„ magic commandsï¼š**
- âœ… `%pip` - å®‰è£ Python å¥—ä»¶
- âœ… `%sh` - åŸ·è¡Œ shell å‘½ä»¤ï¼ˆä½†ä¸æ¨è–¦ç”¨æ–¼å®‰è£å¥—ä»¶ï¼‰
- âœ… `%python`, `%scala`, `%sql`, `%r` - åˆ‡æ›èªè¨€
- âŒ `%conda` - ä¸æ”¯æ´

### D - `%sh pip install` (ä¾†æºæ¨™è¨»ç­”æ¡ˆï¼Œä½†éŒ¯èª¤)

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … Dï¼ˆä¾†æºç­”æ¡ˆï¼Œä½†ä¸æ­£ç¢ºï¼‰
%sh pip install pandas
```

**âŒ å•é¡Œï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **åªå®‰è£åˆ° Driver** | `%sh` åªåœ¨ driver ç¯€é»åŸ·è¡Œï¼Œä¸æœƒåˆ†ç™¼åˆ° workers |
| **Workers ç„¡æ³•ä½¿ç”¨** | åˆ†æ•£å¼é‹ç®—æ™‚æœƒæ‰¾ä¸åˆ°å¥—ä»¶ |
| **é Notebook-scoped** | å¯èƒ½å½±éŸ¿ç³»çµ±å…¨åŸŸç’°å¢ƒ |
| **ä¸æ˜¯æœ€ä½³å¯¦å‹™** | Databricks å®˜æ–¹ä¸æ¨è–¦æ­¤æ–¹å¼ |

**%sh pip install çš„å•é¡Œç¤ºç¯„ï¼š**

```python
# Cell 1: ä½¿ç”¨ %sh pip installï¼ˆéŒ¯èª¤æ–¹å¼ï¼‰
%sh pip install requests
# âœ… Driver å®‰è£æˆåŠŸ

# Cell 2: åœ¨ Driver ä½¿ç”¨ï¼ˆå¯ä»¥ï¼‰
import requests
response = requests.get("https://api.example.com")
print(response.status_code)  # âœ… åœ¨ driver ä¸Šæ­£å¸¸é‹ä½œ

# Cell 3: åœ¨ Workers ä½¿ç”¨ï¼ˆå¤±æ•—ï¼ï¼‰
from pyspark.sql.functions import udf

@udf("string")
def fetch_data(url):
    import requests  # âŒ ModuleNotFoundError on workers!
    return requests.get(url).text

# åŸ·è¡Œæœƒå¤±æ•—ï¼Œå› ç‚º workers æ²’æœ‰ requests å¥—ä»¶
spark.range(10).select(fetch_data(lit("https://api.example.com"))).show()
# âŒ éŒ¯èª¤: ModuleNotFoundError: No module named 'requests'
```

**å°æ¯”ï¼š**

| å‘½ä»¤ | å®‰è£ç¯„åœ | é©ç”¨æ€§ |
|------|---------|--------|
| `%pip install` | Driver + Workers âœ… | æ¨è–¦ |
| `%sh pip install` | åƒ… Driver âŒ | ä¸æ¨è–¦ |

### E - Cluster UI Installation

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … E
# é€é Cluster UI å®‰è£å¥—ä»¶
```

**âŒ å•é¡Œï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **Cluster-scoped** | å®‰è£åˆ°æ•´å€‹ clusterï¼Œä¸æ˜¯ notebook-scoped |
| **å½±éŸ¿æ‰€æœ‰ notebooks** | è©² cluster ä¸Šçš„æ‰€æœ‰ notebook éƒ½æœƒä½¿ç”¨è©²å¥—ä»¶ |
| **éœ€è¦é‡å•Ÿ** | é€šå¸¸éœ€è¦é‡å•Ÿ cluster æ‰ç”Ÿæ•ˆ |
| **ä¸ç¬¦é¡Œç›®è¦æ±‚** | é¡Œç›®æ˜ç¢ºè¦æ±‚ "notebook level" |

**Cluster UI å®‰è£çš„ç‰¹æ€§ï¼š**

```
Cluster UI å®‰è£:
  â†“
Cluster-level libraries
  â†“
æ‰€æœ‰ä½¿ç”¨è©² cluster çš„ notebooks éƒ½æœƒæœ‰æ­¤å¥—ä»¶
  â†“
âŒ ä¸ç¬¦åˆ "notebook level" çš„è¦æ±‚
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€ŒNotebook è£å¥—ä»¶ï¼Œ%pip æœ€æ­£ç¢ºã€**

```
%pip install â†’ Notebook-scoped + All nodes âœ…
%sh pip install â†’ Only driver âŒ
Cluster UI â†’ Cluster-scoped âŒ
```

### ä¸‰ç¨®å®‰è£ç¯„åœå°æ¯”

| æ–¹å¼ | ä½œç”¨åŸŸ | ç¯€é» | é‡å•Ÿ | æ¨è–¦åº¦ |
|------|--------|------|------|--------|
| **%pip install** | Notebook | All nodes | âŒ å¦ | â­â­â­â­â­ |
| %sh pip install | Notebook | Driver only | âŒ å¦ | â­ |
| Cluster UI | Cluster | All nodes | âœ… æ˜¯ | â­â­â­ |
| Init Script | Cluster | All nodes | âœ… æ˜¯ | â­â­â­â­ |

### Magic Commands é€ŸæŸ¥

```python
# âœ… æ¨è–¦ä½¿ç”¨
%pip install package_name          # Notebook-scoped, all nodes
%pip install package_name==1.2.3   # æŒ‡å®šç‰ˆæœ¬
%pip install -r requirements.txt   # æ‰¹æ¬¡å®‰è£

# âŒ ä¸æ¨è–¦
%sh pip install package_name       # åªå®‰è£åˆ° driver
%conda install package_name        # ä¸æ”¯æ´

# â„¹ï¸ å…¶ä»–å¸¸ç”¨ magic commands
%python  # åˆ‡æ›åˆ° Python
%scala   # åˆ‡æ›åˆ° Scala
%sql     # åŸ·è¡Œ SQL
%sh      # åŸ·è¡Œ shell å‘½ä»¤
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Notebook-scoped Libraries](https://docs.databricks.com/libraries/notebooks-python-libraries.html)
- [%pip Magic Command](https://docs.databricks.com/libraries/notebooks-python-libraries.html#pip-commands)
- [Library Installation Best Practices](https://docs.databricks.com/libraries/index.html)

---

## ğŸ’¡ å¯¦å‹™å»ºè­°

### æœ€ä½³å¯¦å‹™

```python
# âœ… æ¨è–¦ï¼šä½¿ç”¨ %pip install
%pip install pandas numpy scikit-learn

# âœ… æŒ‡å®šç‰ˆæœ¬ï¼ˆé¿å…è¡çªï¼‰
%pip install pandas==2.0.0

# âœ… å¾ requirements.txt å®‰è£
%pip install -r /dbfs/requirements.txt

# âœ… éœé»˜å®‰è£ï¼ˆæ¸›å°‘è¼¸å‡ºï¼‰
%pip install -q pandas

# âš ï¸ æª¢æŸ¥å·²å®‰è£çš„å¥—ä»¶
%pip list

# âš ï¸ è§£é™¤å®‰è£
%pip uninstall -y pandas
```

### å®‰è£æ–¹å¼é¸æ“‡æŒ‡å—

| å ´æ™¯ | æ¨è–¦æ–¹å¼ | åŸå›  |
|------|---------|------|
| **å–®ä¸€ notebook æ¸¬è©¦** | `%pip install` | å¿«é€Ÿã€éš”é›¢ |
| **å¤šå€‹ notebooks å…±ç”¨** | Cluster UI | çµ±ä¸€ç‰ˆæœ¬ |
| **ç”Ÿç”¢ç’°å¢ƒ** | Init Script | ç©©å®šã€å¯é‡ç¾ |
| **è‡¨æ™‚è©¦é©—** | `%pip install` | ä¸å½±éŸ¿å…¶ä»–äºº |

### å¸¸è¦‹å•é¡Œè™•ç†

```python
# å•é¡Œ 1: ç‰ˆæœ¬è¡çª
# è§£æ±º: æ˜ç¢ºæŒ‡å®šç‰ˆæœ¬
%pip install pandas==2.0.0 numpy==1.24.0

# å•é¡Œ 2: å®‰è£å¾Œ import å¤±æ•—
# è§£æ±º: é‡å•Ÿ Python kernel
dbutils.library.restartPython()

# å•é¡Œ 3: éœ€è¦ç³»çµ±å¥—ä»¶
# è§£æ±º: ä½¿ç”¨ %sh å®‰è£ç³»çµ±å¥—ä»¶ï¼Œ%pip å®‰è£ Python å¥—ä»¶
%sh apt-get update && apt-get install -y libpq-dev
%pip install psycopg2

# å•é¡Œ 4: ç§æœ‰ PyPI æº
# è§£æ±º: æŒ‡å®š index-url
%pip install --index-url https://private.pypi.org/simple/ my-package
```

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

**é¡Œç›®é—œéµå­—è­˜åˆ¥ï¼š**

```
"notebook level" + "all nodes"
  â†“
éœ€è¦: Notebook-scoped + åˆ†ç™¼åˆ°æ‰€æœ‰ç¯€é»
  â†“
ç­”æ¡ˆ: %pip installï¼ˆé¸é … Cï¼‰âœ…
```

**æ’é™¤æ³•ï¼š**

1. âŒ A - `source env/bin/activate`ï¼ˆä¸æ˜¯å®‰è£å¥—ä»¶ï¼‰
2. âŒ B - `%conda install`ï¼ˆDatabricks ä¸æ”¯æ´ï¼‰
3. âœ… C - `%pip install`ï¼ˆæ­£ç¢ºç­”æ¡ˆï¼‰
4. âŒ D - `%sh pip install`ï¼ˆåªå®‰è£åˆ° driverï¼‰
5. âŒ E - Cluster UIï¼ˆcluster-scopedï¼Œé notebook-scopedï¼‰

### ç­”æ¡ˆçˆ­è­°èªªæ˜

**ç‚ºä»€éº¼ç¤¾ç¾¤é¸ C è€Œé Dï¼Ÿ**

| é¸é … | ç¯€é»è¦†è“‹ | Notebook-scoped | å¯¦å‹™é©—è­‰ | å®˜æ–¹æ¨è–¦ |
|------|---------|----------------|---------|---------|
| **C (%pip)** | âœ… All nodes | âœ… æ˜¯ | âœ… æ­£ç¢º | âœ… æ˜¯ |
| **D (%sh pip)** | âŒ Driver only | âš ï¸ éƒ¨åˆ† | âŒ Workers å¤±æ•— | âŒ å¦ |

**å¯¦éš›æ¸¬è©¦è­‰æ˜ï¼š**
- `%pip install` ç¢ºå¯¦æœƒå®‰è£åˆ°æ‰€æœ‰ç¯€é»
- `%sh pip install` åªæœƒå®‰è£åˆ° driver ç¯€é»
- åœ¨åˆ†æ•£å¼é‹ç®—æ™‚ï¼Œé¸é … D æœƒå°è‡´ workers æ‰¾ä¸åˆ°å¥—ä»¶

---

## âš ï¸ é‡é»ç¸½çµ

### æ ¸å¿ƒè¦é»

1. **%pip install æ˜¯ Databricks æ¨è–¦çš„æ–¹å¼**
   - Notebook-scopedï¼ˆä¸å½±éŸ¿å…¶ä»– notebooksï¼‰
   - è‡ªå‹•åˆ†ç™¼åˆ°æ‰€æœ‰ç¯€é»ï¼ˆdriver + workersï¼‰
   - ç«‹å³ç”Ÿæ•ˆï¼Œç„¡éœ€é‡å•Ÿ

2. **é¿å…ä½¿ç”¨ %sh pip install**
   - åªå®‰è£åˆ° driver ç¯€é»
   - Workers ç„¡æ³•ä½¿ç”¨ï¼Œæœƒå°è‡´åˆ†æ•£å¼é‹ç®—å¤±æ•—

3. **Cluster UI å®‰è£æ˜¯ cluster-scoped**
   - ä¸ç¬¦åˆ "notebook level" çš„è¦æ±‚
   - å½±éŸ¿æ•´å€‹ cluster çš„æ‰€æœ‰ notebooks

### è¨˜æ†¶å£è¨£

**ã€ŒNotebook å±¤ç´šè£ï¼Œ%pip åˆ†ç™¼å¼·ã€**
- %pip = Notebook-scoped + All nodes
- ç¤¾ç¾¤å…±è­˜ 100% é¸ Cï¼Œå¯¦å‹™é©—è­‰æ­£ç¢º

---

**[è¿”å›é¡Œç›®](#question-111)**