# Question #14

---

## 題目資訊

### 題目編號
**ID:** `Q-01-014`

### 來源
**來源:** Real Exam Recall

### 難度等級
**難度:** `L3-Advanced`

---

## 題目內容

### 題幹

An hourly batch job is configured to ingest data files from a cloud object storage container where each batch represent all records produced by the source system in a given hour.

The batch job to process these records into the Lakehouse is sufficiently delayed to ensure no late-arriving data is missed.

The user_id field represents a unique key for the data, which has the following schema: 

```
user_id BIGINT, username STRING, user_utc STRING, user_region STRING, last_login BIGINT, auto_pay BOOLEAN, last_updated BIGINT
```

New records are all ingested into a table named account_history which maintains a full record of all data in the same schema as the source.

The next table in the system is named account_current and is implemented as a Type 1 table representing the most recent value for each unique user_id.

Assuming there are millions of user accounts and tens of thousands of records processed hourly, which implementation can be used to efficiently update the described account_current table as part of each hourly batch job?

### 選項

- **A.** Use Auto Loader to subscribe to new files in the account_history directory; configure a Structured Streaming trigger once job to batch update newly detected files into the account_current table.
- **B.** Overwrite the account_current table with each batch using the results of a query against the account_history table grouping by user_id and filtering for the max value of last_updated.
- **C.** Filter records in account_history using the last_updated field and the most recent hour processed, as well as the max last_login by user_id write a merge statement to update or insert the most recent value for each user_id.
- **D.** Use Delta Lake version history to get the difference between the latest version of account_history and one version prior, then write these records to account_current.
- **E.** Filter records in account_history using the last_updated field and the most recent hour processed, making sure to deduplicate on username; write a merge statement to update or insert the most recent value for each username.

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Type-1-SCD`, `MERGE`, `Incremental-Processing`, `Performance-Tuning`

### Trap Tags (陷阱類型標籤)
**Traps:** `Key-Selection`, `Filter-Logic`

### Knowledge Domain (知識領域)
**Domain:** `Data Engineering`

---

## 答案與解析連結

### 正確答案
**正解:** `C`

### 解析檔案
**詳細解析:** [點此查看解析](../analysis/Q-01-014-analysis.md)

---

## 相關資源

### 官方文件
- [Slowly Changing Dimensions](https://docs.databricks.com/delta/merge.html#slowly-changing-data-scd-type-1)
- [MERGE INTO Performance](https://docs.databricks.com/delta/merge.html#performance-tuning)
