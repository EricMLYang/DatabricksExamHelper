# Q-076 - Delta Lake Schema Evolution 與歷史資料限制

---

## 題目資訊

### 題目編號
**ID:** `Q-076`

### 來源
**來源:** Real Exam Recall (實際考試回憶 - Question #276)

### 難度等級
**難度:** `L2-Intermediate`

---

## 題目內容

### 題幹（中文）

一條資料管線使用 Structured Streaming 從 Apache Kafka 攝取資料到 Delta Lake。資料被儲存在 bronze 表中，包含 Kafka 產生的時間戳記（timestamp）、鍵（key）和值（value）。在管線部署三個月後，資料工程團隊注意到在某些時段出現延遲問題。

一位資深資料工程師更新了 Delta 表的 schema 和攝取邏輯，以包含當前時間戳記（由 Apache Spark 記錄）以及 Kafka 主題（topic）和分區（partition）。團隊計劃使用這些額外的元資料欄位來診斷暫時性的處理延遲。

團隊在診斷此問題時將面臨哪個限制？

### 題幹（英文）

A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.

A senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.

Which limitation will the team face while diagnosing this problem?

### 選項

- **A.** New fields will not be computed for historic records.
- **B.** Spark cannot capture the topic and partition fields from a Kafka source.
- **C.** New fields cannot be added to a production Delta table.
- **D.** Updating the table schema will invalidate the Delta transaction log metadata.
- **E.** Updating the table schema requires a default value provided for each field added.

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Delta-Lake`, `Delta-Schema-Evolution`, `Streaming`

### Trap Tags (陷阱類型標籤)
**Traps:** `Schema-Evolution-Misconception`, `Historical-Data-Limitation`

### Knowledge Domain (知識領域)
**Domain:** `資料轉換與管理 (Data Transformation & Management)`, `資料攝取 (Data Ingestion)`

---

## 答案與解析

**✅ 正確答案:** A

---

# 📝 詳細解析

> **考點:** Delta Lake Schema Evolution 的歷史資料限制  
> **難度:** L2-Intermediate  
> **重要性:** ⭐⭐⭐⭐⭐ (生產環境常見問題)

---

## 📍 考點識別

### 主要考點
**Delta Lake Schema Evolution 與資料回填限制**
- 技術領域：Delta Lake Schema 管理
- 核心概念：Schema Evolution 只影響新資料，不回填歷史資料
- 實務場景：新增元資料欄位進行問題診斷

### 知識領域
**對應考綱:** [Part 1 - 資料開發與轉換](../../docs/exam-map/part-1-開發與轉換.md) (32%)  
**相關章節:** Delta Lake Schema Evolution、Streaming 資料品質

### 關鍵概念
1. **Schema Evolution** - Delta Lake 支援動態新增欄位
2. **歷史資料不變性** - 已寫入的資料不會被自動更新
3. **診斷窗口限制** - 新欄位只能用於診斷未來資料
4. **Kafka 元資料** - Topic、Partition、Timestamp 可從 Kafka 擷取

### 次要考點
- Structured Streaming 與 Kafka 整合
- 生產環境問題診斷策略
- Delta Lake Transaction Log 機制

---

## ✅ 正解說明

### 為什麼選項 A 是正確的？

#### 技術原理

**Delta Lake Schema Evolution 的運作機制：**

1. **新增欄位只影響新資料：**
   - 當使用 `.option("mergeSchema", "true")` 或 `.option("overwriteSchema", "true")` 新增欄位時
   - Delta Lake 只會在**新寫入的資料**中填充這些欄位
   - **歷史資料保持不變**，新欄位的值為 `null`

2. **資料不可變性原則：**
   - Delta Lake 遵循「寫入後不可變」(Write-Once) 原則
   - 已經寫入的 Parquet 檔案不會被自動修改
   - 要更新歷史資料需要執行明確的 UPDATE 或 MERGE 操作

3. **診斷範圍受限：**
   ```
   管線時間軸：
   ├─ 0 個月 → 部署管線（舊 schema）
   ├─ 3 個月 → 發現延遲問題
   └─ 3 個月+ → 更新 schema 加入新欄位
   
   診斷能力：
   ✅ 可診斷：更新後的新資料（3 個月+）
   ❌ 無法診斷：歷史資料（0-3 個月）← 缺少新欄位！
   ```

#### 符合需求

題目情境的限制分析：

| 需求 | Schema Evolution 能做到嗎 | 限制 |
|------|-------------------------|------|
| 新增 Spark timestamp | ✅ 可以 | 只對未來資料有效 |
| 新增 Kafka topic/partition | ✅ 可以 | 只對未來資料有效 |
| 診斷**過去 3 個月**的延遲 | ❌ 不行 | **歷史資料沒有新欄位** |
| 診斷**未來**的延遲 | ✅ 可以 | 新資料包含完整元資料 |

**核心問題：**
團隊想用新欄位診斷「某些時段」的延遲問題，但這些延遲發生在過去 3 個月，而歷史資料並不包含新增的欄位。

#### 實務應用

**原始 Schema（部署時）：**
```python
from pyspark.sql.functions import col

# 原始攝取邏輯
kafka_stream = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load()
  .select(
    col("timestamp").alias("kafka_timestamp"),  # Kafka 產生的時間
    col("key"),
    col("value")
  ))

# 寫入 bronze 表
(kafka_stream.writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/checkpoints/bronze")
  .start("/delta/bronze_orders"))
```

**更新後的 Schema（3 個月後）：**
```python
from pyspark.sql.functions import col, current_timestamp

# 新的攝取邏輯：加入 Spark timestamp 和 Kafka metadata
kafka_stream_enhanced = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load()
  .select(
    col("timestamp").alias("kafka_timestamp"),
    col("key"),
    col("value"),
    current_timestamp().alias("spark_timestamp"),  # 🆕 新增
    col("topic"),                                   # 🆕 新增
    col("partition")                                # 🆕 新增
  ))

# 寫入時啟用 Schema Evolution
(kafka_stream_enhanced.writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/checkpoints/bronze_v2")
  .option("mergeSchema", "true")  # ⚠️ 允許 schema 變更
  .start("/delta/bronze_orders"))
```

#### 效能與成本考量

**如果要回填歷史資料（可行但不實際）：**

```python
# 方案 1: UPDATE（極高成本）
spark.sql("""
  UPDATE bronze_orders
  SET 
    spark_timestamp = kafka_timestamp,  -- 用 Kafka timestamp 估算
    topic = 'orders',                    -- 手動填入
    partition = 0                        -- 無法準確回填
  WHERE spark_timestamp IS NULL
""")
# ❌ 問題：
#    - 需要重寫大量 Parquet 檔案（3 個月資料）
#    - 無法準確取得當時的 Spark timestamp
#    - 無法得知歷史資料來自哪個 partition
```

**正確的處理方式：**
```python
# 接受限制，只分析未來資料
# 或者從 Kafka 的 consumer metrics 中補充歷史診斷資訊
```

---

## ❌ 錯誤選項排除

### 選項 B - Spark cannot capture topic and partition

#### 為什麼錯誤？

- ❌ **這是完全錯誤的技術陳述！**
- ❌ Spark 的 Kafka connector **完全可以**擷取 topic 和 partition 資訊
- ❌ 這些是 Kafka 標準的 metadata 欄位

#### 技術事實

**Kafka DataFrame 的完整 Schema：**
```python
kafka_df = (spark.read
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load())

kafka_df.printSchema()
# 輸出：
# root
#  |-- key: binary (nullable = true)
#  |-- value: binary (nullable = true)
#  |-- topic: string (nullable = true)        ✅ 可用！
#  |-- partition: integer (nullable = true)   ✅ 可用！
#  |-- offset: long (nullable = true)
#  |-- timestamp: timestamp (nullable = true)
#  |-- timestampType: integer (nullable = true)
```

**實際使用範例：**
```python
# 完全可以擷取 topic 和 partition
from pyspark.sql.functions import col

enriched_df = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders,payments,shipments")
  .load()
  .select(
    col("topic"),          # ✅ 正常運作
    col("partition"),      # ✅ 正常運作
    col("offset"),
    col("timestamp"),
    col("key"),
    col("value")
  ))

# 按 topic 分組統計
enriched_df.groupBy("topic", "partition").count().show()
# +----------+---------+-----+
# |topic     |partition|count|
# +----------+---------+-----+
# |orders    |0        |1234 |
# |orders    |1        |1189 |
# |payments  |0        |2345 |
# +----------+---------+-----+
```

#### 陷阱設計

考官想測試考生是否真正使用過 Kafka + Spark 整合，還是只是死背概念。

**記憶法：** Kafka connector 提供**完整的 metadata 欄位**，包括 topic, partition, offset, timestamp。

---

### 選項 C - New fields cannot be added to production Delta table

#### 為什麼錯誤？

- ❌ **Delta Lake 的核心特性就是支援 Schema Evolution！**
- ❌ 生產環境的 Delta 表**完全可以**新增欄位
- ❌ 這是 Delta Lake 相比傳統資料倉儲的重要優勢

#### Delta Lake Schema Evolution 能力

**支援的 Schema 變更：**

| 操作 | 支援程度 | 如何啟用 |
|------|---------|---------|
| **新增欄位** | ✅ 完全支援 | `.option("mergeSchema", "true")` |
| **刪除欄位** | ⚠️ 不建議 | 只能標記為廢棄 |
| **修改欄位型別** | ⚠️ 有限制 | 向上轉型可以（int→long） |
| **重新命名欄位** | ⚠️ 需特殊處理 | 使用 RENAME COLUMN |

**新增欄位的三種方式：**

```python
# 方式 1: mergeSchema (推薦)
df_with_new_columns.write \
  .format("delta") \
  .mode("append") \
  .option("mergeSchema", "true") \
  .save("/delta/table")

# 方式 2: overwriteSchema (謹慎使用)
df_with_new_columns.write \
  .format("delta") \
  .mode("overwrite") \
  .option("overwriteSchema", "true") \
  .save("/delta/table")

# 方式 3: SQL ALTER TABLE
spark.sql("""
  ALTER TABLE bronze_orders 
  ADD COLUMNS (
    spark_timestamp TIMESTAMP,
    topic STRING,
    partition INT
  )
""")
```

**生產環境最佳實踐：**
```python
# 測試環境驗證新 schema
test_df.write.format("delta").mode("append") \
  .option("mergeSchema", "true") \
  .save("/test/bronze_orders")

# 驗證 schema 正確
spark.read.format("delta").load("/test/bronze_orders").printSchema()

# 確認無誤後部署到生產環境（完全可行！）
prod_df.write.format("delta").mode("append") \
  .option("mergeSchema", "true") \
  .save("/prod/bronze_orders")  # ✅ 生產環境正常運作
```

#### 陷阱設計

測試考生是否了解 Delta Lake 的 Schema Evolution 特性，以及是否誤以為生產環境無法變更 schema。

**記憶法：** Delta Lake = **彈性 Schema**，生產環境可安全新增欄位。

---

### 選項 D - Updating schema invalidates transaction log

#### 為什麼錯誤？

- ❌ **Schema 更新不會使 Transaction Log 失效！**
- ❌ Delta Lake 的 Transaction Log 設計就是為了追蹤 schema 變更
- ❌ Schema 變更會被記錄在 Transaction Log 中，而非破壞它

#### Delta Lake Transaction Log 機制

**Transaction Log 的作用：**
```
_delta_log/
├── 00000000000000000000.json  ← 初始 schema
├── 00000000000000000001.json
├── 00000000000000000002.json
├── ...
├── 00000000000000012345.json  ← Schema 更新記錄在此
└── 00000000000000012346.json  ← 繼續正常運作
```

**Schema 更新的 Transaction Log 條目：**
```json
// 00000000000000012345.json
{
  "metaData": {
    "schemaString": "{\"type\":\"struct\",\"fields\":[
      {\"name\":\"kafka_timestamp\",\"type\":\"timestamp\"},
      {\"name\":\"key\",\"type\":\"binary\"},
      {\"name\":\"value\",\"type\":\"binary\"},
      {\"name\":\"spark_timestamp\",\"type\":\"timestamp\"},  // 🆕 新增
      {\"name\":\"topic\",\"type\":\"string\"},               // 🆕 新增
      {\"name\":\"partition\",\"type\":\"integer\"}           // 🆕 新增
    ]}",
    "configuration": {},
    "createdTime": 1736899200000
  }
}
```

**讀取時自動處理 Schema Evolution：**
```python
# Delta Lake 自動從 Transaction Log 讀取最新 schema
df = spark.read.format("delta").load("/delta/bronze_orders")

# ✅ 舊檔案（舊 schema）：新欄位自動填 NULL
# ✅ 新檔案（新 schema）：包含完整欄位
# ✅ Transaction Log 完全有效！
```

**Transaction Log 失效的真正原因（與 schema 無關）：**
- 手動刪除 `_delta_log/` 目錄
- 檔案系統損壞
- 並行寫入衝突（極罕見，有自動重試機制）

#### 陷阱設計

混淆「schema 變更」與「transaction log 損壞」的概念，測試對 Delta Lake 內部機制的理解。

**記憶法：** Transaction Log **記錄** schema 變更，不會被**破壞**。

---

### 選項 E - Schema update requires default values

#### 為什麼錯誤？

- ❌ **Delta Lake 新增欄位不需要提供預設值！**
- ❌ 新欄位在歷史資料中會自動填充 `NULL`（如果是 nullable）
- ❌ 只有在 SQL `ALTER TABLE` 中明確加入 `NOT NULL` 約束時才需要預設值

#### Delta Lake 欄位預設值規則

**新增 nullable 欄位（最常見）：**
```python
# 不需要預設值
df_with_new_columns.write \
  .format("delta") \
  .mode("append") \
  .option("mergeSchema", "true") \
  .save("/delta/table")

# 歷史資料自動處理：
# 舊記錄的新欄位 = NULL（自動）
# 新記錄的新欄位 = 實際值
```

**SQL 新增欄位（也不需要預設值）：**
```sql
-- 不需要 DEFAULT 子句
ALTER TABLE bronze_orders 
ADD COLUMNS (
  spark_timestamp TIMESTAMP,
  topic STRING,
  partition INT
);

-- 讀取時：
SELECT * FROM bronze_orders LIMIT 2;
-- 舊記錄: spark_timestamp=NULL, topic=NULL, partition=NULL
-- 新記錄: spark_timestamp='2026-01-14 10:00:00', topic='orders', partition=0
```

**唯一需要預設值的情況（NOT NULL 約束）：**
```sql
-- ❌ 這會失敗（舊資料無法滿足 NOT NULL）
ALTER TABLE bronze_orders 
ADD COLUMNS (
  spark_timestamp TIMESTAMP NOT NULL
);
-- Error: Cannot add NOT NULL column to table with existing data

-- ✅ 正確做法：先加 nullable 欄位，回填後再加約束
ALTER TABLE bronze_orders 
ADD COLUMNS (spark_timestamp TIMESTAMP);

UPDATE bronze_orders SET spark_timestamp = kafka_timestamp WHERE spark_timestamp IS NULL;

ALTER TABLE bronze_orders 
ALTER COLUMN spark_timestamp SET NOT NULL;
```

#### 概念對比

| 資料庫系統 | 新增欄位是否需要預設值？ |
|-----------|----------------------|
| **Delta Lake** | ❌ 不需要（nullable 自動填 NULL） |
| **Traditional SQL** | ⚠️ 依情況（NOT NULL 需要） |
| **Parquet（靜態）** | ❌ 不支援 schema 變更 |

#### 陷阱設計

混淆 Delta Lake 與傳統 SQL 資料庫的 schema 變更規則，測試對 Delta Lake 靈活性的理解。

**記憶法：** Delta Lake 新增欄位 = **自動 NULL**，不需要預設值。

---

## 🧠 記憶法與技巧

### 記憶口訣

**「Schema 往前看，歷史看不見」**
- **Schema 往前看** = Schema Evolution 只影響未來資料
- **歷史看不見** = 歷史資料不會被自動回填新欄位

### 對比表（重要！）

| 項目 | Schema Evolution 做得到？ | 說明 |
|------|-------------------------|------|
| **新增欄位到未來資料** | ✅ 是 | 新寫入的資料包含新欄位 |
| **自動回填歷史資料** | ❌ 否 | 歷史資料新欄位 = NULL |
| **手動回填（UPDATE）** | ✅ 可以（但成本高） | 需重寫 Parquet 檔案 |
| **從 Kafka 擷取 metadata** | ✅ 是 | Topic, Partition 都可擷取 |
| **在生產環境變更 schema** | ✅ 是 | Delta Lake 完全支援 |
| **破壞 Transaction Log** | ❌ 否 | Schema 變更被記錄，不破壞 |

### 決策樹

```
需要新增欄位到 Delta 表？
├─ 只需要未來資料有新欄位？
│  └─ ✅ 使用 mergeSchema，直接新增
└─ 需要歷史資料也有新欄位？
   ├─ 可以接受 NULL 值？
   │  └─ ⚠️ 直接新增（歷史 = NULL）
   └─ 必須有實際值？
      └─ ❌ 需要執行 UPDATE（高成本）
         或從其他來源補資料
```

### 實務記憶

**Streaming 問題診斷的正確策略：**

```python
# ❌ 錯誤：依賴 schema evolution 診斷歷史問題
# 新增欄位後，歷史資料仍缺少診斷資訊

# ✅ 正確：多管齊下
# 1. 立即新增欄位（為未來診斷做準備）
streaming_df_enhanced.writeStream \
  .option("mergeSchema", "true") \
  .start("/delta/bronze")

# 2. 同時從其他來源取得歷史診斷資訊
# - Kafka consumer metrics
# - Databricks job logs
# - Spark event logs
# - 系統監控指標

# 3. 建立完整的診斷視圖
# 歷史診斷 + 未來診斷
```

### 延伸閱讀

- [Schema Evolution Best Practices](https://docs.databricks.com/delta/delta-batch.html#-automatic-schema-evolution) - 生產環境 schema 變更策略
- [Backfilling Data in Delta Lake](https://docs.delta.io/latest/delta-update.html) - 如何安全地回填歷史資料

---

## 🎯 考試技巧

### 快速判斷法

看到「新增欄位」+「診斷歷史問題」關鍵字時：

1. ✅ **核心限制：歷史資料不會被自動更新** - 這是最常見的陷阱
2. ❌ **排除技術不可行的選項** - Spark 可以讀 Kafka metadata, Delta 支援 schema evolution
3. ❌ **排除誇大的選項** - Transaction log 不會失效，不需要預設值

### 陷阱識別

此題包含的陷阱類型：

| 陷阱類型 | 體現選項 | 識別技巧 |
|---------|---------|---------|
| **歷史資料限制** | A（正確答案） | Schema evolution 不溯及既往 |
| **技術能力誤解** | B | Spark+Kafka 完全支援 metadata |
| **Schema Evolution 誤解** | C | Delta Lake 核心特性 |
| **Transaction Log 誤解** | D | Schema 變更被記錄，不破壞 |
| **預設值誤解** | E | Nullable 欄位不需要預設值 |

### 關鍵字提示

題目中的線索：
- ✅ **「three months after」** → 有歷史資料
- ✅ **「updates the schema」** → Schema Evolution
- ✅ **「diagnose」** → 需要歷史資訊
- ✅ **「limitation」** → 找出 schema evolution 的限制

---

## 🔗 相關題目

建議按此順序練習：

1. **[Q-007](./Q-007.md)** - Delta Lake 基礎操作（建議先做）
2. **[Q-012](./Q-012.md)** - Structured Streaming 基礎（Streaming 概念）
3. **[Q-075](./Q-075.md)** - Delta Lake MERGE 去重（相關主題）
4. **[Q-023](./Q-023.md)** - Delta Lake VACUUM（schema 相關）

---

## 💡 實務補充

### 生產環境最佳實踐

```python
# 完整的 Streaming + Schema Evolution 模式
from pyspark.sql.functions import col, current_timestamp, from_json

# 1. 定義可演化的 schema
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType

# 初始 schema
base_schema = StructType([
    StructField("kafka_timestamp", TimestampType()),
    StructField("key", StringType()),
    StructField("value", StringType())
])

# 增強 schema（加入診斷欄位）
enhanced_schema = base_schema.add(
    StructField("spark_timestamp", TimestampType())
).add(
    StructField("topic", StringType())
).add(
    StructField("partition", IntegerType())
)

# 2. 啟用 Schema Evolution 的 Streaming
(spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load()
  .select(
    col("timestamp").alias("kafka_timestamp"),
    col("key").cast("string"),
    col("value").cast("string"),
    current_timestamp().alias("spark_timestamp"),
    col("topic"),
    col("partition")
  )
  .writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/checkpoints/bronze")
  .option("mergeSchema", "true")  # ⚠️ 關鍵設定
  .start("/delta/bronze_orders"))

# 3. 建立診斷查詢（僅適用於新資料）
spark.sql("""
  CREATE OR REPLACE VIEW latency_analysis AS
  SELECT 
    DATE(kafka_timestamp) as date,
    HOUR(kafka_timestamp) as hour,
    topic,
    partition,
    AVG(unix_timestamp(spark_timestamp) - unix_timestamp(kafka_timestamp)) as avg_latency_sec,
    COUNT(*) as record_count
  FROM bronze_orders
  WHERE spark_timestamp IS NOT NULL  -- ⚠️ 排除歷史資料
  GROUP BY date, hour, topic, partition
  HAVING avg_latency_sec > 5  -- 找出延遲 > 5 秒的情況
  ORDER BY avg_latency_sec DESC
""")
```




## 新增欄位的底層行為（transaction log + Parquet）

* **新增欄位發生在 `_delta_log`：**
  schema 變更會寫進新的 Delta transaction（metadata/schema 更新），形成新的 table version。
* **讀取時欄位怎麼「長出來」：**
  查表會先讀 `_delta_log` 決定「這個版本的 table schema」，所以你在查詢結果會看到新欄位已存在。
* **新舊 Parquet 怎麼補數據：**

  * **新寫入的 Parquet 檔**：包含新欄位的實體資料，所以欄位有值。
  * **舊 Parquet 檔**：檔案內沒有新欄位，讀取時做 schema projection，缺欄位回傳 `null`。
    → 所以新欄位只對「更新後寫入」的資料有效，歷史資料不會自動被計算/回填。

---

## schema evaluation 範疇有哪些？哪些動作算 schema evaluation？

常見 schema evaluation（Spark/Delta 的 schema 相關檢查與對齊）可拆成三塊：

1. **Write-time schema enforcement（寫入時）**

   * 檢查 DataFrame schema 是否符合 table schema（欄位名、型別、nullable、欄位順序/映射規則等）
   * 不符合時寫入失敗或需要設定允許演進
     ✅ 這塊算 schema evaluation

2. **Write-time schema evolution / metadata update（寫入時演進）**

   * 允許新增欄位/調整 schema，並把新 schema 寫進新的 `_delta_log` transaction
     ✅ 這塊算 schema evaluation（因為它決定 schema 是否可被接受並更新 metadata）

3. **Read-time schema-on-read / projection（讀取時）**

   * 讀取先用 `_delta_log` 的 schema 當 target schema
   * 對每個 Parquet 檔做欄位投影與對齊：缺欄位補 `null`、型別依規則轉換或報錯
     ✅ 這塊也算 schema evaluation

你上面描述的流程裡：

* 「新增欄位靠新的 transaction log 管理」→ **(2)**
* 「讀取時透過 transaction log 長出 table 欄位」→ **(3) 的 target schema 決定**
* 「新舊 parquet 檔補上數據（新檔有值、舊檔缺欄為 null）」→ **(3) 的 projection / alignment**

![image](../../images/delta-schema-evolution.png)