# Q-076 - Delta Lake Schema Evolution èˆ‡æ­·å²è³‡æ–™é™åˆ¶

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-076`

### ä¾†æº
**ä¾†æº:** Real Exam Recall (å¯¦éš›è€ƒè©¦å›æ†¶ - Question #276)

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹ï¼ˆä¸­æ–‡ï¼‰

ä¸€æ¢è³‡æ–™ç®¡ç·šä½¿ç”¨ Structured Streaming å¾ Apache Kafka æ”å–è³‡æ–™åˆ° Delta Lakeã€‚è³‡æ–™è¢«å„²å­˜åœ¨ bronze è¡¨ä¸­ï¼ŒåŒ…å« Kafka ç”¢ç”Ÿçš„æ™‚é–“æˆ³è¨˜ï¼ˆtimestampï¼‰ã€éµï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰ã€‚åœ¨ç®¡ç·šéƒ¨ç½²ä¸‰å€‹æœˆå¾Œï¼Œè³‡æ–™å·¥ç¨‹åœ˜éšŠæ³¨æ„åˆ°åœ¨æŸäº›æ™‚æ®µå‡ºç¾å»¶é²å•é¡Œã€‚

ä¸€ä½è³‡æ·±è³‡æ–™å·¥ç¨‹å¸«æ›´æ–°äº† Delta è¡¨çš„ schema å’Œæ”å–é‚è¼¯ï¼Œä»¥åŒ…å«ç•¶å‰æ™‚é–“æˆ³è¨˜ï¼ˆç”± Apache Spark è¨˜éŒ„ï¼‰ä»¥åŠ Kafka ä¸»é¡Œï¼ˆtopicï¼‰å’Œåˆ†å€ï¼ˆpartitionï¼‰ã€‚åœ˜éšŠè¨ˆåŠƒä½¿ç”¨é€™äº›é¡å¤–çš„å…ƒè³‡æ–™æ¬„ä½ä¾†è¨ºæ–·æš«æ™‚æ€§çš„è™•ç†å»¶é²ã€‚

åœ˜éšŠåœ¨è¨ºæ–·æ­¤å•é¡Œæ™‚å°‡é¢è‡¨å“ªå€‹é™åˆ¶ï¼Ÿ

### é¡Œå¹¹ï¼ˆè‹±æ–‡ï¼‰

A data pipeline uses Structured Streaming to ingest data from Apache Kafka to Delta Lake. Data is being stored in a bronze table, and includes the Kafka-generated timestamp, key, and value. Three months after the pipeline is deployed, the data engineering team has noticed some latency issues during certain times of the day.

A senior data engineer updates the Delta Table's schema and ingestion logic to include the current timestamp (as recorded by Apache Spark) as well as the Kafka topic and partition. The team plans to use these additional metadata fields to diagnose the transient processing delays.

Which limitation will the team face while diagnosing this problem?

### é¸é …

- **A.** New fields will not be computed for historic records.
- **B.** Spark cannot capture the topic and partition fields from a Kafka source.
- **C.** New fields cannot be added to a production Delta table.
- **D.** Updating the table schema will invalidate the Delta transaction log metadata.
- **E.** Updating the table schema requires a default value provided for each field added.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Delta-Lake`, `Delta-Schema-Evolution`, `Streaming`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Schema-Evolution-Misconception`, `Historical-Data-Limitation`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** `è³‡æ–™è½‰æ›èˆ‡ç®¡ç† (Data Transformation & Management)`, `è³‡æ–™æ”å– (Data Ingestion)`

---

## ç­”æ¡ˆèˆ‡è§£æ

**âœ… æ­£ç¢ºç­”æ¡ˆ:** A

---

# ğŸ“ è©³ç´°è§£æ

> **è€ƒé»:** Delta Lake Schema Evolution çš„æ­·å²è³‡æ–™é™åˆ¶  
> **é›£åº¦:** L2-Intermediate  
> **é‡è¦æ€§:** â­â­â­â­â­ (ç”Ÿç”¢ç’°å¢ƒå¸¸è¦‹å•é¡Œ)

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**Delta Lake Schema Evolution èˆ‡è³‡æ–™å›å¡«é™åˆ¶**
- æŠ€è¡“é ˜åŸŸï¼šDelta Lake Schema ç®¡ç†
- æ ¸å¿ƒæ¦‚å¿µï¼šSchema Evolution åªå½±éŸ¿æ–°è³‡æ–™ï¼Œä¸å›å¡«æ­·å²è³‡æ–™
- å¯¦å‹™å ´æ™¯ï¼šæ–°å¢å…ƒè³‡æ–™æ¬„ä½é€²è¡Œå•é¡Œè¨ºæ–·

### çŸ¥è­˜é ˜åŸŸ
**å°æ‡‰è€ƒç¶±:** [Part 1 - è³‡æ–™é–‹ç™¼èˆ‡è½‰æ›](../../docs/exam-map/part-1-é–‹ç™¼èˆ‡è½‰æ›.md) (32%)  
**ç›¸é—œç« ç¯€:** Delta Lake Schema Evolutionã€Streaming è³‡æ–™å“è³ª

### é—œéµæ¦‚å¿µ
1. **Schema Evolution** - Delta Lake æ”¯æ´å‹•æ…‹æ–°å¢æ¬„ä½
2. **æ­·å²è³‡æ–™ä¸è®Šæ€§** - å·²å¯«å…¥çš„è³‡æ–™ä¸æœƒè¢«è‡ªå‹•æ›´æ–°
3. **è¨ºæ–·çª—å£é™åˆ¶** - æ–°æ¬„ä½åªèƒ½ç”¨æ–¼è¨ºæ–·æœªä¾†è³‡æ–™
4. **Kafka å…ƒè³‡æ–™** - Topicã€Partitionã€Timestamp å¯å¾ Kafka æ“·å–

### æ¬¡è¦è€ƒé»
- Structured Streaming èˆ‡ Kafka æ•´åˆ
- ç”Ÿç”¢ç’°å¢ƒå•é¡Œè¨ºæ–·ç­–ç•¥
- Delta Lake Transaction Log æ©Ÿåˆ¶

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼é¸é … A æ˜¯æ­£ç¢ºçš„ï¼Ÿ

#### æŠ€è¡“åŸç†

**Delta Lake Schema Evolution çš„é‹ä½œæ©Ÿåˆ¶ï¼š**

1. **æ–°å¢æ¬„ä½åªå½±éŸ¿æ–°è³‡æ–™ï¼š**
   - ç•¶ä½¿ç”¨ `.option("mergeSchema", "true")` æˆ– `.option("overwriteSchema", "true")` æ–°å¢æ¬„ä½æ™‚
   - Delta Lake åªæœƒåœ¨**æ–°å¯«å…¥çš„è³‡æ–™**ä¸­å¡«å……é€™äº›æ¬„ä½
   - **æ­·å²è³‡æ–™ä¿æŒä¸è®Š**ï¼Œæ–°æ¬„ä½çš„å€¼ç‚º `null`

2. **è³‡æ–™ä¸å¯è®Šæ€§åŸå‰‡ï¼š**
   - Delta Lake éµå¾ªã€Œå¯«å…¥å¾Œä¸å¯è®Šã€(Write-Once) åŸå‰‡
   - å·²ç¶“å¯«å…¥çš„ Parquet æª”æ¡ˆä¸æœƒè¢«è‡ªå‹•ä¿®æ”¹
   - è¦æ›´æ–°æ­·å²è³‡æ–™éœ€è¦åŸ·è¡Œæ˜ç¢ºçš„ UPDATE æˆ– MERGE æ“ä½œ

3. **è¨ºæ–·ç¯„åœå—é™ï¼š**
   ```
   ç®¡ç·šæ™‚é–“è»¸ï¼š
   â”œâ”€ 0 å€‹æœˆ â†’ éƒ¨ç½²ç®¡ç·šï¼ˆèˆŠ schemaï¼‰
   â”œâ”€ 3 å€‹æœˆ â†’ ç™¼ç¾å»¶é²å•é¡Œ
   â””â”€ 3 å€‹æœˆ+ â†’ æ›´æ–° schema åŠ å…¥æ–°æ¬„ä½
   
   è¨ºæ–·èƒ½åŠ›ï¼š
   âœ… å¯è¨ºæ–·ï¼šæ›´æ–°å¾Œçš„æ–°è³‡æ–™ï¼ˆ3 å€‹æœˆ+ï¼‰
   âŒ ç„¡æ³•è¨ºæ–·ï¼šæ­·å²è³‡æ–™ï¼ˆ0-3 å€‹æœˆï¼‰â† ç¼ºå°‘æ–°æ¬„ä½ï¼
   ```

#### ç¬¦åˆéœ€æ±‚

é¡Œç›®æƒ…å¢ƒçš„é™åˆ¶åˆ†æï¼š

| éœ€æ±‚ | Schema Evolution èƒ½åšåˆ°å— | é™åˆ¶ |
|------|-------------------------|------|
| æ–°å¢ Spark timestamp | âœ… å¯ä»¥ | åªå°æœªä¾†è³‡æ–™æœ‰æ•ˆ |
| æ–°å¢ Kafka topic/partition | âœ… å¯ä»¥ | åªå°æœªä¾†è³‡æ–™æœ‰æ•ˆ |
| è¨ºæ–·**éå» 3 å€‹æœˆ**çš„å»¶é² | âŒ ä¸è¡Œ | **æ­·å²è³‡æ–™æ²’æœ‰æ–°æ¬„ä½** |
| è¨ºæ–·**æœªä¾†**çš„å»¶é² | âœ… å¯ä»¥ | æ–°è³‡æ–™åŒ…å«å®Œæ•´å…ƒè³‡æ–™ |

**æ ¸å¿ƒå•é¡Œï¼š**
åœ˜éšŠæƒ³ç”¨æ–°æ¬„ä½è¨ºæ–·ã€ŒæŸäº›æ™‚æ®µã€çš„å»¶é²å•é¡Œï¼Œä½†é€™äº›å»¶é²ç™¼ç”Ÿåœ¨éå» 3 å€‹æœˆï¼Œè€Œæ­·å²è³‡æ–™ä¸¦ä¸åŒ…å«æ–°å¢çš„æ¬„ä½ã€‚

#### å¯¦å‹™æ‡‰ç”¨

**åŸå§‹ Schemaï¼ˆéƒ¨ç½²æ™‚ï¼‰ï¼š**
```python
from pyspark.sql.functions import col

# åŸå§‹æ”å–é‚è¼¯
kafka_stream = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load()
  .select(
    col("timestamp").alias("kafka_timestamp"),  # Kafka ç”¢ç”Ÿçš„æ™‚é–“
    col("key"),
    col("value")
  ))

# å¯«å…¥ bronze è¡¨
(kafka_stream.writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/checkpoints/bronze")
  .start("/delta/bronze_orders"))
```

**æ›´æ–°å¾Œçš„ Schemaï¼ˆ3 å€‹æœˆå¾Œï¼‰ï¼š**
```python
from pyspark.sql.functions import col, current_timestamp

# æ–°çš„æ”å–é‚è¼¯ï¼šåŠ å…¥ Spark timestamp å’Œ Kafka metadata
kafka_stream_enhanced = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load()
  .select(
    col("timestamp").alias("kafka_timestamp"),
    col("key"),
    col("value"),
    current_timestamp().alias("spark_timestamp"),  # ğŸ†• æ–°å¢
    col("topic"),                                   # ğŸ†• æ–°å¢
    col("partition")                                # ğŸ†• æ–°å¢
  ))

# å¯«å…¥æ™‚å•Ÿç”¨ Schema Evolution
(kafka_stream_enhanced.writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/checkpoints/bronze_v2")
  .option("mergeSchema", "true")  # âš ï¸ å…è¨± schema è®Šæ›´
  .start("/delta/bronze_orders"))
```

**æŸ¥è©¢çµæœï¼ˆæ›´æ–° schema å¾Œï¼‰ï¼š**
```sql
SELECT 
  kafka_timestamp,
  spark_timestamp,    -- ğŸ†• æ–°æ¬„ä½
  topic,              -- ğŸ†• æ–°æ¬„ä½
  partition,          -- ğŸ†• æ–°æ¬„ä½
  key,
  value
FROM bronze_orders
WHERE kafka_timestamp < '2026-01-14'  -- æ­·å²è³‡æ–™
LIMIT 5;

-- çµæœï¼š
-- kafka_timestamp | spark_timestamp | topic | partition | key | value
-- 2025-10-15      | NULL           | NULL  | NULL      | ... | ...
-- 2025-11-20      | NULL           | NULL  | NULL      | ... | ...
-- âŒ æ­·å²è³‡æ–™çš„æ–°æ¬„ä½éƒ½æ˜¯ NULLï¼
```

**è¨ºæ–·èƒ½åŠ›å—é™ï¼š**
```python
# åœ˜éšŠæƒ³åˆ†æå»¶é²æ¨¡å¼
delay_analysis = spark.sql("""
  SELECT 
    DATE(kafka_timestamp) as date,
    HOUR(kafka_timestamp) as hour,
    topic,
    partition,
    (unix_timestamp(spark_timestamp) - unix_timestamp(kafka_timestamp)) as latency_seconds
  FROM bronze_orders
  WHERE kafka_timestamp BETWEEN '2025-10-01' AND '2026-01-14'  -- åŒ…å«æ­·å²è³‡æ–™
  GROUP BY date, hour, topic, partition
  ORDER BY latency_seconds DESC
""")

# âŒ å•é¡Œï¼šæ­·å²è³‡æ–™ï¼ˆ2025-10-01 åˆ° 2026-01-14ï¼‰ç¼ºå°‘æ–°æ¬„ä½
#    ç„¡æ³•è¨ˆç®—é‚£æ®µæ™‚é–“çš„ latencyï¼Œä¹Ÿç„¡æ³•æŒ‰ topic/partition åˆ†çµ„åˆ†æ
```

#### æ•ˆèƒ½èˆ‡æˆæœ¬è€ƒé‡

**å¦‚æœè¦å›å¡«æ­·å²è³‡æ–™ï¼ˆå¯è¡Œä½†ä¸å¯¦éš›ï¼‰ï¼š**

```python
# æ–¹æ¡ˆ 1: UPDATEï¼ˆæ¥µé«˜æˆæœ¬ï¼‰
spark.sql("""
  UPDATE bronze_orders
  SET 
    spark_timestamp = kafka_timestamp,  -- ç”¨ Kafka timestamp ä¼°ç®—
    topic = 'orders',                    -- æ‰‹å‹•å¡«å…¥
    partition = 0                        -- ç„¡æ³•æº–ç¢ºå›å¡«
  WHERE spark_timestamp IS NULL
""")
# âŒ å•é¡Œï¼š
#    - éœ€è¦é‡å¯«å¤§é‡ Parquet æª”æ¡ˆï¼ˆ3 å€‹æœˆè³‡æ–™ï¼‰
#    - ç„¡æ³•æº–ç¢ºå–å¾—ç•¶æ™‚çš„ Spark timestamp
#    - ç„¡æ³•å¾—çŸ¥æ­·å²è³‡æ–™ä¾†è‡ªå“ªå€‹ partition
```

**æ­£ç¢ºçš„è™•ç†æ–¹å¼ï¼š**
```python
# æ¥å—é™åˆ¶ï¼Œåªåˆ†ææœªä¾†è³‡æ–™
# æˆ–è€…å¾ Kafka çš„ consumer metrics ä¸­è£œå……æ­·å²è¨ºæ–·è³‡è¨Š
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … B - Spark cannot capture topic and partition

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **é€™æ˜¯å®Œå…¨éŒ¯èª¤çš„æŠ€è¡“é™³è¿°ï¼**
- âŒ Spark çš„ Kafka connector **å®Œå…¨å¯ä»¥**æ“·å– topic å’Œ partition è³‡è¨Š
- âŒ é€™äº›æ˜¯ Kafka æ¨™æº–çš„ metadata æ¬„ä½

#### æŠ€è¡“äº‹å¯¦

**Kafka DataFrame çš„å®Œæ•´ Schemaï¼š**
```python
kafka_df = (spark.read
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load())

kafka_df.printSchema()
# è¼¸å‡ºï¼š
# root
#  |-- key: binary (nullable = true)
#  |-- value: binary (nullable = true)
#  |-- topic: string (nullable = true)        âœ… å¯ç”¨ï¼
#  |-- partition: integer (nullable = true)   âœ… å¯ç”¨ï¼
#  |-- offset: long (nullable = true)
#  |-- timestamp: timestamp (nullable = true)
#  |-- timestampType: integer (nullable = true)
```

**å¯¦éš›ä½¿ç”¨ç¯„ä¾‹ï¼š**
```python
# å®Œå…¨å¯ä»¥æ“·å– topic å’Œ partition
from pyspark.sql.functions import col

enriched_df = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders,payments,shipments")
  .load()
  .select(
    col("topic"),          # âœ… æ­£å¸¸é‹ä½œ
    col("partition"),      # âœ… æ­£å¸¸é‹ä½œ
    col("offset"),
    col("timestamp"),
    col("key"),
    col("value")
  ))

# æŒ‰ topic åˆ†çµ„çµ±è¨ˆ
enriched_df.groupBy("topic", "partition").count().show()
# +----------+---------+-----+
# |topic     |partition|count|
# +----------+---------+-----+
# |orders    |0        |1234 |
# |orders    |1        |1189 |
# |payments  |0        |2345 |
# +----------+---------+-----+
```

#### é™·é˜±è¨­è¨ˆ

è€ƒå®˜æƒ³æ¸¬è©¦è€ƒç”Ÿæ˜¯å¦çœŸæ­£ä½¿ç”¨é Kafka + Spark æ•´åˆï¼Œé‚„æ˜¯åªæ˜¯æ­»èƒŒæ¦‚å¿µã€‚

**è¨˜æ†¶æ³•ï¼š** Kafka connector æä¾›**å®Œæ•´çš„ metadata æ¬„ä½**ï¼ŒåŒ…æ‹¬ topic, partition, offset, timestampã€‚

---

### é¸é … C - New fields cannot be added to production Delta table

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **Delta Lake çš„æ ¸å¿ƒç‰¹æ€§å°±æ˜¯æ”¯æ´ Schema Evolutionï¼**
- âŒ ç”Ÿç”¢ç’°å¢ƒçš„ Delta è¡¨**å®Œå…¨å¯ä»¥**æ–°å¢æ¬„ä½
- âŒ é€™æ˜¯ Delta Lake ç›¸æ¯”å‚³çµ±è³‡æ–™å€‰å„²çš„é‡è¦å„ªå‹¢

#### Delta Lake Schema Evolution èƒ½åŠ›

**æ”¯æ´çš„ Schema è®Šæ›´ï¼š**

| æ“ä½œ | æ”¯æ´ç¨‹åº¦ | å¦‚ä½•å•Ÿç”¨ |
|------|---------|---------|
| **æ–°å¢æ¬„ä½** | âœ… å®Œå…¨æ”¯æ´ | `.option("mergeSchema", "true")` |
| **åˆªé™¤æ¬„ä½** | âš ï¸ ä¸å»ºè­° | åªèƒ½æ¨™è¨˜ç‚ºå»¢æ£„ |
| **ä¿®æ”¹æ¬„ä½å‹åˆ¥** | âš ï¸ æœ‰é™åˆ¶ | å‘ä¸Šè½‰å‹å¯ä»¥ï¼ˆintâ†’longï¼‰ |
| **é‡æ–°å‘½åæ¬„ä½** | âš ï¸ éœ€ç‰¹æ®Šè™•ç† | ä½¿ç”¨ RENAME COLUMN |

**æ–°å¢æ¬„ä½çš„ä¸‰ç¨®æ–¹å¼ï¼š**

```python
# æ–¹å¼ 1: mergeSchema (æ¨è–¦)
df_with_new_columns.write \
  .format("delta") \
  .mode("append") \
  .option("mergeSchema", "true") \
  .save("/delta/table")

# æ–¹å¼ 2: overwriteSchema (è¬¹æ…ä½¿ç”¨)
df_with_new_columns.write \
  .format("delta") \
  .mode("overwrite") \
  .option("overwriteSchema", "true") \
  .save("/delta/table")

# æ–¹å¼ 3: SQL ALTER TABLE
spark.sql("""
  ALTER TABLE bronze_orders 
  ADD COLUMNS (
    spark_timestamp TIMESTAMP,
    topic STRING,
    partition INT
  )
""")
```

**ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸ï¼š**
```python
# æ¸¬è©¦ç’°å¢ƒé©—è­‰æ–° schema
test_df.write.format("delta").mode("append") \
  .option("mergeSchema", "true") \
  .save("/test/bronze_orders")

# é©—è­‰ schema æ­£ç¢º
spark.read.format("delta").load("/test/bronze_orders").printSchema()

# ç¢ºèªç„¡èª¤å¾Œéƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒï¼ˆå®Œå…¨å¯è¡Œï¼ï¼‰
prod_df.write.format("delta").mode("append") \
  .option("mergeSchema", "true") \
  .save("/prod/bronze_orders")  # âœ… ç”Ÿç”¢ç’°å¢ƒæ­£å¸¸é‹ä½œ
```

#### é™·é˜±è¨­è¨ˆ

æ¸¬è©¦è€ƒç”Ÿæ˜¯å¦äº†è§£ Delta Lake çš„ Schema Evolution ç‰¹æ€§ï¼Œä»¥åŠæ˜¯å¦èª¤ä»¥ç‚ºç”Ÿç”¢ç’°å¢ƒç„¡æ³•è®Šæ›´ schemaã€‚

**è¨˜æ†¶æ³•ï¼š** Delta Lake = **å½ˆæ€§ Schema**ï¼Œç”Ÿç”¢ç’°å¢ƒå¯å®‰å…¨æ–°å¢æ¬„ä½ã€‚

---

### é¸é … D - Updating schema invalidates transaction log

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **Schema æ›´æ–°ä¸æœƒä½¿ Transaction Log å¤±æ•ˆï¼**
- âŒ Delta Lake çš„ Transaction Log è¨­è¨ˆå°±æ˜¯ç‚ºäº†è¿½è¹¤ schema è®Šæ›´
- âŒ Schema è®Šæ›´æœƒè¢«è¨˜éŒ„åœ¨ Transaction Log ä¸­ï¼Œè€Œéç ´å£å®ƒ

#### Delta Lake Transaction Log æ©Ÿåˆ¶

**Transaction Log çš„ä½œç”¨ï¼š**
```
_delta_log/
â”œâ”€â”€ 00000000000000000000.json  â† åˆå§‹ schema
â”œâ”€â”€ 00000000000000000001.json
â”œâ”€â”€ 00000000000000000002.json
â”œâ”€â”€ ...
â”œâ”€â”€ 00000000000000012345.json  â† Schema æ›´æ–°è¨˜éŒ„åœ¨æ­¤
â””â”€â”€ 00000000000000012346.json  â† ç¹¼çºŒæ­£å¸¸é‹ä½œ
```

**Schema æ›´æ–°çš„ Transaction Log æ¢ç›®ï¼š**
```json
// 00000000000000012345.json
{
  "metaData": {
    "schemaString": "{\"type\":\"struct\",\"fields\":[
      {\"name\":\"kafka_timestamp\",\"type\":\"timestamp\"},
      {\"name\":\"key\",\"type\":\"binary\"},
      {\"name\":\"value\",\"type\":\"binary\"},
      {\"name\":\"spark_timestamp\",\"type\":\"timestamp\"},  // ğŸ†• æ–°å¢
      {\"name\":\"topic\",\"type\":\"string\"},               // ğŸ†• æ–°å¢
      {\"name\":\"partition\",\"type\":\"integer\"}           // ğŸ†• æ–°å¢
    ]}",
    "configuration": {},
    "createdTime": 1736899200000
  }
}
```

**è®€å–æ™‚è‡ªå‹•è™•ç† Schema Evolutionï¼š**
```python
# Delta Lake è‡ªå‹•å¾ Transaction Log è®€å–æœ€æ–° schema
df = spark.read.format("delta").load("/delta/bronze_orders")

# âœ… èˆŠæª”æ¡ˆï¼ˆèˆŠ schemaï¼‰ï¼šæ–°æ¬„ä½è‡ªå‹•å¡« NULL
# âœ… æ–°æª”æ¡ˆï¼ˆæ–° schemaï¼‰ï¼šåŒ…å«å®Œæ•´æ¬„ä½
# âœ… Transaction Log å®Œå…¨æœ‰æ•ˆï¼
```

**Transaction Log å¤±æ•ˆçš„çœŸæ­£åŸå› ï¼ˆèˆ‡ schema ç„¡é—œï¼‰ï¼š**
- æ‰‹å‹•åˆªé™¤ `_delta_log/` ç›®éŒ„
- æª”æ¡ˆç³»çµ±æå£
- ä¸¦è¡Œå¯«å…¥è¡çªï¼ˆæ¥µç½•è¦‹ï¼Œæœ‰è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼‰

#### é™·é˜±è¨­è¨ˆ

æ··æ·†ã€Œschema è®Šæ›´ã€èˆ‡ã€Œtransaction log æå£ã€çš„æ¦‚å¿µï¼Œæ¸¬è©¦å° Delta Lake å…§éƒ¨æ©Ÿåˆ¶çš„ç†è§£ã€‚

**è¨˜æ†¶æ³•ï¼š** Transaction Log **è¨˜éŒ„** schema è®Šæ›´ï¼Œä¸æœƒè¢«**ç ´å£**ã€‚

---

### é¸é … E - Schema update requires default values

#### ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ

- âŒ **Delta Lake æ–°å¢æ¬„ä½ä¸éœ€è¦æä¾›é è¨­å€¼ï¼**
- âŒ æ–°æ¬„ä½åœ¨æ­·å²è³‡æ–™ä¸­æœƒè‡ªå‹•å¡«å…… `NULL`ï¼ˆå¦‚æœæ˜¯ nullableï¼‰
- âŒ åªæœ‰åœ¨ SQL `ALTER TABLE` ä¸­æ˜ç¢ºåŠ å…¥ `NOT NULL` ç´„æŸæ™‚æ‰éœ€è¦é è¨­å€¼

#### Delta Lake æ¬„ä½é è¨­å€¼è¦å‰‡

**æ–°å¢ nullable æ¬„ä½ï¼ˆæœ€å¸¸è¦‹ï¼‰ï¼š**
```python
# ä¸éœ€è¦é è¨­å€¼
df_with_new_columns.write \
  .format("delta") \
  .mode("append") \
  .option("mergeSchema", "true") \
  .save("/delta/table")

# æ­·å²è³‡æ–™è‡ªå‹•è™•ç†ï¼š
# èˆŠè¨˜éŒ„çš„æ–°æ¬„ä½ = NULLï¼ˆè‡ªå‹•ï¼‰
# æ–°è¨˜éŒ„çš„æ–°æ¬„ä½ = å¯¦éš›å€¼
```

**SQL æ–°å¢æ¬„ä½ï¼ˆä¹Ÿä¸éœ€è¦é è¨­å€¼ï¼‰ï¼š**
```sql
-- ä¸éœ€è¦ DEFAULT å­å¥
ALTER TABLE bronze_orders 
ADD COLUMNS (
  spark_timestamp TIMESTAMP,
  topic STRING,
  partition INT
);

-- è®€å–æ™‚ï¼š
SELECT * FROM bronze_orders LIMIT 2;
-- èˆŠè¨˜éŒ„: spark_timestamp=NULL, topic=NULL, partition=NULL
-- æ–°è¨˜éŒ„: spark_timestamp='2026-01-14 10:00:00', topic='orders', partition=0
```

**å”¯ä¸€éœ€è¦é è¨­å€¼çš„æƒ…æ³ï¼ˆNOT NULL ç´„æŸï¼‰ï¼š**
```sql
-- âŒ é€™æœƒå¤±æ•—ï¼ˆèˆŠè³‡æ–™ç„¡æ³•æ»¿è¶³ NOT NULLï¼‰
ALTER TABLE bronze_orders 
ADD COLUMNS (
  spark_timestamp TIMESTAMP NOT NULL
);
-- Error: Cannot add NOT NULL column to table with existing data

-- âœ… æ­£ç¢ºåšæ³•ï¼šå…ˆåŠ  nullable æ¬„ä½ï¼Œå›å¡«å¾Œå†åŠ ç´„æŸ
ALTER TABLE bronze_orders 
ADD COLUMNS (spark_timestamp TIMESTAMP);

UPDATE bronze_orders SET spark_timestamp = kafka_timestamp WHERE spark_timestamp IS NULL;

ALTER TABLE bronze_orders 
ALTER COLUMN spark_timestamp SET NOT NULL;
```

#### æ¦‚å¿µå°æ¯”

| è³‡æ–™åº«ç³»çµ± | æ–°å¢æ¬„ä½æ˜¯å¦éœ€è¦é è¨­å€¼ï¼Ÿ |
|-----------|----------------------|
| **Delta Lake** | âŒ ä¸éœ€è¦ï¼ˆnullable è‡ªå‹•å¡« NULLï¼‰ |
| **Traditional SQL** | âš ï¸ ä¾æƒ…æ³ï¼ˆNOT NULL éœ€è¦ï¼‰ |
| **Parquetï¼ˆéœæ…‹ï¼‰** | âŒ ä¸æ”¯æ´ schema è®Šæ›´ |

#### é™·é˜±è¨­è¨ˆ

æ··æ·† Delta Lake èˆ‡å‚³çµ± SQL è³‡æ–™åº«çš„ schema è®Šæ›´è¦å‰‡ï¼Œæ¸¬è©¦å° Delta Lake éˆæ´»æ€§çš„ç†è§£ã€‚

**è¨˜æ†¶æ³•ï¼š** Delta Lake æ–°å¢æ¬„ä½ = **è‡ªå‹• NULL**ï¼Œä¸éœ€è¦é è¨­å€¼ã€‚

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### è¨˜æ†¶å£è¨£

**ã€ŒSchema å¾€å‰çœ‹ï¼Œæ­·å²çœ‹ä¸è¦‹ã€**
- **Schema å¾€å‰çœ‹** = Schema Evolution åªå½±éŸ¿æœªä¾†è³‡æ–™
- **æ­·å²çœ‹ä¸è¦‹** = æ­·å²è³‡æ–™ä¸æœƒè¢«è‡ªå‹•å›å¡«æ–°æ¬„ä½

### å°æ¯”è¡¨ï¼ˆé‡è¦ï¼ï¼‰

| é …ç›® | Schema Evolution åšå¾—åˆ°ï¼Ÿ | èªªæ˜ |
|------|-------------------------|------|
| **æ–°å¢æ¬„ä½åˆ°æœªä¾†è³‡æ–™** | âœ… æ˜¯ | æ–°å¯«å…¥çš„è³‡æ–™åŒ…å«æ–°æ¬„ä½ |
| **è‡ªå‹•å›å¡«æ­·å²è³‡æ–™** | âŒ å¦ | æ­·å²è³‡æ–™æ–°æ¬„ä½ = NULL |
| **æ‰‹å‹•å›å¡«ï¼ˆUPDATEï¼‰** | âœ… å¯ä»¥ï¼ˆä½†æˆæœ¬é«˜ï¼‰ | éœ€é‡å¯« Parquet æª”æ¡ˆ |
| **å¾ Kafka æ“·å– metadata** | âœ… æ˜¯ | Topic, Partition éƒ½å¯æ“·å– |
| **åœ¨ç”Ÿç”¢ç’°å¢ƒè®Šæ›´ schema** | âœ… æ˜¯ | Delta Lake å®Œå…¨æ”¯æ´ |
| **ç ´å£ Transaction Log** | âŒ å¦ | Schema è®Šæ›´è¢«è¨˜éŒ„ï¼Œä¸ç ´å£ |

### æ±ºç­–æ¨¹

```
éœ€è¦æ–°å¢æ¬„ä½åˆ° Delta è¡¨ï¼Ÿ
â”œâ”€ åªéœ€è¦æœªä¾†è³‡æ–™æœ‰æ–°æ¬„ä½ï¼Ÿ
â”‚  â””â”€ âœ… ä½¿ç”¨ mergeSchemaï¼Œç›´æ¥æ–°å¢
â””â”€ éœ€è¦æ­·å²è³‡æ–™ä¹Ÿæœ‰æ–°æ¬„ä½ï¼Ÿ
   â”œâ”€ å¯ä»¥æ¥å— NULL å€¼ï¼Ÿ
   â”‚  â””â”€ âš ï¸ ç›´æ¥æ–°å¢ï¼ˆæ­·å² = NULLï¼‰
   â””â”€ å¿…é ˆæœ‰å¯¦éš›å€¼ï¼Ÿ
      â””â”€ âŒ éœ€è¦åŸ·è¡Œ UPDATEï¼ˆé«˜æˆæœ¬ï¼‰
         æˆ–å¾å…¶ä»–ä¾†æºè£œè³‡æ–™
```

### å¯¦å‹™è¨˜æ†¶

**Streaming å•é¡Œè¨ºæ–·çš„æ­£ç¢ºç­–ç•¥ï¼š**

```python
# âŒ éŒ¯èª¤ï¼šä¾è³´ schema evolution è¨ºæ–·æ­·å²å•é¡Œ
# æ–°å¢æ¬„ä½å¾Œï¼Œæ­·å²è³‡æ–™ä»ç¼ºå°‘è¨ºæ–·è³‡è¨Š

# âœ… æ­£ç¢ºï¼šå¤šç®¡é½Šä¸‹
# 1. ç«‹å³æ–°å¢æ¬„ä½ï¼ˆç‚ºæœªä¾†è¨ºæ–·åšæº–å‚™ï¼‰
streaming_df_enhanced.writeStream \
  .option("mergeSchema", "true") \
  .start("/delta/bronze")

# 2. åŒæ™‚å¾å…¶ä»–ä¾†æºå–å¾—æ­·å²è¨ºæ–·è³‡è¨Š
# - Kafka consumer metrics
# - Databricks job logs
# - Spark event logs
# - ç³»çµ±ç›£æ§æŒ‡æ¨™

# 3. å»ºç«‹å®Œæ•´çš„è¨ºæ–·è¦–åœ–
# æ­·å²è¨ºæ–· + æœªä¾†è¨ºæ–·
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶å¼•ç”¨

### æ ¸å¿ƒæ–‡ä»¶

1. **[Delta Lake Schema Evolution](https://docs.delta.io/latest/delta-batch.html#schema-evolution)**
   - mergeSchema é¸é …èªªæ˜
   - overwriteSchema é¸é …èªªæ˜
   - Schema è®Šæ›´çš„é™åˆ¶

2. **[Structured Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)**
   - Kafka DataFrame Schema
   - å¯ç”¨çš„ metadata æ¬„ä½ï¼ˆtopic, partition, offsetï¼‰

3. **[Delta Lake Transaction Log](https://databricks.com/blog/2019/08/21/diving-into-delta-lake-unpacking-the-transaction-log.html)**
   - Transaction Log çµæ§‹
   - Schema è®Šæ›´å¦‚ä½•è¢«è¨˜éŒ„

### å»¶ä¼¸é–±è®€

- [Schema Evolution Best Practices](https://docs.databricks.com/delta/delta-batch.html#-automatic-schema-evolution) - ç”Ÿç”¢ç’°å¢ƒ schema è®Šæ›´ç­–ç•¥
- [Backfilling Data in Delta Lake](https://docs.delta.io/latest/delta-update.html) - å¦‚ä½•å®‰å…¨åœ°å›å¡«æ­·å²è³‡æ–™

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

çœ‹åˆ°ã€Œæ–°å¢æ¬„ä½ã€+ã€Œè¨ºæ–·æ­·å²å•é¡Œã€é—œéµå­—æ™‚ï¼š

1. âœ… **æ ¸å¿ƒé™åˆ¶ï¼šæ­·å²è³‡æ–™ä¸æœƒè¢«è‡ªå‹•æ›´æ–°** - é€™æ˜¯æœ€å¸¸è¦‹çš„é™·é˜±
2. âŒ **æ’é™¤æŠ€è¡“ä¸å¯è¡Œçš„é¸é …** - Spark å¯ä»¥è®€ Kafka metadata, Delta æ”¯æ´ schema evolution
3. âŒ **æ’é™¤èª‡å¤§çš„é¸é …** - Transaction log ä¸æœƒå¤±æ•ˆï¼Œä¸éœ€è¦é è¨­å€¼

### é™·é˜±è­˜åˆ¥

æ­¤é¡ŒåŒ…å«çš„é™·é˜±é¡å‹ï¼š

| é™·é˜±é¡å‹ | é«”ç¾é¸é … | è­˜åˆ¥æŠ€å·§ |
|---------|---------|---------|
| **æ­·å²è³‡æ–™é™åˆ¶** | Aï¼ˆæ­£ç¢ºç­”æ¡ˆï¼‰ | Schema evolution ä¸æº¯åŠæ—¢å¾€ |
| **æŠ€è¡“èƒ½åŠ›èª¤è§£** | B | Spark+Kafka å®Œå…¨æ”¯æ´ metadata |
| **Schema Evolution èª¤è§£** | C | Delta Lake æ ¸å¿ƒç‰¹æ€§ |
| **Transaction Log èª¤è§£** | D | Schema è®Šæ›´è¢«è¨˜éŒ„ï¼Œä¸ç ´å£ |
| **é è¨­å€¼èª¤è§£** | E | Nullable æ¬„ä½ä¸éœ€è¦é è¨­å€¼ |

### é—œéµå­—æç¤º

é¡Œç›®ä¸­çš„ç·šç´¢ï¼š
- âœ… **ã€Œthree months afterã€** â†’ æœ‰æ­·å²è³‡æ–™
- âœ… **ã€Œupdates the schemaã€** â†’ Schema Evolution
- âœ… **ã€Œdiagnoseã€** â†’ éœ€è¦æ­·å²è³‡è¨Š
- âœ… **ã€Œlimitationã€** â†’ æ‰¾å‡º schema evolution çš„é™åˆ¶

---

## ğŸ”— ç›¸é—œé¡Œç›®

å»ºè­°æŒ‰æ­¤é †åºç·´ç¿’ï¼š

1. **[Q-007](./Q-007.md)** - Delta Lake åŸºç¤æ“ä½œï¼ˆå»ºè­°å…ˆåšï¼‰
2. **[Q-012](./Q-012.md)** - Structured Streaming åŸºç¤ï¼ˆStreaming æ¦‚å¿µï¼‰
3. **[Q-075](./Q-075.md)** - Delta Lake MERGE å»é‡ï¼ˆç›¸é—œä¸»é¡Œï¼‰
4. **[Q-023](./Q-023.md)** - Delta Lake VACUUMï¼ˆschema ç›¸é—œï¼‰

---

## ğŸ’¡ å¯¦å‹™è£œå……

### ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

```python
# å®Œæ•´çš„ Streaming + Schema Evolution æ¨¡å¼
from pyspark.sql.functions import col, current_timestamp, from_json

# 1. å®šç¾©å¯æ¼”åŒ–çš„ schema
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType

# åˆå§‹ schema
base_schema = StructType([
    StructField("kafka_timestamp", TimestampType()),
    StructField("key", StringType()),
    StructField("value", StringType())
])

# å¢å¼· schemaï¼ˆåŠ å…¥è¨ºæ–·æ¬„ä½ï¼‰
enhanced_schema = base_schema.add(
    StructField("spark_timestamp", TimestampType())
).add(
    StructField("topic", StringType())
).add(
    StructField("partition", IntegerType())
)

# 2. å•Ÿç”¨ Schema Evolution çš„ Streaming
(spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "broker:9092")
  .option("subscribe", "orders")
  .load()
  .select(
    col("timestamp").alias("kafka_timestamp"),
    col("key").cast("string"),
    col("value").cast("string"),
    current_timestamp().alias("spark_timestamp"),
    col("topic"),
    col("partition")
  )
  .writeStream
  .format("delta")
  .outputMode("append")
  .option("checkpointLocation", "/checkpoints/bronze")
  .option("mergeSchema", "true")  # âš ï¸ é—œéµè¨­å®š
  .start("/delta/bronze_orders"))

# 3. å»ºç«‹è¨ºæ–·æŸ¥è©¢ï¼ˆåƒ…é©ç”¨æ–¼æ–°è³‡æ–™ï¼‰
spark.sql("""
  CREATE OR REPLACE VIEW latency_analysis AS
  SELECT 
    DATE(kafka_timestamp) as date,
    HOUR(kafka_timestamp) as hour,
    topic,
    partition,
    AVG(unix_timestamp(spark_timestamp) - unix_timestamp(kafka_timestamp)) as avg_latency_sec,
    COUNT(*) as record_count
  FROM bronze_orders
  WHERE spark_timestamp IS NOT NULL  -- âš ï¸ æ’é™¤æ­·å²è³‡æ–™
  GROUP BY date, hour, topic, partition
  HAVING avg_latency_sec > 5  -- æ‰¾å‡ºå»¶é² > 5 ç§’çš„æƒ…æ³
  ORDER BY avg_latency_sec DESC
""")
```

### ç›£æ§èˆ‡è­¦å ±

```python
# è¨­å®šè³‡æ–™å“è³ªç›£æ§
from pyspark.sql.functions import sum as _sum, when

quality_check = spark.sql("""
  SELECT 
    COUNT(*) as total_records,
    SUM(CASE WHEN spark_timestamp IS NULL THEN 1 ELSE 0 END) as records_without_metadata,
    MIN(kafka_timestamp) as oldest_record,
    MAX(kafka_timestamp) as newest_record
  FROM bronze_orders
""")

quality_check.show()
# +-------------+-------------------------+-------------------+-------------------+
# |total_records|records_without_metadata |oldest_record      |newest_record      |
# +-------------+-------------------------+-------------------+-------------------+
# |10,500,000   |9,000,000               |2025-10-15 08:00:00|2026-01-14 10:00:00|
# +-------------+-------------------------+-------------------+-------------------+
# âš ï¸ å¯ä»¥æ¸…æ¥šçœ‹åˆ°ï¼š900 è¬ç­†æ­·å²è³‡æ–™ç¼ºå°‘æ–°æ¬„ä½
```

---

**æœ€å¾Œæ›´æ–°:** 2026-01-14  
**è§£æç‰ˆæœ¬:** v1.0  
**å¯©æ ¸ç‹€æ…‹:** âœ… å·²é©—è­‰
