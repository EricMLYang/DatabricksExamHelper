# Question #25

---

## 題目資訊

### 題目編號
**ID:** `Q-01-025`

### 來源
**來源:** Real Exam Recall

### 難度等級
**難度:** `L3-Advanced`

---

## 題目內容

### 題幹

A Spark job is taking longer than expected. Using the Spark UI, a data engineer notes that the Min, Median, and Max Durations for tasks in a particular stage show the minimum and median time to complete a task as roughly the same, but the max duration for a task to be roughly 100 times as long as the minimum.

Which situation is causing increased duration of the overall job?

### 選項

- **A.** Task queueing resulting from improper thread pool assignment.
- **B.** Spill resulting from attached volume storage being too small.
- **C.** Network latency due to some cluster nodes being in different regions from the source data
- **D.** Skew caused by more data being assigned to a subset of spark-partitions.
- **E.** Credential validation errors while pulling data from an external system.

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Data-Skew`, `Spark-UI`, `Performance-Tuning`, `Troubleshooting`

### Trap Tags (陷阱類型標籤)
**Traps:** `Performance-Diagnosis`, `Symptom-Interpretation`

### Knowledge Domain (知識領域)
**Domain:** `Spark Performance`

---

## 答案與解析連結

### 正確答案
**正解:** `D`

### 解析檔案
**詳細解析:** [點此查看解析](../analysis/Q-01-025-analysis.md)

---

## 相關資源

### 官方文件
- [Spark UI](https://docs.databricks.com/clusters/spark-ui-guide.html)
- [Data Skew Handling](https://docs.databricks.com/optimizations/skew-join.html)
