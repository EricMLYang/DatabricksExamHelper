# Question #108

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-108`

### ä¾†æº
**ä¾†æº:** Mock Exam / Community Contributed

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

Which statement describes the **default execution mode** for Databricks Auto Loader?

### é¸é …

- **A.** Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; the target table is materialized by directly querying all valid files in the source directory.
- **B.** New files are identified by listing the input directory; the target table is materialized by directly querying all valid files in the source directory.
- **C.** Webhooks trigger a Databricks job to run anytime new data arrives in a source directory; new data are automatically merged into target tables using rules inferred from the data.
- **D.** New files are identified by listing the input directory; new files are incrementally and idempotently loaded into the target Delta Lake table.
- **E.** Cloud vendor-specific queue storage and notification services are configured to track newly arriving files; new files are incrementally and idempotently loaded into the target Delta Lake table.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Auto-Loader`, `Streaming`, `Data-Ingestion`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Default-vs-Optional`, `Mode-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Data Ingestion

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** D
- **ç¤¾ç¾¤å…±è­˜:** D

---

# é¡Œç›®è§£æ

---

## é¡Œç›®å›é¡§

### é¡Œç›®ç·¨è™Ÿèˆ‡é€£çµ
**é¡Œç›® ID:** `Q-108`
**é¡Œç›®é€£çµ:** [é»æ­¤è¿”å›é¡Œç›®](#question-108)

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Databricks Auto Loader - Default Execution Mode
**çŸ¥è­˜é ˜åŸŸ:** Data Ingestion / Streaming
**é—œéµæ¦‚å¿µ:**
- Auto Loader çš„å…©ç¨®æ¨¡å¼ï¼šDirectory Listing vs File Notification
- **é è¨­æ¨¡å¼**ï¼šDirectory Listing
- Incremental and Idempotent è³‡æ–™è¼‰å…¥
- cloudFiles æ ¼å¼

### æ¬¡è¦è€ƒé»
- File Notification æ¨¡å¼çš„è§¸ç™¼æ¢ä»¶
- Auto Loader èˆ‡ Structured Streaming çš„æ•´åˆ
- å¢é‡è™•ç†çš„ checkpoint æ©Ÿåˆ¶

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†:**

**1. Auto Loader çš„é è¨­æ¨¡å¼ï¼šDirectory Listing**

Auto Loader é è¨­ä½¿ç”¨ **Directory Listing æ¨¡å¼**ä¾†è­˜åˆ¥æ–°æª”æ¡ˆï¼š

```python
# Auto Loader çš„åŸºæœ¬ç”¨æ³•ï¼ˆé è¨­ Directory Listing æ¨¡å¼ï¼‰
df = (spark.readStream
    .format("cloudFiles")  # â† Auto Loader çš„æ ¼å¼
    .option("cloudFiles.format", "json")  # ä¾†æºæª”æ¡ˆæ ¼å¼
    .load("/mnt/source/data/")  # ä¾†æºç›®éŒ„
)

# å¯«å…¥ Delta Lake
(df.writeStream
    .format("delta")
    .option("checkpointLocation", "/mnt/checkpoints/autoloader")
    .start("/mnt/delta/target_table")
)

# âš ï¸ é è¨­è¡Œç‚ºï¼š
# 1. åˆ—å‡ºï¼ˆlistï¼‰ä¾†æºç›®éŒ„çš„æª”æ¡ˆ
# 2. è­˜åˆ¥æ–°å¢çš„æª”æ¡ˆï¼ˆæ¯”å° checkpointï¼‰
# 3. å¢é‡è¼‰å…¥æ–°æª”æ¡ˆåˆ° Delta Lake
# 4. Idempotentï¼šç›¸åŒæª”æ¡ˆä¸æœƒé‡è¤‡è™•ç†
```

**âœ… ç‚ºä»€éº¼é¸é … D å®Œå…¨æ­£ç¢ºï¼Ÿ**

é¸é … D çš„å…©å€‹é—œéµéƒ¨åˆ†ï¼š

| éƒ¨åˆ† | æè¿° | æ­£ç¢ºæ€§ |
|------|------|--------|
| **"New files are identified by listing the input directory"** | é€éåˆ—å‡ºç›®éŒ„ä¾†è­˜åˆ¥æ–°æª”æ¡ˆ | âœ… Directory Listing æ¨¡å¼ï¼ˆé è¨­ï¼‰ |
| **"incrementally and idempotently loaded into the target Delta Lake table"** | å¢é‡ä¸”å†ªç­‰åœ°è¼‰å…¥åˆ° Delta Lake | âœ… Auto Loader çš„æ ¸å¿ƒç‰¹æ€§ |

**2. Directory Listing æ¨¡å¼çš„å·¥ä½œåŸç†ï¼š**

```
æ™‚é–“è»¸è¦–åœ–ï¼š

åˆå§‹ç‹€æ…‹ï¼š/mnt/source/data/
  â”œâ”€â”€ file1.json
  â””â”€â”€ file2.json

[Auto Loader é¦–æ¬¡åŸ·è¡Œ]
  â†“
Step 1: åˆ—å‡ºç›®éŒ„ï¼ˆDirectory Listingï¼‰
  â†’ ç™¼ç¾: file1.json, file2.json
  
Step 2: æª¢æŸ¥ Checkpoint
  â†’ Checkpoint ç„¡è¨˜éŒ„ï¼ˆé¦–æ¬¡åŸ·è¡Œï¼‰
  
Step 3: è¼‰å…¥æ–°æª”æ¡ˆ
  â†’ è¼‰å…¥ file1.json, file2.json
  
Step 4: æ›´æ–° Checkpoint
  â†’ è¨˜éŒ„: processed = [file1.json, file2.json]

---

æ–°æª”æ¡ˆåˆ°é”ï¼š
  â”œâ”€â”€ file1.json (å·²å­˜åœ¨)
  â”œâ”€â”€ file2.json (å·²å­˜åœ¨)
  â””â”€â”€ file3.json (æ–°å¢)

[Auto Loader ç¬¬äºŒæ¬¡åŸ·è¡Œ]
  â†“
Step 1: åˆ—å‡ºç›®éŒ„ï¼ˆDirectory Listingï¼‰
  â†’ ç™¼ç¾: file1.json, file2.json, file3.json
  
Step 2: æª¢æŸ¥ Checkpoint
  â†’ Checkpoint è¨˜éŒ„: [file1.json, file2.json]
  
Step 3: è­˜åˆ¥æ–°æª”æ¡ˆ
  â†’ æ–°æª”æ¡ˆ: file3.json (å·®ç•°)
  
Step 4: è¼‰å…¥æ–°æª”æ¡ˆï¼ˆIncrementalï¼‰
  â†’ åªè¼‰å…¥ file3.json â† å¢é‡è™•ç†ï¼
  
Step 5: æ›´æ–° Checkpoint
  â†’ è¨˜éŒ„: processed = [file1.json, file2.json, file3.json]
```

**3. å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼ˆé è¨­æ¨¡å¼ï¼‰ï¼š**

```python
# å®Œæ•´çš„ Auto Loader å¯¦ä½œï¼ˆDirectory Listing é è¨­æ¨¡å¼ï¼‰

from pyspark.sql.functions import *
from pyspark.sql.types import *

# === é…ç½® Auto Loaderï¼ˆä½¿ç”¨é è¨­ Directory Listing æ¨¡å¼ï¼‰ ===
source_path = "/mnt/raw/events/"
checkpoint_path = "/mnt/checkpoints/events_ingestion"
target_table = "bronze.events"

# å®šç¾© schemaï¼ˆå¯é¸ï¼ŒAuto Loader å¯è‡ªå‹•æ¨æ–·ï¼‰
schema = StructType([
    StructField("event_id", StringType(), True),
    StructField("user_id", StringType(), True),
    StructField("event_type", StringType(), True),
    StructField("timestamp", TimestampType(), True),
    StructField("properties", StringType(), True)
])

# è®€å–ä¸²æµï¼ˆAuto Loaderï¼‰
events_stream = (spark.readStream
    .format("cloudFiles")  # â† Auto Loader
    
    # ä¾†æºæª”æ¡ˆæ ¼å¼
    .option("cloudFiles.format", "json")
    
    # Schema ç›¸é—œè¨­å®š
    .option("cloudFiles.schemaLocation", f"{checkpoint_path}/schema")
    .option("cloudFiles.inferColumnTypes", "true")
    
    # âš ï¸ é è¨­ä½¿ç”¨ Directory Listingï¼ˆä¸éœ€æ˜ç¢ºæŒ‡å®šï¼‰
    # å¦‚æœè¦æ˜ç¢ºæŒ‡å®šï¼Œå¯ä»¥åŠ ä¸Šï¼š
    # .option("cloudFiles.useNotifications", "false")  # false = Directory Listing
    
    .schema(schema)  # å¯é¸
    .load(source_path)
)

# è³‡æ–™è½‰æ›ï¼ˆå¯é¸ï¼‰
transformed_df = events_stream.withColumn(
    "ingestion_timestamp", current_timestamp()
).withColumn(
    "source_file", input_file_name()
)

# å¯«å…¥ Delta Lakeï¼ˆå¢é‡ä¸”å†ªç­‰ï¼‰
query = (transformed_df.writeStream
    .format("delta")
    .outputMode("append")
    .option("checkpointLocation", checkpoint_path)
    
    # è‡ªå‹•è™•ç†é‡è¤‡æª”æ¡ˆï¼ˆIdempotentï¼‰
    .option("mergeSchema", "true")  # å…è¨± schema æ¼”é€²
    
    .trigger(processingTime="10 seconds")  # æ¯ 10 ç§’æª¢æŸ¥ä¸€æ¬¡
    .table(target_table)
)

query.awaitTermination()

print(f"""
Auto Loader é…ç½®ï¼ˆé è¨­æ¨¡å¼ï¼‰ï¼š
- æ¨¡å¼: Directory Listingï¼ˆé è¨­ï¼‰
- ä¾†æº: {source_path}
- ç›®æ¨™: {target_table}
- ç‰¹æ€§: Incremental + Idempotent
""")
```

**4. Incremental å’Œ Idempotent çš„ä¿è­‰ï¼š**

```python
# Auto Loader çš„ Incremental å’Œ Idempotent ç‰¹æ€§

# === Incrementalï¼ˆå¢é‡ï¼‰ ===
# åªè™•ç†æ–°å¢çš„æª”æ¡ˆï¼Œä¸é‡è¤‡è™•ç†å·²è™•ç†çš„æª”æ¡ˆ

# Day 1: è™•ç† 100 å€‹æª”æ¡ˆ
# Day 2: åªè™•ç†æ–°å¢çš„ 10 å€‹æª”æ¡ˆï¼ˆä¸æ˜¯ 110 å€‹ï¼‰
# Day 3: åªè™•ç†æ–°å¢çš„ 15 å€‹æª”æ¡ˆï¼ˆä¸æ˜¯ 125 å€‹ï¼‰

# === Idempotentï¼ˆå†ªç­‰ï¼‰ ===
# ç›¸åŒçš„æª”æ¡ˆå¤šæ¬¡åŸ·è¡Œçµæœç›¸åŒï¼Œä¸æœƒé‡è¤‡è¼‰å…¥

# å‡è¨­ file1.json è¢«è™•ç†
# å¦‚æœä½œæ¥­å¤±æ•—é‡è©¦ï¼Œfile1.json ä¸æœƒè¢«é‡è¤‡è™•ç†
# Checkpoint ç¢ºä¿ exactly-once èªç¾©

# æ¸¬è©¦ Idempotent ç‰¹æ€§
def test_idempotency():
    """
    æ¸¬è©¦ Auto Loader çš„å†ªç­‰æ€§
    """
    
    # ç¬¬ä¸€æ¬¡åŸ·è¡Œ
    query1 = spark.readStream \
        .format("cloudFiles") \
        .option("cloudFiles.format", "json") \
        .load("/mnt/source/test/") \
        .writeStream \
        .format("delta") \
        .option("checkpointLocation", "/mnt/checkpoints/test") \
        .trigger(once=True) \
        .table("test_table")
    
    query1.awaitTermination()
    count1 = spark.table("test_table").count()
    
    # ç¬¬äºŒæ¬¡åŸ·è¡Œï¼ˆç›¸åŒä¾†æºï¼Œç„¡æ–°æª”æ¡ˆï¼‰
    query2 = spark.readStream \
        .format("cloudFiles") \
        .option("cloudFiles.format", "json") \
        .load("/mnt/source/test/") \
        .writeStream \
        .format("delta") \
        .option("checkpointLocation", "/mnt/checkpoints/test") \
        .trigger(once=True) \
        .table("test_table")
    
    query2.awaitTermination()
    count2 = spark.table("test_table").count()
    
    # é©—è­‰å†ªç­‰æ€§
    assert count1 == count2, "Idempotency violated!"
    print(f"âœ… Idempotent confirmed: {count1} = {count2}")

test_idempotency()
```

**ç¬¦åˆéœ€æ±‚:**

é¡Œç›®å•ï¼šã€ŒWhich statement describes the **default execution mode**ï¼Ÿã€

é¸é … D å®Œå…¨ç¬¦åˆé è¨­è¡Œç‚ºï¼š
- âœ… **é è¨­æ¨¡å¼** - Directory Listingï¼ˆä¸æ˜¯ File Notificationï¼‰
- âœ… **è­˜åˆ¥æ–°æª”æ¡ˆ** - é€éåˆ—å‡ºç›®éŒ„
- âœ… **å¢é‡è¼‰å…¥** - åªè™•ç†æ–°æª”æ¡ˆ
- âœ… **å†ªç­‰æ€§** - ä¸é‡è¤‡è™•ç†ç›¸åŒæª”æ¡ˆ
- âœ… **Delta Lake** - å¯«å…¥ Delta Lake è¡¨

---

## ğŸ”„ Directory Listing vs File Notification

### å…©ç¨®æ¨¡å¼çš„å°æ¯”

**Auto Loader æ”¯æ´å…©ç¨®æ¨¡å¼ï¼Œä½†é è¨­æ˜¯ Directory Listingï¼š**

| ç‰¹æ€§ | Directory Listingï¼ˆé è¨­ï¼‰| File Notificationï¼ˆé€²éšï¼‰ |
|------|------------------------|--------------------------|
| **å•Ÿç”¨æ–¹å¼** | é è¨­ï¼ˆä¸éœ€è¨­å®šï¼‰ | `cloudFiles.useNotifications = true` |
| **æª”æ¡ˆç™¼ç¾** | åˆ—å‡ºç›®éŒ„ï¼ˆlist directoryï¼‰ | é›²ç«¯é€šçŸ¥æœå‹™ï¼ˆEvent Grid/SNS/Pub/Subï¼‰ |
| **æ“´å±•æ€§** | é©åˆæ•¸åƒå€‹æª”æ¡ˆ | é©åˆæ•¸ç™¾è¬å€‹æª”æ¡ˆ |
| **å»¶é²** | è¼ƒé«˜ï¼ˆéœ€å®šæœŸåˆ—å‡ºç›®éŒ„ï¼‰ | è¼ƒä½ï¼ˆå³æ™‚é€šçŸ¥ï¼‰ |
| **è¨­å®šè¤‡é›œåº¦** | ç°¡å–®ï¼ˆç„¡éœ€é¡å¤–è¨­å®šï¼‰ | è¤‡é›œï¼ˆéœ€è¨­å®šé›²ç«¯é€šçŸ¥ï¼‰ |
| **æˆæœ¬** | è¼ƒä½ï¼ˆåªæœ‰åˆ—ç›®éŒ„æˆæœ¬ï¼‰ | è¼ƒé«˜ï¼ˆé€šçŸ¥æœå‹™æˆæœ¬ï¼‰ |
| **é©ç”¨å ´æ™¯** | å°åˆ°ä¸­å‹è¦æ¨¡ | å¤§è¦æ¨¡ã€é«˜é »ç‡ |

**File Notification æ¨¡å¼ç¯„ä¾‹ï¼ˆéé è¨­ï¼‰ï¼š**

```python
# File Notification æ¨¡å¼ï¼ˆéœ€æ˜ç¢ºå•Ÿç”¨ï¼‰
df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    
    # âš ï¸ æ˜ç¢ºå•Ÿç”¨ File Notification æ¨¡å¼
    .option("cloudFiles.useNotifications", "true")
    
    # éœ€è¦è¨­å®šé›²ç«¯é€šçŸ¥æœå‹™
    # Azure: Event Grid
    # AWS: SNS/SQS
    # GCP: Pub/Sub
    .option("cloudFiles.resourceGroup", "my-resource-group")  # Azure
    .option("cloudFiles.subscriptionId", "my-subscription-id")  # Azure
    
    .load("/mnt/source/data/")
)

# é€™æœƒè‡ªå‹•è¨­å®šé›²ç«¯é€šçŸ¥æœå‹™
# âœ… é¸é … E æè¿°çš„å°±æ˜¯é€™å€‹æ¨¡å¼ï¼ˆä½†ä¸æ˜¯é è¨­ï¼ï¼‰
```

**æ¨¡å¼é¸æ“‡æ±ºç­–æ¨¹ï¼š**

```
éœ€è¦æ”å–æª”æ¡ˆï¼Ÿ
  â”œâ”€ æª”æ¡ˆæ•¸é‡ < 10,000ï¼Ÿ
  â”‚     â””â”€ æ˜¯ â†’ Directory Listing âœ… (é è¨­ï¼Œé¸é … D)
  â”‚
  â””â”€ æª”æ¡ˆæ•¸é‡ > 10,000 æˆ–é«˜é »ç‡åˆ°é”ï¼Ÿ
        â””â”€ æ˜¯ â†’ File Notification (é¸é … Eï¼Œéœ€æ˜ç¢ºå•Ÿç”¨)
        
å»¶é²è¦æ±‚ï¼š
  â”œâ”€ å¯æ¥å—åˆ†é˜ç´šå»¶é²ï¼Ÿ
  â”‚     â””â”€ æ˜¯ â†’ Directory Listing âœ…
  â”‚
  â””â”€ éœ€è¦ç§’ç´šå»¶é²ï¼Ÿ
        â””â”€ æ˜¯ â†’ File Notification

è¨­å®šè¤‡é›œåº¦ï¼š
  â”œâ”€ å¸Œæœ›ç°¡å–®å¿«é€Ÿè¨­å®šï¼Ÿ
  â”‚     â””â”€ æ˜¯ â†’ Directory Listing âœ…
  â”‚
  â””â”€ å¯æ¥å—è¤‡é›œè¨­å®šï¼ˆé›²ç«¯æ¬Šé™ã€é€šçŸ¥æœå‹™ï¼‰ï¼Ÿ
        â””â”€ æ˜¯ â†’ File Notification
```

---

## âŒ å…¶ä»–éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A - "File Notification + directly querying all files"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

é¸é … A çš„å…©å€‹éƒ¨åˆ†ï¼š
- **ç¬¬ä¸€éƒ¨åˆ†ï¼š** "Cloud vendor-specific queue storage and notification services"
  - âŒ é€™æè¿°çš„æ˜¯ **File Notification æ¨¡å¼**ï¼Œä¸æ˜¯é è¨­æ¨¡å¼
  
- **ç¬¬äºŒéƒ¨åˆ†ï¼š** "target table is materialized by directly querying all valid files"
  - âŒ é€™æè¿°çš„æ˜¯**å…¨é‡æŸ¥è©¢**ï¼Œä¸æ˜¯å¢é‡è¼‰å…¥

```python
# é¸é … A æè¿°çš„è¡Œç‚ºï¼ˆéŒ¯èª¤ï¼‰

# ç¬¬ä¸€éƒ¨åˆ†éŒ¯èª¤ï¼šä½¿ç”¨ File Notificationï¼ˆéé è¨­ï¼‰
.option("cloudFiles.useNotifications", "true")  # âŒ éé è¨­

# ç¬¬äºŒéƒ¨åˆ†éŒ¯èª¤ï¼šç›´æ¥æŸ¥è©¢æ‰€æœ‰æª”æ¡ˆï¼ˆéå¢é‡ï¼‰
# é€™æ›´åƒå‚³çµ±çš„æ‰¹æ¬¡è®€å–
df = spark.read.format("json").load("/mnt/source/data/")  # âŒ ä¸æ˜¯ Auto Loader
```

**éŒ¯èª¤é»ï¼š**

| éŒ¯èª¤ | èªªæ˜ |
|------|------|
| **File Notification** | ä¸æ˜¯é è¨­æ¨¡å¼ï¼ˆéœ€æ˜ç¢ºå•Ÿç”¨ï¼‰ |
| **Directly querying all files** | å…¨é‡æŸ¥è©¢ï¼Œéå¢é‡è¼‰å…¥ |
| **Materialized** | æš—ç¤ºç‰©åŒ–è¦–åœ–ï¼Œéä¸²æµè™•ç† |

---

### é¸é … B - "Directory Listing + directly querying all files"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

é¸é … B çš„å…©å€‹éƒ¨åˆ†ï¼š
- **ç¬¬ä¸€éƒ¨åˆ†ï¼š** "New files are identified by listing the input directory"
  - âœ… æ­£ç¢ºï¼Directory Listing æ¨¡å¼ï¼ˆé è¨­ï¼‰
  
- **ç¬¬äºŒéƒ¨åˆ†ï¼š** "target table is materialized by directly querying all valid files"
  - âŒ éŒ¯èª¤ï¼æ‡‰è©²æ˜¯å¢é‡è¼‰å…¥ï¼Œä¸æ˜¯æŸ¥è©¢æ‰€æœ‰æª”æ¡ˆ

```python
# é¸é … B çš„å•é¡Œ

# âœ… ç¬¬ä¸€éƒ¨åˆ†æ­£ç¢ºï¼šDirectory Listing
spark.readStream.format("cloudFiles").load("/path/")

# âŒ ç¬¬äºŒéƒ¨åˆ†éŒ¯èª¤ï¼šä¸æ˜¯æŸ¥è©¢æ‰€æœ‰æª”æ¡ˆ
# Auto Loader æ˜¯å¢é‡è™•ç†ï¼Œä¸æ˜¯å…¨é‡æŸ¥è©¢

# é¸é … B æè¿°çš„æ›´åƒï¼š
# 1. ç”¨ Directory Listing æ‰¾æª”æ¡ˆ
# 2. ç„¶å¾Œæ‰¹æ¬¡è®€å–æ‰€æœ‰æª”æ¡ˆ
df = spark.read.format("json").load("/path/")  # âŒ ä¸æ˜¯ Auto Loader

# Auto Loader çš„å¯¦éš›è¡Œç‚ºï¼š
# 1. ç”¨ Directory Listing æ‰¾æ–°æª”æ¡ˆ
# 2. åªè¼‰å…¥æ–°æª”æ¡ˆï¼ˆå¢é‡ï¼‰
stream = spark.readStream.format("cloudFiles").load("/path/")  # âœ…
```

**é—œéµå·®ç•°ï¼š**

| é …ç›® | é¸é … Bï¼ˆéŒ¯èª¤ï¼‰ | å¯¦éš›è¡Œç‚ºï¼ˆé¸é … Dï¼‰ |
|------|--------------|------------------|
| **æª”æ¡ˆè­˜åˆ¥** | âœ… Directory Listing | âœ… Directory Listing |
| **è™•ç†æ–¹å¼** | âŒ æŸ¥è©¢æ‰€æœ‰æª”æ¡ˆ | âœ… å¢é‡è¼‰å…¥æ–°æª”æ¡ˆ |
| **ç›®æ¨™è¡¨** | âŒ Materializedï¼ˆå…¨é‡ï¼‰ | âœ… Incrementalï¼ˆå¢é‡ï¼‰ |

---

### é¸é … C - "Webhooks trigger Databricks job"

**ç‚ºä»€éº¼å®Œå…¨éŒ¯èª¤ï¼Ÿ**

```python
# é¸é … C æè¿°çš„æ¶æ§‹ï¼ˆä¸å­˜åœ¨ï¼‰
# "Webhooks trigger a Databricks job..."
```

**âŒ å®Œå…¨éŒ¯èª¤çš„æè¿°ï¼š**

| å•é¡Œ | èªªæ˜ |
|------|------|
| **ç„¡ Webhooks** | Auto Loader ä¸ä½¿ç”¨ webhooks |
| **ä¸æ˜¯è§¸ç™¼ä½œæ¥­** | Auto Loader æ˜¯ä¸²æµè™•ç†ï¼ŒæŒçºŒé‹è¡Œ |
| **éè‡ªå‹• MERGE** | Auto Loader åŸ·è¡Œ appendï¼Œä¸æ˜¯ merge |
| **éæ¨æ–·è¦å‰‡** | è³‡æ–™è¼‰å…¥é‚è¼¯ç”±ä½¿ç”¨è€…å®šç¾©ï¼Œéè‡ªå‹•æ¨æ–· |

**Auto Loader çš„å¯¦éš›é‹ä½œæ–¹å¼ï¼š**

```python
# Auto Loader çš„å¯¦éš›æ¶æ§‹ï¼ˆç„¡ Webhooksï¼‰

# 1. æŒçºŒé‹è¡Œçš„ä¸²æµæŸ¥è©¢ï¼ˆéè§¸ç™¼å¼ä½œæ¥­ï¼‰
query = (spark.readStream
    .format("cloudFiles")
    .load("/path/")
    .writeStream
    .format("delta")
    .trigger(processingTime="10 seconds")  # å®šæœŸæª¢æŸ¥ï¼Œé webhook
    .start()
)

# 2. æª¢æŸ¥æ–°æª”æ¡ˆçš„æ©Ÿåˆ¶ï¼š
# - Directory Listing: å®šæœŸåˆ—å‡ºç›®éŒ„
# - File Notification: é›²ç«¯é€šçŸ¥ï¼ˆEvent Grid/SNSï¼‰ï¼Œä¸æ˜¯ webhook

# 3. è³‡æ–™è¼‰å…¥æ–¹å¼ï¼š
# - Appendï¼ˆè¿½åŠ ï¼‰ï¼Œä¸æ˜¯ Merge
# - ä¸æœƒè‡ªå‹•æ¨æ–· merge è¦å‰‡

# âŒ é¸é … C æè¿°çš„åƒæ˜¯ï¼š
# Webhook â†’ è§¸ç™¼ä½œæ¥­ â†’ è‡ªå‹• Merge
# é€™å®Œå…¨ä¸æ˜¯ Auto Loader çš„é‹ä½œæ–¹å¼ï¼
```

---

### é¸é … E - "File Notification + incremental loading"

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

é¸é … E çš„å…©å€‹éƒ¨åˆ†ï¼š
- **ç¬¬ä¸€éƒ¨åˆ†ï¼š** "Cloud vendor-specific queue storage and notification services"
  - âŒ é€™æ˜¯ **File Notification æ¨¡å¼**ï¼Œä¸æ˜¯é è¨­æ¨¡å¼ï¼
  
- **ç¬¬äºŒéƒ¨åˆ†ï¼š** "incrementally and idempotently loaded into the target Delta Lake table"
  - âœ… é€™éƒ¨åˆ†æ­£ç¢ºï¼ˆå¢é‡ä¸”å†ªç­‰ï¼‰

```python
# é¸é … E æè¿°çš„æ˜¯ File Notification æ¨¡å¼ï¼ˆéé è¨­ï¼‰

df = (spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    
    # âš ï¸ é€™éœ€è¦æ˜ç¢ºå•Ÿç”¨ï¼ˆä¸æ˜¯é è¨­ï¼‰
    .option("cloudFiles.useNotifications", "true")
    
    .load("/mnt/source/data/")
)

# âœ… ç¬¬äºŒéƒ¨åˆ†æ­£ç¢ºï¼šå¢é‡ä¸”å†ªç­‰è¼‰å…¥
# âŒ ç¬¬ä¸€éƒ¨åˆ†éŒ¯èª¤ï¼šFile Notification ä¸æ˜¯é è¨­æ¨¡å¼
```

**é¸é … E çš„é™·é˜±ï¼š**

| é™·é˜± | èªªæ˜ |
|------|------|
| **çœ‹èµ·ä¾†å¾ˆå…ˆé€²** | File Notification æ˜¯æ›´é€²éšçš„åŠŸèƒ½ |
| **ç¬¬äºŒéƒ¨åˆ†æ­£ç¢º** | å¢é‡ä¸”å†ªç­‰è¼‰å…¥ç¢ºå¯¦æ­£ç¢º |
| **å¿½ç•¥ã€Œé è¨­ã€** | é¡Œç›®å•çš„æ˜¯ã€Œ**default** execution modeã€|
| **æ··æ·†æ¨¡å¼** | File Notification éœ€è¦æ˜ç¢ºå•Ÿç”¨ï¼Œä¸æ˜¯é è¨­ |

**è¨˜æ†¶æ³•ï¼š**

```
é è¨­ = ç°¡å–® = Directory Listingï¼ˆé¸é … Dï¼‰
é€²éš = è¤‡é›œ = File Notificationï¼ˆé¸é … Eï¼‰
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£è¨˜æ†¶

**ã€Œé è¨­åˆ—ç›®éŒ„ï¼Œå¢é‡åˆå†ªç­‰ã€**

```python
# é è¨­ = Directory Listing
# åˆ—ç›®éŒ„ = list directory
# å¢é‡ = incremental
# å†ªç­‰ = idempotent
```

**ã€Œç°¡å–®å…ˆç”¨ Listï¼Œè¤‡é›œæ‰ Notifyã€**
- Directory Listing = é è¨­ã€ç°¡å–®
- File Notification = é€²éšã€è¤‡é›œ

### æ¦‚å¿µå°æ¯”è¡¨

| é …ç›® | Directory Listingï¼ˆé è¨­ï¼‰| File Notificationï¼ˆé€²éšï¼‰|
|------|------------------------|-------------------------|
| **é è¨­æ¨¡å¼** | âœ… æ˜¯ | âŒ å¦ |
| **æª”æ¡ˆç™¼ç¾** | åˆ—å‡ºç›®éŒ„ | é›²ç«¯é€šçŸ¥æœå‹™ |
| **è¨­å®š** | ç„¡éœ€é¡å¤–è¨­å®š | éœ€è¨­å®šé€šçŸ¥æœå‹™ |
| **å°æ‡‰é¸é …** | D âœ… | E âŒ |
| **æ“´å±•æ€§** | < 10,000 æª”æ¡ˆ | > 10,000 æª”æ¡ˆ |
| **æˆæœ¬** | ä½ | é«˜ |

### é¸é …å¿«é€Ÿåˆ¤æ–·

```
çœ‹é¸é …é—œéµå­—ï¼š

"listing the input directory" 
  â†’ Directory Listing æ¨¡å¼
  â†’ å¯èƒ½æ­£ç¢ºï¼ˆé è¨­æ¨¡å¼ï¼‰
  â†’ é¸é … B æˆ– D

"Cloud vendor-specific queue storage and notification"
  â†’ File Notification æ¨¡å¼  
  â†’ ä¸æ˜¯é è¨­ï¼
  â†’ é¸é … A æˆ– E âŒ

"incrementally and idempotently loaded"
  â†’ å¢é‡ä¸”å†ªç­‰
  â†’ Auto Loader çš„æ ¸å¿ƒç‰¹æ€§ âœ…
  â†’ é¸é … D æˆ– E

"directly querying all valid files"
  â†’ å…¨é‡æŸ¥è©¢
  â†’ ä¸æ˜¯å¢é‡è¼‰å…¥ âŒ
  â†’ é¸é … A æˆ– B

"Webhooks"
  â†’ å®Œå…¨éŒ¯èª¤ âŒ
  â†’ é¸é … C
```

### æ±ºç­–æ¨¹

```
é¡Œç›®å•ï¼šé è¨­åŸ·è¡Œæ¨¡å¼ï¼ˆdefaultï¼‰
  â†“
ç¬¬ä¸€æ­¥ï¼šæ’é™¤éé è¨­æ¨¡å¼
  â”œâ”€ File Notificationï¼ˆé¸é … A, Eï¼‰âŒ éé è¨­
  â”œâ”€ Webhooksï¼ˆé¸é … Cï¼‰âŒ ä¸å­˜åœ¨
  â””â”€ Directory Listingï¼ˆé¸é … B, Dï¼‰âœ… å¯èƒ½æ­£ç¢º
  
ç¬¬äºŒæ­¥ï¼šç¢ºèªè™•ç†æ–¹å¼
  â”œâ”€ é¸é … B: "directly querying all files" âŒ å…¨é‡æŸ¥è©¢
  â””â”€ é¸é … D: "incrementally and idempotently loaded" âœ… å¢é‡è¼‰å…¥

ç­”æ¡ˆï¼šD âœ…
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶èˆ‡åƒè€ƒè³‡æº

### å®˜æ–¹æ–‡ä»¶é€£çµ

1. **Auto Loader æ¦‚è¿°:**
   - [What is Auto Loader?](https://docs.databricks.com/ingestion/auto-loader/index.html)
   - èªªæ˜ Auto Loader çš„åŸºæœ¬æ¦‚å¿µèˆ‡å„ªå‹¢

2. **Directory Listing vs File Notification:**
   - [Auto Loader File Detection Modes](https://docs.databricks.com/ingestion/auto-loader/file-detection.html)
   - è©³ç´°æ¯”è¼ƒå…©ç¨®æ¨¡å¼çš„å·®ç•°

3. **cloudFiles æ ¼å¼é¸é …:**
   - [Auto Loader Options](https://docs.databricks.com/ingestion/auto-loader/options.html)
   - æ‰€æœ‰ cloudFiles çš„é…ç½®é¸é …

4. **Schema æ¨æ–·èˆ‡æ¼”é€²:**
   - [Schema Inference and Evolution](https://docs.databricks.com/ingestion/auto-loader/schema.html)
   - Auto Loader çš„ schema è™•ç†æ©Ÿåˆ¶

### å»¶ä¼¸å­¸ç¿’è³‡æº

- **File Notification è¨­å®šæŒ‡å—:** å¦‚ä½•é…ç½®é›²ç«¯é€šçŸ¥æœå‹™
- **æ•ˆèƒ½å„ªåŒ–:** Auto Loader çš„æœ€ä½³å¯¦å‹™
- **éŒ¯èª¤è™•ç†:** Rescue æ¬„ä½èˆ‡å£è³‡æ–™è™•ç†

---

## ğŸ’¡ è£œå……èªªæ˜

### å¯¦å‹™æ‡‰ç”¨å ´æ™¯

**1. å®Œæ•´çš„ Auto Loader ç®¡é“ï¼ˆé è¨­æ¨¡å¼ï¼‰ï¼š**

```python
# å ´æ™¯ï¼šå¾ S3/ADLS æ”å– JSON æ—¥èªŒåˆ° Delta Lake

from pyspark.sql.functions import *
from pyspark.sql.types import *

# === é…ç½®åƒæ•¸ ===
source_path = "/mnt/raw/application_logs/"
checkpoint_path = "/mnt/checkpoints/logs_ingestion"
schema_location = "/mnt/schemas/logs"
target_table = "bronze.application_logs"

# === è¨­å®š Auto Loaderï¼ˆé è¨­ Directory Listingï¼‰ ===
logs_stream = (spark.readStream
    .format("cloudFiles")
    
    # ä¾†æºæª”æ¡ˆæ ¼å¼
    .option("cloudFiles.format", "json")
    
    # Schema ç®¡ç†
    .option("cloudFiles.schemaLocation", schema_location)
    .option("cloudFiles.inferColumnTypes", "true")
    .option("cloudFiles.schemaEvolutionMode", "addNewColumns")
    
    # å£è³‡æ–™è™•ç†
    .option("cloudFiles.rescuedDataColumn", "_rescued_data")
    
    # âš ï¸ é è¨­ä½¿ç”¨ Directory Listingï¼ˆä¸éœ€æ˜ç¢ºè¨­å®šï¼‰
    # å¦‚æœè¦ç¢ºèªï¼Œå¯ä»¥åŠ ä¸Šï¼š
    # .option("cloudFiles.useNotifications", "false")
    
    # é¡å¤–çš„è®€å–é¸é …
    .option("multiLine", "false")
    .option("pathGlobFilter", "*.json")  # åªè™•ç† .json æª”æ¡ˆ
    
    .load(source_path)
)

# === è³‡æ–™è½‰æ› ===
transformed_logs = (logs_stream
    # æ–°å¢ metadata æ¬„ä½
    .withColumn("ingestion_timestamp", current_timestamp())
    .withColumn("source_file", input_file_name())
    .withColumn("file_modification_time", col("_metadata.file_modification_time"))
    
    # è³‡æ–™æ¸…æ´—
    .withColumn("log_level", upper(col("level")))
    .withColumn("timestamp", to_timestamp(col("timestamp")))
    
    # åˆ†å€æ¬„ä½
    .withColumn("date", to_date(col("timestamp")))
    .withColumn("hour", hour(col("timestamp")))
)

# === å¯«å…¥ Delta Lakeï¼ˆå¢é‡ä¸”å†ªç­‰ï¼‰ ===
query = (transformed_logs.writeStream
    .format("delta")
    .outputMode("append")
    
    # Checkpointï¼ˆç¢ºä¿ exactly-once å’Œ idempotentï¼‰
    .option("checkpointLocation", checkpoint_path)
    
    # Schema æ¼”é€²
    .option("mergeSchema", "true")
    
    # è§¸ç™¼é »ç‡
    .trigger(processingTime="30 seconds")  # æ¯ 30 ç§’æª¢æŸ¥æ–°æª”æ¡ˆ
    
    # åˆ†å€ç­–ç•¥
    .partitionBy("date", "hour")
    
    # ç›®æ¨™è¡¨
    .table(target_table)
)

# === ç›£æ§ ===
print(f"""
Auto Loader ç®¡é“å·²å•Ÿå‹•ï¼š
- æ¨¡å¼: Directory Listingï¼ˆé è¨­ï¼‰
- ä¾†æº: {source_path}
- ç›®æ¨™: {target_table}
- Checkpoint: {checkpoint_path}
- è§¸ç™¼é »ç‡: 30 ç§’
- ç‰¹æ€§: Incremental + Idempotent
""")

# ä¿æŒé‹è¡Œ
query.awaitTermination()
```

**2. å¾ Directory Listing å‡ç´šåˆ° File Notificationï¼š**

```python
# å ´æ™¯ï¼šè³‡æ–™é‡å¢é•·ï¼Œéœ€è¦å‡ç´šåˆ° File Notification

# === è©•ä¼°æ˜¯å¦éœ€è¦å‡ç´š ===
def should_upgrade_to_file_notification(source_path):
    """
    è©•ä¼°æ˜¯å¦éœ€è¦å¾ Directory Listing å‡ç´šåˆ° File Notification
    """
    
    # è¨ˆç®—æª”æ¡ˆæ•¸é‡
    files = dbutils.fs.ls(source_path)
    total_files = len([f for f in files if not f.name.startswith('_')])
    
    # è¨ˆç®—æ¯æ—¥æ–°å¢æª”æ¡ˆæ•¸ï¼ˆå‡è¨­ï¼‰
    daily_new_files = 1000  # æ ¹æ“šå¯¦éš›æƒ…æ³èª¿æ•´
    
    print(f"""
å‡ç´šè©•ä¼°ï¼š
- ç›®å‰ç¸½æª”æ¡ˆæ•¸: {total_files:,}
- æ¯æ—¥æ–°å¢: {daily_new_files:,}
""")
    
    # å»ºè­°
    if total_files > 10000 or daily_new_files > 100:
        print("âœ… å»ºè­°å‡ç´šåˆ° File Notification æ¨¡å¼")
        return True
    else:
        print("âœ… ç¹¼çºŒä½¿ç”¨ Directory Listing æ¨¡å¼ï¼ˆé è¨­ï¼‰")
        return False

# === å‡ç´šåˆ° File Notificationï¼ˆå¦‚æœéœ€è¦ï¼‰ ===
if should_upgrade_to_file_notification("/mnt/raw/application_logs/"):
    
    # åœæ­¢ç¾æœ‰çš„ä¸²æµæŸ¥è©¢
    for s in spark.streams.active:
        if s.name == "application_logs":
            s.stop()
    
    # é‡æ–°é…ç½®ç‚º File Notification æ¨¡å¼
    logs_stream = (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        
        # âš ï¸ å•Ÿç”¨ File Notificationï¼ˆéé è¨­ï¼‰
        .option("cloudFiles.useNotifications", "true")
        
        # Azure ç‰¹å®šè¨­å®š
        .option("cloudFiles.resourceGroup", "my-resource-group")
        .option("cloudFiles.subscriptionId", "my-subscription-id")
        .option("cloudFiles.clientId", "my-client-id")
        .option("cloudFiles.clientSecret", "my-client-secret")
        
        # æˆ– AWS ç‰¹å®šè¨­å®š
        # .option("cloudFiles.region", "us-west-2")
        
        .load("/mnt/raw/application_logs/")
    )
    
    # å…¶é¤˜é…ç½®ç›¸åŒ...
    query = logs_stream.writeStream.format("delta").start()
```

**3. æ¸¬è©¦ Idempotencyï¼ˆå†ªç­‰æ€§ï¼‰ï¼š**

```python
# é©—è­‰ Auto Loader çš„å†ªç­‰æ€§

def test_auto_loader_idempotency():
    """
    æ¸¬è©¦ Auto Loader åœ¨å¤±æ•—é‡è©¦æ™‚çš„å†ªç­‰æ€§
    """
    
    # æº–å‚™æ¸¬è©¦è³‡æ–™
    test_path = "/tmp/autoloader_test/source/"
    checkpoint_path = "/tmp/autoloader_test/checkpoint/"
    target_table = "test.idempotency"
    
    # æ¸…ç†èˆŠè³‡æ–™
    dbutils.fs.rm(test_path, recurse=True)
    dbutils.fs.rm(checkpoint_path, recurse=True)
    spark.sql(f"DROP TABLE IF EXISTS {target_table}")
    
    # å»ºç«‹æ¸¬è©¦æª”æ¡ˆ
    test_data = spark.range(0, 100).toDF("id")
    test_data.write.format("json").save(f"{test_path}batch1/")
    
    # === ç¬¬ä¸€æ¬¡åŸ·è¡Œ ===
    query1 = (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .load(test_path)
        .writeStream
        .format("delta")
        .option("checkpointLocation", checkpoint_path)
        .trigger(once=True)
        .table(target_table)
    )
    query1.awaitTermination()
    count1 = spark.table(target_table).count()
    print(f"ç¬¬ä¸€æ¬¡åŸ·è¡Œ: {count1} ç­†è¨˜éŒ„")
    
    # === ç¬¬äºŒæ¬¡åŸ·è¡Œï¼ˆç„¡æ–°æª”æ¡ˆï¼‰ ===
    query2 = (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .load(test_path)
        .writeStream
        .format("delta")
        .option("checkpointLocation", checkpoint_path)
        .trigger(once=True)
        .table(target_table)
    )
    query2.awaitTermination()
    count2 = spark.table(target_table).count()
    print(f"ç¬¬äºŒæ¬¡åŸ·è¡Œ: {count2} ç­†è¨˜éŒ„")
    
    # === æ–°å¢æª”æ¡ˆ ===
    new_data = spark.range(100, 150).toDF("id")
    new_data.write.format("json").save(f"{test_path}batch2/")
    
    # === ç¬¬ä¸‰æ¬¡åŸ·è¡Œï¼ˆæœ‰æ–°æª”æ¡ˆï¼‰ ===
    query3 = (spark.readStream
        .format("cloudFiles")
        .option("cloudFiles.format", "json")
        .load(test_path)
        .writeStream
        .format("delta")
        .option("checkpointLocation", checkpoint_path)
        .trigger(once=True)
        .table(target_table)
    )
    query3.awaitTermination()
    count3 = spark.table(target_table).count()
    print(f"ç¬¬ä¸‰æ¬¡åŸ·è¡Œ: {count3} ç­†è¨˜éŒ„")
    
    # === é©—è­‰ ===
    assert count1 == 100, "ç¬¬ä¸€æ¬¡æ‡‰è™•ç† 100 ç­†"
    assert count2 == 100, "ç¬¬äºŒæ¬¡æ‡‰ä¿æŒ 100 ç­†ï¼ˆIdempotentï¼‰"
    assert count3 == 150, "ç¬¬ä¸‰æ¬¡æ‡‰ç‚º 150 ç­†ï¼ˆ100 + 50ï¼‰"
    
    print("""
âœ… å†ªç­‰æ€§æ¸¬è©¦é€šéï¼
- ç¬¬ä¸€æ¬¡: 100 ç­†ï¼ˆè™•ç† batch1ï¼‰
- ç¬¬äºŒæ¬¡: 100 ç­†ï¼ˆç„¡è®ŠåŒ–ï¼ŒIdempotentï¼‰
- ç¬¬ä¸‰æ¬¡: 150 ç­†ï¼ˆåªè™•ç† batch2 çš„æ–°å¢ 50 ç­†ï¼ŒIncrementalï¼‰
""")

test_auto_loader_idempotency()
```

**4. ç›£æ§ Auto Loader æ•ˆèƒ½ï¼š**

```python
# ç›£æ§ Auto Loader çš„è™•ç†æ•ˆèƒ½

from delta.tables import DeltaTable
import time

def monitor_auto_loader_performance(table_name, checkpoint_path):
    """
    ç›£æ§ Auto Loader çš„è™•ç†æ•ˆèƒ½èˆ‡æª”æ¡ˆæ•¸é‡
    """
    
    delta_table = DeltaTable.forName(spark, table_name)
    
    # === æŸ¥çœ‹è™•ç†æ­·å² ===
    history_df = delta_table.history(10).select(
        "timestamp",
        "operation",
        "operationMetrics.numOutputRows",
        "operationMetrics.numFiles",
        "operationMetrics.executionTimeMs"
    )
    
    print("è™•ç†æ­·å²ï¼ˆæœ€è¿‘ 10 æ¬¡ï¼‰ï¼š")
    history_df.show(truncate=False)
    
    # === æŸ¥çœ‹ Checkpoint ç‹€æ…‹ ===
    checkpoint_files = dbutils.fs.ls(checkpoint_path)
    print(f"\nCheckpoint æª”æ¡ˆæ•¸: {len(checkpoint_files)}")
    
    # === æŸ¥çœ‹å·²è™•ç†æª”æ¡ˆæ¸…å–® ===
    sources_path = f"{checkpoint_path}/sources"
    if any(f.path.endswith('sources/') for f in checkpoint_files):
        sources = dbutils.fs.ls(sources_path)
        print(f"å·²è¿½è¹¤ä¾†æºæ•¸: {len(sources)}")
    
    # === è¨ˆç®—è™•ç†é€Ÿç‡ ===
    recent_ops = history_df.filter("operation = 'STREAMING UPDATE'").limit(5)
    if recent_ops.count() > 0:
        avg_time = recent_ops.agg({"executionTimeMs": "avg"}).collect()[0][0]
        avg_rows = recent_ops.agg({"numOutputRows": "avg"}).collect()[0][0]
        
        if avg_time and avg_rows:
            throughput = (avg_rows / avg_time) * 1000  # rows/second
            print(f"\nå¹³å‡è™•ç†æ•ˆèƒ½:")
            print(f"- åŸ·è¡Œæ™‚é–“: {avg_time:.0f} ms")
            print(f"- è™•ç†ç­†æ•¸: {avg_rows:.0f}")
            print(f"- ååé‡: {throughput:.0f} rows/second")
    
    # === æª¢æŸ¥æ˜¯å¦éœ€è¦å„ªåŒ– ===
    total_files = spark.sql(f"""
        SELECT COUNT(DISTINCT input_file_name()) as file_count
        FROM {table_name}
    """).collect()[0]["file_count"]
    
    print(f"\nç›®æ¨™è¡¨æª”æ¡ˆæ•¸: {total_files}")
    
    if total_files > 1000:
        print("âš ï¸ å»ºè­°: è€ƒæ…®åŸ·è¡Œ OPTIMIZE æ¸›å°‘æª”æ¡ˆæ•¸")

# ä½¿ç”¨ç¯„ä¾‹
monitor_auto_loader_performance("bronze.application_logs", "/mnt/checkpoints/logs_ingestion")
```

---

## ğŸ¯ è€ƒè©¦æŠ€å·§

### å¿«é€Ÿåˆ¤æ–·æ³•

**çœ‹åˆ°ã€Œdefault execution modeã€é¡Œç›®ï¼Œé—œéµåˆ¤æ–·ï¼š**

| é—œéµå­— | åˆ¤æ–· | å°æ‡‰é¸é … |
|--------|------|---------|
| **"default"** | é è¨­ = ç°¡å–® = Directory Listing | D âœ… |
| **"listing the input directory"** | Directory Listing æ¨¡å¼ | B æˆ– D |
| **"incrementally and idempotently"** | å¢é‡ä¸”å†ªç­‰ï¼ˆæ­£ç¢ºç‰¹æ€§ï¼‰ | D æˆ– E |
| **"cloud...notification services"** | File Notificationï¼ˆéé è¨­ï¼‰ | A æˆ– E âŒ |
| **"directly querying all files"** | å…¨é‡æŸ¥è©¢ï¼ˆéå¢é‡ï¼‰ | A æˆ– B âŒ |
| **"webhooks"** | ä¸å­˜åœ¨çš„åŠŸèƒ½ | C âŒ |

### é™·é˜±è­˜åˆ¥

**æœ¬é¡Œçš„é™·é˜±è¨­è¨ˆï¼š**

1. **é¸é … Eï¼ˆæœ€å¤§é™·é˜±ï¼‰** - File Notification çœ‹èµ·ä¾†æ›´å…ˆé€²ï¼Œç¬¬äºŒéƒ¨åˆ†ä¹Ÿæ­£ç¢ºï¼Œä½†ä¸æ˜¯é è¨­æ¨¡å¼
2. **é¸é … B** - Directory Listing æ­£ç¢ºï¼Œä½†è™•ç†æ–¹å¼éŒ¯èª¤ï¼ˆå…¨é‡ vs å¢é‡ï¼‰
3. **é¸é … A** - çµåˆäº†éé è¨­æ¨¡å¼å’ŒéŒ¯èª¤è™•ç†æ–¹å¼
4. **é¸é … C** - å®Œå…¨è™›æ§‹çš„åŠŸèƒ½ï¼ˆwebhooksï¼‰

### å…©æ­¥é©Ÿæ’é™¤æ³•

```
ç¬¬ä¸€æ­¥ï¼šæ’é™¤è™•ç†æ–¹å¼éŒ¯èª¤çš„é¸é …
  â”œâ”€ "directly querying all files" â†’ é¸é … A, B âŒ
  â”œâ”€ "webhooks" + "automatically merged" â†’ é¸é … C âŒ
  â””â”€ "incrementally and idempotently loaded" â†’ é¸é … D, E âœ…

ç¬¬äºŒæ­¥ï¼šç¢ºèªæ˜¯å¦ç‚ºé è¨­æ¨¡å¼
  â”œâ”€ é¸é … E: File Notification â†’ âŒ éé è¨­
  â””â”€ é¸é … D: Directory Listing â†’ âœ… é è¨­

ç­”æ¡ˆï¼šD âœ…
```

---

## âš ï¸ é‡é»ç¸½çµ

### æ ¸å¿ƒè¦é»

1. **Auto Loader é è¨­æ¨¡å¼ = Directory Listing**
   - é€éåˆ—å‡ºç›®éŒ„è­˜åˆ¥æ–°æª”æ¡ˆ
   - ç„¡éœ€é¡å¤–è¨­å®šé›²ç«¯é€šçŸ¥æœå‹™
   - é©åˆå°åˆ°ä¸­å‹è¦æ¨¡ï¼ˆ< 10,000 æª”æ¡ˆï¼‰

2. **è™•ç†ç‰¹æ€§ï¼šIncremental + Idempotent**
   - Incrementalï¼šåªè™•ç†æ–°å¢æª”æ¡ˆ
   - Idempotentï¼šç›¸åŒæª”æ¡ˆä¸é‡è¤‡è™•ç†
   - Checkpoint ç¢ºä¿ exactly-once èªç¾©

3. **File Notification æ˜¯é€²éšåŠŸèƒ½ï¼ˆéé è¨­ï¼‰**
   - éœ€æ˜ç¢ºè¨­å®š `cloudFiles.useNotifications = true`
   - éœ€é…ç½®é›²ç«¯é€šçŸ¥æœå‹™
   - é©åˆå¤§è¦æ¨¡æˆ–é«˜é »ç‡å ´æ™¯

### è¨˜æ†¶å£è¨£

**ã€Œé è¨­åˆ—ç›®éŒ„ï¼Œå¢é‡åˆå†ªç­‰ã€**
- é è¨­ = Directory Listing
- å¢é‡ = Incremental
- å†ªç­‰ = Idempotent

---

**[è¿”å›é¡Œç›®](#question-108)**