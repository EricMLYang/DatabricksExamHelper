# Q-095

## é¡Œç›®è³‡è¨Š

**ID:** `Q-095`

**ä¾†æº:** Mock Exam

**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A task orchestrator has been configured to run two hourly tasks. First, an outside system writes Parquet data to a directory mounted at /mnt/raw_orders/. After this data is written, a Databricks job containing the following code is executed:

```python
(spark.readStream
.format("parquet")
.load("/mnt/raw_orders/")
.withWatermark("time", "2 hours")
.dropDuplicates(["customer_id", "order_id"])
.writeStream
.trigger(once=True)
.table("orders")
)
```

Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order, and that the time field indicates when the record was queued in the source system.

If the upstream system is known to occasionally enqueue duplicate entries for a single order hours apart, which statement is correct?

### é¸é …

- **A.** Duplicate records enqueued more than 2 hours apart may be retained and the orders table may contain duplicate records with the same customer_id and order_id.
- **B.** All records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.
- **C.** The orders table will contain only the most recent 2 hours of records and no duplicates will be present.
- **D.** Duplicate records arriving more than 2 hours apart will be dropped, but duplicates that arrive in the same batch may both be written to the orders table.
- **E.** The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table.

---

## æ¨™ç±¤ç³»çµ±

**Topics:** `Streaming`, `Streaming-Stateful`, `Data-Quality`

**Traps:** `Watermark-Misunderstanding`, `State-Management-Confusion`

**Domain:** `æ•¸æ“šæ”å– (Data Ingestion)`

---

## ç­”æ¡ˆèˆ‡åˆ†æ

### æ­£ç¢ºç­”æ¡ˆ

**æ­£è§£:** `A`

**ç¤¾ç¾¤æŠ•ç¥¨:** A (86%)
**ä¾†æºæ¨™è¨»:** A

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming çš„ Watermark èˆ‡ Deduplication æ©Ÿåˆ¶
**çŸ¥è­˜é ˜åŸŸ:** æ•¸æ“šæ”å– - Streaming è™•ç†èˆ‡ç‹€æ…‹ç®¡ç†
**é—œéµæ¦‚å¿µ:** 
- Watermark çš„ä½œç”¨èˆ‡é™åˆ¶
- dropDuplicates çš„ç‹€æ…‹ç®¡ç†
- State store çš„è¨˜æ†¶é«”ç®¡ç†

### æ¬¡è¦è€ƒé»
- Trigger(once=True) çš„åŸ·è¡Œæ¨¡å¼
- ä¸²æµå»é‡çš„æœ€ä½³å¯¦è¸
- Late-arriving data è™•ç†

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æŠ€è¡“åŸç†:**

**Watermark åœ¨ Streaming Deduplication ä¸­çš„ä½œç”¨ï¼š**

```
Watermark çš„æ ¸å¿ƒæ¦‚å¿µï¼š
1. Watermark å®šç¾©ã€Œå¤šä¹…ä»¥å‰ã€çš„è³‡æ–™å¯ä»¥è¢«æ¸…é™¤
2. åœ¨å»é‡å ´æ™¯ä¸­ï¼ŒWatermark æ§åˆ¶ state store çš„ä¿ç•™æ™‚é–“
3. è¶…é Watermark æ™‚é–“çš„é‡è¤‡è¨˜éŒ„ç‹€æ…‹æœƒè¢«ç§»é™¤

åœ¨æœ¬é¡Œä¸­ï¼š
- withWatermark("time", "2 hours")
- è¡¨ç¤ºï¼šä¿ç•™æœ€è¿‘ 2 å°æ™‚çš„å»é‡ç‹€æ…‹
- æ„å‘³ï¼šè¶…é 2 å°æ™‚çš„èˆŠè¨˜éŒ„ç‹€æ…‹æœƒè¢«æ¸…é™¤
```

**æœ¬é¡Œç¨‹å¼ç¢¼çš„é‹ä½œæ©Ÿåˆ¶ï¼š**

```python
(spark.readStream
.format("parquet")
.load("/mnt/raw_orders/")
.withWatermark("time", "2 hours")           # â† Watermark è¨­å®š
.dropDuplicates(["customer_id", "order_id"]) # â† å»é‡é‚è¼¯
.writeStream
.trigger(once=True)                         # â† å¾®æ‰¹æ¬¡åŸ·è¡Œ
.table("orders")
)
```

**åŸ·è¡Œæµç¨‹èˆ‡ç‹€æ…‹ç®¡ç†ï¼š**

```
æ™‚é–“è»¸ç¯„ä¾‹ï¼š

T0 (10:00): ç¬¬ä¸€ç­†è¨‚å–®é€²å…¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_id: 123               â”‚
â”‚ order_id: A001                 â”‚
â”‚ time: 10:00                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ å¯«å…¥ orders è¡¨æ ¼
â†’ State store è¨˜éŒ„: (123, A001) = å·²è¦‹é

T1 (11:00): 1 å°æ™‚å¾Œï¼Œç›¸åŒè¨‚å–®å†æ¬¡é€²å…¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_id: 123               â”‚
â”‚ order_id: A001                 â”‚
â”‚ time: 11:00                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†’ State store æª¢æŸ¥: (123, A001) å·²å­˜åœ¨
â†’ è¢«å»é‡ï¼Œä¸å¯«å…¥ orders è¡¨æ ¼ âœ“

T2 (13:00): 3 å°æ™‚å¾Œï¼Œç›¸åŒè¨‚å–®å†æ¬¡é€²å…¥
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ customer_id: 123               â”‚
â”‚ order_id: A001                 â”‚
â”‚ time: 13:00                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å•é¡Œï¼šæ­¤æ™‚ Watermark æ˜¯å¤šå°‘ï¼Ÿ
Current Watermark = max(event time) - 2 hours
                  = 13:00 - 2 hours = 11:00

State store æ¸…ç†é‚è¼¯ï¼š
- æ™‚é–“æˆ³è¨˜ < 11:00 çš„ç‹€æ…‹æœƒè¢«æ¸…é™¤
- T0 çš„è¨˜éŒ„ (time: 10:00) å·²è¢«æ¸…é™¤ï¼
- State store ä¸­æ²’æœ‰ (123, A001) çš„è¨˜éŒ„

çµæœï¼š
â†’ æ­¤ç­†è¨˜éŒ„è¢«è¦–ç‚ºã€Œæ–°çš„ã€è¨˜éŒ„
â†’ å†æ¬¡å¯«å…¥ orders è¡¨æ ¼
â†’ é€ æˆé‡è¤‡ï¼âŒ
```

**ç¬¦åˆéœ€æ±‚:**

é¡Œç›®å•ï¼šã€ŒIf the upstream system is known to occasionally enqueue duplicate entries for a single order **hours apart**ã€

```
æƒ…å¢ƒåˆ†æï¼š

ä¸Šæ¸¸ç³»çµ±å¯èƒ½åœ¨ã€Œæ•¸å°æ™‚å¾Œã€é‡è¤‡ç™¼é€ç›¸åŒè¨‚å–®

æ¡ˆä¾‹ 1: é‡è¤‡é–“éš” < 2 å°æ™‚
10:00 â†’ è¨‚å–® A001 é€²å…¥
11:00 â†’ è¨‚å–® A001 å†æ¬¡é€²å…¥
â†’ State store ä»ä¿æœ‰è¨˜éŒ„
â†’ æˆåŠŸå»é‡ âœ“

æ¡ˆä¾‹ 2: é‡è¤‡é–“éš” > 2 å°æ™‚
10:00 â†’ è¨‚å–® A001 é€²å…¥
13:00 â†’ è¨‚å–® A001 å†æ¬¡é€²å…¥
â†’ State store å·²æ¸…é™¤ 10:00 çš„è¨˜éŒ„
â†’ ç„¡æ³•å»é‡ï¼Œé€ æˆé‡è¤‡ âŒ

é¸é … A æ­£ç¢ºæè¿°äº†é€™å€‹è¡Œç‚ºï¼š
"Duplicate records enqueued more than 2 hours apart 
may be retained and the orders table may contain 
duplicate records with the same customer_id and order_id."
```

**å¯¦å‹™æ‡‰ç”¨:**

**ç‚ºä½•æœƒç™¼ç”Ÿé€™ç¨®æƒ…æ³ï¼š**

```python
# Watermark çš„é›™åˆƒåŠæ•ˆæœ

å„ªé»ï¼š
- é˜²æ­¢ state store ç„¡é™å¢é•·
- æ§åˆ¶è¨˜æ†¶é«”ä½¿ç”¨
- æå‡ä¸²æµæ•ˆèƒ½

ç¼ºé»ï¼š
- è¶…é watermark æ™‚é–“çš„é‡è¤‡è¨˜éŒ„ç„¡æ³•è¢«åµæ¸¬
- éœ€è¦æ¬Šè¡¡ state ä¿ç•™æ™‚é–“ vs è¨˜æ†¶é«”æ¶ˆè€—
```

**è¦–è¦ºåŒ–ç¯„ä¾‹ï¼š**

```
State Store çš„ç”Ÿå‘½é€±æœŸï¼š

æ™‚é–“ | äº‹ä»¶                  | State Store å…§å®¹     | Watermark
-----|----------------------|---------------------|----------
10:00| è¨‚å–® A001 é€²å…¥       | {A001: 10:00}       | 08:00
11:00| è¨‚å–® B002 é€²å…¥       | {A001: 10:00,       | 09:00
     |                      |  B002: 11:00}       |
12:00| è¨‚å–® C003 é€²å…¥       | {A001: 10:00,       | 10:00
     |                      |  B002: 11:00,       |
     |                      |  C003: 12:00}       |
13:00| è¨‚å–® D004 é€²å…¥       | {B002: 11:00,       | 11:00
     |                      |  C003: 12:00,       |
     |                      |  D004: 13:00}       |
     |                      | â† A001 å·²è¢«æ¸…é™¤ï¼   |
13:30| è¨‚å–® A001 å†æ¬¡é€²å…¥ï¼ | {B002: 11:00,       | 11:30
     | (é‡è¤‡è¨˜éŒ„)           |  C003: 12:00,       |
     |                      |  D004: 13:00,       |
     |                      |  A001: 13:30}       |
     |                      | â† è¢«è¦–ç‚ºæ–°è¨˜éŒ„ï¼âŒ  |
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … B - All records will be held in the state store for 2 hours before being deduplicated and committed to the orders table.

**éŒ¯èª¤åŸå› :** èª¤è§£ Watermark çš„ä½œç”¨ï¼Œè¨˜éŒ„**ä¸æ˜¯ç­‰å¾… 2 å°æ™‚å¾Œæ‰è™•ç†**

**è©³ç´°åˆ†æ:**

**Watermark ä¸æ˜¯å»¶é²è™•ç†ï¼š**

```
éŒ¯èª¤ç†è§£ï¼š
ã€Œè¨˜éŒ„æœƒåœ¨ state store ä¸­ç­‰å¾… 2 å°æ™‚ï¼Œç„¶å¾Œæ‰å»é‡ä¸¦å¯«å…¥ã€

å¯¦éš›é‹ä½œï¼š
- è¨˜éŒ„ç«‹å³è¢«è™•ç†å’Œå»é‡
- Watermark åªæ§åˆ¶ state çš„ã€Œä¿ç•™æ™‚é–“ã€
- ä¸å½±éŸ¿è¨˜éŒ„çš„è™•ç†æ™‚é–“

æ™‚é–“è»¸å°æ¯”ï¼š

éŒ¯èª¤èªçŸ¥ï¼ˆé¸é … Bï¼‰ï¼š
10:00 â†’ è¨‚å–®é€²å…¥ â†’ å­˜å…¥ state â†’ ç­‰å¾… 2 å°æ™‚
12:00 â†’ å»é‡æª¢æŸ¥ â†’ å¯«å…¥ orders è¡¨æ ¼

å¯¦éš›é‹ä½œï¼ˆæ­£ç¢ºï¼‰ï¼š
10:00 â†’ è¨‚å–®é€²å…¥ â†’ ç«‹å³å»é‡æª¢æŸ¥ â†’ ç«‹å³å¯«å…¥ orders
     â†’ åŒæ™‚è¨˜éŒ„åˆ° state store
     â†’ State ä¿ç•™ 2 å°æ™‚ï¼ˆç”¨æ–¼æœªä¾†çš„å»é‡ï¼‰
```

**Trigger(once=True) çš„å½±éŸ¿ï¼š**

```python
.trigger(once=True)

ä½œç”¨ï¼š
- è™•ç†ç•¶å‰å¯ç”¨çš„æ‰€æœ‰è³‡æ–™å¾Œåœæ­¢
- é¡ä¼¼å¾®æ‰¹æ¬¡è™•ç†ï¼ˆmicro-batchï¼‰
- ä¸æœƒç­‰å¾…ä»»ä½•å»¶é²

åŸ·è¡Œæµç¨‹ï¼š
1. è®€å– /mnt/raw_orders/ ä¸­çš„æ‰€æœ‰æª”æ¡ˆ
2. ç«‹å³æ‡‰ç”¨ watermark å’Œå»é‡é‚è¼¯
3. ç«‹å³å¯«å…¥ orders è¡¨æ ¼
4. å®Œæˆå¾Œåœæ­¢

â†’ æ²’æœ‰ã€Œç­‰å¾… 2 å°æ™‚ã€çš„æ­¥é©Ÿ
```

**æ˜“æ··æ·†é»:**
å¯èƒ½èª¤ä»¥ç‚º Watermark æ™‚é–“æ˜¯ã€Œè™•ç†å»¶é²ã€ï¼Œä½†å¯¦éš›ä¸Šæ˜¯ã€Œç‹€æ…‹ä¿ç•™æ™‚é–“ã€

---

### é¸é … C - The orders table will contain only the most recent 2 hours of records and no duplicates will be present.

**éŒ¯èª¤åŸå› :** èª¤è§£ Watermark çš„ä½œç”¨ç¯„åœï¼ŒWatermark **ä¸æœƒé™åˆ¶è¼¸å‡ºè³‡æ–™çš„æ™‚é–“ç¯„åœ**

**è©³ç´°åˆ†æ:**

**Watermark ä¸æ§åˆ¶è¼¸å‡ºçš„æ™‚é–“ç¯„åœï¼š**

```
éŒ¯èª¤ç†è§£ï¼š
ã€ŒWatermark æœƒéæ¿¾æ‰è¶…é 2 å°æ™‚çš„èˆŠè³‡æ–™ã€

å¯¦éš›é‹ä½œï¼š
- Watermark åªæ§åˆ¶ state store çš„æ¸…ç†
- ä¸æœƒéæ¿¾è¼¸å…¥æˆ–è¼¸å‡ºè³‡æ–™
- æ‰€æœ‰è³‡æ–™éƒ½æœƒè¢«å¯«å…¥ï¼ˆé™¤éæ˜¯é‡è¤‡ï¼‰

ç¯„ä¾‹ï¼š

æ™‚é–“ | äº‹ä»¶                | æ˜¯å¦å¯«å…¥ orders?
-----|--------------------|-----------------
10:00| è¨‚å–® A (time=10:00)| âœ“ å¯«å…¥
11:00| è¨‚å–® B (time=11:00)| âœ“ å¯«å…¥
12:00| è¨‚å–® C (time=12:00)| âœ“ å¯«å…¥
13:00| è¨‚å–® D (time=13:00)| âœ“ å¯«å…¥

æŸ¥è©¢ orders è¡¨æ ¼ï¼š
SELECT * FROM orders
â†’ åŒ…å«æ‰€æœ‰æ™‚é–“çš„è¨˜éŒ„ï¼ˆ10:00, 11:00, 12:00, 13:00ï¼‰
â†’ ä¸åªæ˜¯ã€Œæœ€è¿‘ 2 å°æ™‚ã€âŒ
```

**Watermark vs è³‡æ–™éæ¿¾ï¼š**

```python
# Watermark çš„ä½œç”¨ï¼ˆæœ¬é¡Œï¼‰
(spark.readStream
.withWatermark("time", "2 hours")      # â† åªå½±éŸ¿ state store
.dropDuplicates(["customer_id", "order_id"])
)
# æ‰€æœ‰è³‡æ–™éƒ½æœƒè¼¸å‡ºï¼Œåªæ˜¯å»é‡é‚è¼¯æœƒã€Œå¿˜è¨˜ã€2 å°æ™‚å‰çš„ç‹€æ…‹

# å¦‚æœè¦éæ¿¾æ™‚é–“ç¯„åœï¼ˆä¸åŒçš„éœ€æ±‚ï¼‰
(spark.readStream
.filter(col("time") > current_timestamp() - expr("INTERVAL 2 HOURS"))
)  # â† é€™æ‰æœƒé™åˆ¶è¼¸å‡ºç‚ºæœ€è¿‘ 2 å°æ™‚
```

**ç‚ºä½•é¸é … C çš„å…©å€‹æ–·è¨€éƒ½éŒ¯èª¤ï¼š**

```
æ–·è¨€ 1: "åªåŒ…å«æœ€è¿‘ 2 å°æ™‚çš„è¨˜éŒ„"
â†’ éŒ¯èª¤ï¼šWatermark ä¸éæ¿¾è³‡æ–™ï¼Œæ‰€æœ‰è³‡æ–™éƒ½æœƒä¿ç•™

æ–·è¨€ 2: "ä¸æœƒæœ‰é‡è¤‡"
â†’ éŒ¯èª¤ï¼šè¶…é 2 å°æ™‚çš„é‡è¤‡è¨˜éŒ„ç„¡æ³•è¢«åµæ¸¬ï¼ˆå¦‚é¸é … A æ‰€è¿°ï¼‰
```

---

### é¸é … D - Duplicate records arriving more than 2 hours apart will be dropped, but duplicates that arrive in the same batch may both be written to the orders table.

**éŒ¯èª¤åŸå› :** å‰åŠå¥ç›¸åï¼Œå¾ŒåŠå¥ä¹ŸéŒ¯èª¤

**è©³ç´°åˆ†æ:**

**éŒ¯èª¤ 1: è¶…é 2 å°æ™‚çš„é‡è¤‡è¨˜éŒ„æœƒè¢«ã€Œä¿ç•™ã€è€Œéã€Œä¸Ÿæ£„ã€**

```
é¸é … D èªªï¼š
"Duplicate records arriving more than 2 hours apart will be dropped"
ï¼ˆè¶…é 2 å°æ™‚çš„é‡è¤‡è¨˜éŒ„æœƒè¢«ä¸Ÿæ£„ï¼‰

å¯¦éš›æƒ…æ³ï¼ˆèˆ‡é¸é … A ç›¸ç¬¦ï¼‰ï¼š
- è¶…é 2 å°æ™‚çš„é‡è¤‡è¨˜éŒ„æœƒè¢«ã€Œä¿ç•™ã€
- å› ç‚º state store å·²æ¸…é™¤èˆŠçš„è¨˜éŒ„
- ç³»çµ±ç„¡æ³•è­˜åˆ¥é€™æ˜¯é‡è¤‡

ç¯„ä¾‹ï¼š
10:00 â†’ è¨‚å–® A001 é€²å…¥ â†’ å¯«å…¥ âœ“
13:00 â†’ è¨‚å–® A001 å†æ¬¡é€²å…¥ â†’ ä¹Ÿå¯«å…¥ âœ“ï¼ˆç„¡æ³•è­˜åˆ¥ç‚ºé‡è¤‡ï¼‰
çµæœï¼šorders è¡¨æ ¼åŒ…å«å…©ç­† A001 è¨˜éŒ„
```

**éŒ¯èª¤ 2: åŒä¸€ batch çš„é‡è¤‡è¨˜éŒ„æœƒè¢«å»é‡**

```
é¸é … D èªªï¼š
"duplicates that arrive in the same batch may both be written"
ï¼ˆåŒä¸€æ‰¹æ¬¡çš„é‡è¤‡è¨˜éŒ„å¯èƒ½éƒ½è¢«å¯«å…¥ï¼‰

å¯¦éš›æƒ…æ³ï¼š
- dropDuplicates æœƒåœ¨åŒä¸€ batch å…§å»é‡
- åŒä¸€ batch çš„é‡è¤‡è¨˜éŒ„åªæœƒä¿ç•™ä¸€ç­†

ç¯„ä¾‹ï¼ˆåŒä¸€ batchï¼‰ï¼š
Batch 1 åŒ…å«ï¼š
- è¨‚å–® A001 (time: 10:00)
- è¨‚å–® A001 (time: 10:05) â† é‡è¤‡

dropDuplicates(["customer_id", "order_id"]) è™•ç†ï¼š
â†’ åªä¿ç•™ä¸€ç­†ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ç­†æˆ–æœ€å¾Œä¸€ç­†ï¼Œå–æ±ºæ–¼å¯¦ä½œï¼‰
â†’ ä¸æœƒã€Œéƒ½è¢«å¯«å…¥ã€
```

**é¸é … D vs é¸é … A çš„å°æ¯”ï¼š**

| æƒ…å¢ƒ | é¸é … Dï¼ˆéŒ¯èª¤ï¼‰ | é¸é … Aï¼ˆæ­£ç¢ºï¼‰ |
|------|---------------|---------------|
| è¶…é 2 å°æ™‚çš„é‡è¤‡ | è¢«ä¸Ÿæ£„ âŒ | è¢«ä¿ç•™ï¼ˆé€ æˆé‡è¤‡ï¼‰âœ“ |
| åŒä¸€ batch çš„é‡è¤‡ | éƒ½è¢«å¯«å…¥ âŒ | æœƒè¢«å»é‡ âœ“ |

---

### é¸é … E - The orders table will not contain duplicates, but records arriving more than 2 hours late will be ignored and missing from the table.

**éŒ¯èª¤åŸå› :** å…©å€‹æ–·è¨€éƒ½èˆ‡å¯¦éš›è¡Œç‚ºç›¸å

**è©³ç´°åˆ†æ:**

**éŒ¯èª¤ 1: è¡¨æ ¼ã€Œæœƒã€åŒ…å«é‡è¤‡è¨˜éŒ„ï¼ˆç•¶é–“éš” > 2 å°æ™‚ï¼‰**

```
é¸é … E èªªï¼š
"The orders table will not contain duplicates"
ï¼ˆè¡¨æ ¼ä¸æœƒåŒ…å«é‡è¤‡ï¼‰

å¯¦éš›æƒ…æ³ï¼š
- å¦‚é¸é … A æ‰€è¿°ï¼Œè¶…é 2 å°æ™‚çš„é‡è¤‡æœƒè¢«ä¿ç•™
- è¡¨æ ¼ã€Œå¯èƒ½ã€åŒ…å«é‡è¤‡è¨˜éŒ„

ç¯„ä¾‹ï¼š
10:00 â†’ è¨‚å–® A001 â†’ å¯«å…¥ orders
13:30 â†’ è¨‚å–® A001 â†’ å†æ¬¡å¯«å…¥ ordersï¼ˆstate å·²æ¸…é™¤ï¼‰

SELECT * FROM orders WHERE order_id = 'A001'
â†’ è¿”å› 2 ç­†è¨˜éŒ„ï¼ˆé‡è¤‡ï¼‰âŒ
```

**éŒ¯èª¤ 2: Late-arriving data ä¸æœƒè¢«å¿½ç•¥**

```
é¸é … E èªªï¼š
"records arriving more than 2 hours late will be ignored"
ï¼ˆè¶…é 2 å°æ™‚æ™šåˆ°çš„è¨˜éŒ„æœƒè¢«å¿½ç•¥ï¼‰

å¯¦éš›æƒ…æ³ï¼š
- Watermark ä¸æœƒã€Œéæ¿¾ã€æˆ–ã€Œå¿½ç•¥ã€æ™šåˆ°çš„è³‡æ–™
- æ™šåˆ°çš„è³‡æ–™ä»æœƒè¢«å¯«å…¥
- åªæ˜¯å»é‡ç‹€æ…‹å¯èƒ½å·²è¢«æ¸…é™¤

Late data è™•ç†ï¼š

Event Time | Arrival Time | è™•ç†çµæœ
-----------|-------------|----------
10:00      | 12:00 åˆ°é”  | âœ“ å¯«å…¥ï¼ˆé›–ç„¶æ™šäº† 2 å°æ™‚ï¼‰
10:00      | 14:00 åˆ°é”  | âœ“ ä¹Ÿå¯«å…¥ï¼ˆé›–ç„¶æ™šäº† 4 å°æ™‚ï¼‰

â†’ ä¸æœƒè¢«ã€Œå¿½ç•¥ã€æˆ–ã€Œmissingã€
â†’ åªæ˜¯å¯èƒ½é€ æˆé‡è¤‡
```

**Watermark èˆ‡ Late Data çš„é—œä¿‚ï¼š**

```
å¸¸è¦‹èª¤è§£ï¼š
"Watermark æœƒæ‹’çµ•æ™šåˆ°çš„è³‡æ–™"

å¯¦éš›è¡Œç‚ºï¼š
- Watermark åœ¨ windowing æ“ä½œä¸­æœƒå½±éŸ¿è¦–çª—çš„è¨ˆç®—
- ä½†åœ¨ dropDuplicates ä¸­ï¼Œåªå½±éŸ¿ state ä¿ç•™
- æ™šåˆ°çš„è³‡æ–™ä»æœƒè¢«è™•ç†å’Œå¯«å…¥

æƒ…å¢ƒå°æ¯”ï¼š

ä½¿ç”¨ Window Aggregation + Watermark:
- æ™šæ–¼ watermark çš„è³‡æ–™å¯èƒ½ä¸é€²å…¥å·²é—œé–‰çš„è¦–çª—

ä½¿ç”¨ Deduplication + Watermark:
- æ™šåˆ°çš„è³‡æ–™ä»æœƒè¢«è™•ç†
- åªæ˜¯å»é‡ç‹€æ…‹å¯èƒ½å·²è¢«æ¸…é™¤
```

**é¸é … E çš„å…©å€‹éŒ¯èª¤æ–·è¨€ï¼š**

| æ–·è¨€ | é¸é … Eï¼ˆéŒ¯èª¤ï¼‰ | å¯¦éš›æƒ…æ³ï¼ˆæ­£ç¢ºï¼‰ |
|------|---------------|-----------------|
| é‡è¤‡è¨˜éŒ„ | ä¸æœƒæœ‰é‡è¤‡ âŒ | å¯èƒ½æœ‰é‡è¤‡ï¼ˆé–“éš” > 2hï¼‰âœ“ |
| æ™šåˆ°è³‡æ–™ | æœƒè¢«å¿½ç•¥ âŒ | æœƒè¢«è™•ç†å’Œå¯«å…¥ âœ“ |

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### å£è¨£
**ã€ŒWatermark ç®¡ç‹€æ…‹ï¼Œä¸ç®¡è¼¸å…¥è¼¸å‡ºã€**

### Watermark åœ¨ Deduplication ä¸­çš„ä½œç”¨å°æ¯”è¡¨

| ç‰¹æ€§ | Watermark çš„ä½œç”¨ | Watermark ä¸åšçš„äº‹ |
|------|-----------------|-------------------|
| State Store | âœ“ æ§åˆ¶ç‹€æ…‹ä¿ç•™æ™‚é–“ | âœ— ä¸å»¶é²è™•ç† |
| è¼¸å…¥è³‡æ–™ | âœ— ä¸éæ¿¾è¼¸å…¥ | âœ— ä¸æ‹’çµ•æ™šåˆ°è³‡æ–™ |
| è¼¸å‡ºè³‡æ–™ | âœ— ä¸é™åˆ¶è¼¸å‡ºç¯„åœ | âœ— ä¸ä¿è­‰ç„¡é‡è¤‡ |
| è¨˜æ†¶é«”ç®¡ç† | âœ“ æ¸…ç†èˆŠç‹€æ…‹ | âœ— ä¸å½±éŸ¿ç•¶å‰ batch |
| å»é‡é‚è¼¯ | âœ“ æ±ºå®šã€Œè¨˜æ†¶ã€å¤šä¹… | âœ— ä¸å½±éŸ¿ batch å…§å»é‡ |

### å¿«é€Ÿåˆ¤æ–·æŠ€å·§

**è­˜åˆ¥é—œéµå­—ï¼š**
- âœ“ "more than 2 hours apart" â†’ è¶…é watermarkï¼Œstate è¢«æ¸…é™¤
- âœ“ "may be retained" â†’ å¯èƒ½æœ‰é‡è¤‡
- âœ— "will be dropped" â†’ Watermark ä¸æœƒä¸»å‹•ä¸Ÿæ£„è³‡æ–™
- âœ— "held for 2 hours" â†’ ä¸æ˜¯å»¶é²è™•ç†
- âœ— "only the most recent 2 hours" â†’ ä¸æ˜¯æ™‚é–“ç¯„åœéæ¿¾

**æ±ºç­–æ¨¹ï¼š**

```
å•é¡Œï¼šè¶…é watermark æ™‚é–“çš„é‡è¤‡è¨˜éŒ„æœƒå¦‚ä½•ï¼Ÿ

Step 1: State store é‚„ä¿æœ‰èˆŠè¨˜éŒ„çš„ç‹€æ…‹å—ï¼Ÿ
- é–“éš” < 2 hours â†’ Yes â†’ æˆåŠŸå»é‡ âœ“
- é–“éš” > 2 hours â†’ No â†’ ç¹¼çºŒ Step 2

Step 2: ç³»çµ±èƒ½è­˜åˆ¥é€™æ˜¯é‡è¤‡è¨˜éŒ„å—ï¼Ÿ
- State å·²æ¸…é™¤ â†’ No â†’ è¢«è¦–ç‚ºæ–°è¨˜éŒ„

Step 3: æ–°è¨˜éŒ„æœƒè¢«å¯«å…¥å—ï¼Ÿ
- Yes â†’ é€ æˆé‡è¤‡ âŒ

çµè«–ï¼šé¸é … A æ­£ç¢º
"Duplicate records enqueued more than 2 hours apart 
may be retained"
```

### è¦–è¦ºåŒ–è¨˜æ†¶åœ–

```
Streaming Deduplication + Watermark é‹ä½œæµç¨‹ï¼š

è¼¸å…¥è³‡æ–™æµ
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. è®€å– Parquet æª”æ¡ˆ            â”‚
â”‚    /mnt/raw_orders/             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. æ‡‰ç”¨ Watermark               â”‚
â”‚    withWatermark("time", "2h")  â”‚
â”‚    â†’ æ¨™è¨˜ state ä¿ç•™æ™‚é–“        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. æª¢æŸ¥ State Store             â”‚
â”‚    dropDuplicates([...])        â”‚
â”‚                                 â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚    â”‚ State Store             â”‚ â”‚
â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚    â”‚ â”‚ Order A: 10:00      â”‚ â”‚ â”‚
â”‚    â”‚ â”‚ Order B: 11:00      â”‚ â”‚ â”‚
â”‚    â”‚ â”‚ Order C: 12:00      â”‚ â”‚ â”‚
â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚    â”‚                         â”‚ â”‚
â”‚    â”‚ Watermark = 11:00       â”‚ â”‚
â”‚    â”‚ (æ¸…é™¤ < 11:00 çš„ç‹€æ…‹)   â”‚ â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. æ±ºç­–                         â”‚
â”‚                                 â”‚
â”‚  è¨˜éŒ„åœ¨ State? â†’ Yes â†’ å»é‡     â”‚
â”‚                â†’ No  â†’ å¯«å…¥     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. å¯«å…¥ orders è¡¨æ ¼             â”‚
â”‚    .table("orders")             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

é—œéµé»ï¼š
- State store çš„ã€Œè¨˜æ†¶ã€æœ‰æ™‚é–“é™åˆ¶ï¼ˆ2 hoursï¼‰
- è¶…éæ™‚é–“çš„é‡è¤‡è¨˜éŒ„ç„¡æ³•è¢«è­˜åˆ¥
- çµæœï¼šå¯èƒ½æœ‰é‡è¤‡ï¼ˆé¸é … Aï¼‰âœ“
```

### Watermark æ™‚é–“é¸æ“‡çš„å¯¦å‹™è€ƒé‡

```
é¸æ“‡ Watermark æ™‚é–“çš„æ¬Šè¡¡ï¼š

çŸ­ Watermark (ä¾‹å¦‚ï¼š30 åˆ†é˜)
å„ªé»ï¼š
- State store è¼ƒå°
- è¨˜æ†¶é«”æ¶ˆè€—ä½
- æ•ˆèƒ½è¼ƒå¥½

ç¼ºé»ï¼š
- é–“éš”è¼ƒé•·çš„é‡è¤‡è¨˜éŒ„ç„¡æ³•å»é‡
- å¯èƒ½æœ‰æ›´å¤šé‡è¤‡è³‡æ–™

é•· Watermark (ä¾‹å¦‚ï¼š24 å°æ™‚)
å„ªé»ï¼š
- å¯åµæ¸¬é–“éš”è¼ƒé•·çš„é‡è¤‡è¨˜éŒ„
- æ›´å®Œæ•´çš„å»é‡

ç¼ºé»ï¼š
- State store é¾å¤§
- è¨˜æ†¶é«”æ¶ˆè€—é«˜
- å¯èƒ½å½±éŸ¿æ•ˆèƒ½

æœ¬é¡Œè¨­å®šï¼š2 hours
- ä¸­ç­‰çš„æ¬Šè¡¡
- ä½†ç„¡æ³•è™•ç†é–“éš”è¶…é 2 å°æ™‚çš„é‡è¤‡
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

### å®˜æ–¹æ–‡ä»¶
- [Structured Streaming - Watermarking](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)
- [Structured Streaming - Deduplication](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#deduplication)
- [Databricks - Watermarks](https://docs.databricks.com/structured-streaming/watermarks.html)

### ç›¸é—œæ¦‚å¿µ
- State Store Management
- Late-arriving Data Handling
- Trigger Modes in Structured Streaming
- Event Time vs Processing Time

### å¯¦å‹™ç¯„ä¾‹

**1. å®Œæ•´çš„å»é‡å¯¦ä½œèˆ‡æ¸¬è©¦ï¼š**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, window
from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType

spark = SparkSession.builder.appName("DeduplicationTest").getOrCreate()

# æ¨¡æ“¬è³‡æ–™ï¼šåŒ…å«é‡è¤‡è¨˜éŒ„ï¼Œé–“éš”ä¸åŒ
test_data = [
    # Batch 1 (10:00)
    ("123", "A001", "2024-01-15 10:00:00", 100),
    ("124", "A002", "2024-01-15 10:15:00", 200),
    
    # Batch 2 (11:00) - 1 å°æ™‚å¾Œçš„é‡è¤‡
    ("123", "A001", "2024-01-15 11:00:00", 100),  # é‡è¤‡ï¼Œé–“éš” 1h
    ("125", "A003", "2024-01-15 11:00:00", 300),
    
    # Batch 3 (13:30) - 3.5 å°æ™‚å¾Œçš„é‡è¤‡
    ("123", "A001", "2024-01-15 13:30:00", 100),  # é‡è¤‡ï¼Œé–“éš” 3.5h
    ("126", "A004", "2024-01-15 13:30:00", 400),
]

schema = StructType([
    StructField("customer_id", StringType()),
    StructField("order_id", StringType()),
    StructField("time", StringType()),
    StructField("amount", IntegerType())
])

# å¯«å…¥æ¸¬è©¦è³‡æ–™åˆ° Parquet
from datetime import datetime
import time

for i, record in enumerate(test_data):
    batch_df = spark.createDataFrame([record], schema)
    batch_df = batch_df.withColumn("time", col("time").cast(TimestampType()))
    
    # æ¨¡æ“¬ä¸åŒæ™‚é–“çš„å¯«å…¥
    output_path = f"/tmp/raw_orders/batch_{i}"
    batch_df.write.mode("overwrite").parquet(output_path)
    
    print(f"Batch {i} written: {record}")
    time.sleep(1)  # æ¨¡æ“¬é–“éš”

# åŸ·è¡Œ Streaming Deduplication
query = (spark.readStream
    .format("parquet")
    .schema(schema)
    .load("/tmp/raw_orders/")
    .withColumn("time", col("time").cast(TimestampType()))
    .withWatermark("time", "2 hours")
    .dropDuplicates(["customer_id", "order_id"])
    .writeStream
    .format("console")
    .option("truncate", "false")
    .trigger(once=True)
    .outputMode("append")
    .start()
)

query.awaitTermination()

# æŸ¥è©¢çµæœ
result_df = spark.table("orders")
print("\næœ€çµ‚ orders è¡¨æ ¼å…§å®¹ï¼š")
result_df.orderBy("time").show(truncate=False)

# æª¢æŸ¥é‡è¤‡
duplicates = (result_df
    .groupBy("customer_id", "order_id")
    .count()
    .filter("count > 1")
)

print("\né‡è¤‡è¨˜éŒ„çµ±è¨ˆï¼š")
duplicates.show()

"""
é æœŸçµæœï¼š

orders è¡¨æ ¼æœƒåŒ…å«ï¼š
- 10:00 çš„ A001 âœ“ (ç¬¬ä¸€æ¬¡å‡ºç¾)
- 11:00 çš„ A001 âœ— (è¢«å»é‡ï¼Œstate ä»ä¿ç•™)
- 13:30 çš„ A001 âœ“ (å†æ¬¡å‡ºç¾ï¼Œstate å·²æ¸…é™¤) â† é€ æˆé‡è¤‡ï¼

é‡è¤‡è¨˜éŒ„çµ±è¨ˆæœƒé¡¯ç¤ºï¼š
customer_id | order_id | count
123         | A001     | 2      â† è­‰æ˜é¸é … A æ­£ç¢º
"""
```

**2. èª¿æ•´ Watermark çš„æ•ˆæœæ¯”è¼ƒï¼š**

```python
# æ¸¬è©¦ä¸åŒ Watermark è¨­å®šçš„å½±éŸ¿

def test_watermark(watermark_duration):
    """æ¸¬è©¦ä¸åŒ watermark çš„å»é‡æ•ˆæœ"""
    
    query = (spark.readStream
        .format("parquet")
        .load("/tmp/raw_orders/")
        .withWatermark("time", watermark_duration)  # â† è®Šæ•¸
        .dropDuplicates(["customer_id", "order_id"])
        .writeStream
        .format("memory")
        .queryName(f"orders_{watermark_duration.replace(' ', '_')}")
        .trigger(once=True)
        .start()
    )
    
    query.awaitTermination()
    
    result = spark.sql(f"SELECT * FROM orders_{watermark_duration.replace(' ', '_')}")
    duplicate_count = (result
        .groupBy("customer_id", "order_id")
        .count()
        .filter("count > 1")
        .count()
    )
    
    print(f"\nWatermark: {watermark_duration}")
    print(f"  ç¸½è¨˜éŒ„æ•¸: {result.count()}")
    print(f"  é‡è¤‡çµ„æ•¸: {duplicate_count}")
    
    return result

# æ¸¬è©¦ä¸åŒçš„ watermark
result_30min = test_watermark("30 minutes")
result_2hours = test_watermark("2 hours")  # åŸé¡Œè¨­å®š
result_6hours = test_watermark("6 hours")

"""
é æœŸçµæœï¼š

Watermark: 30 minutes
  ç¸½è¨˜éŒ„æ•¸: 6 (æ›´å¤šé‡è¤‡ç„¡æ³•è¢«å»é™¤)
  é‡è¤‡çµ„æ•¸: 2

Watermark: 2 hours
  ç¸½è¨˜éŒ„æ•¸: 5 (é–“éš” 3.5h çš„é‡è¤‡ä¿ç•™)
  é‡è¤‡çµ„æ•¸: 1

Watermark: 6 hours
  ç¸½è¨˜éŒ„æ•¸: 4 (æ‰€æœ‰é‡è¤‡éƒ½è¢«å»é™¤)
  é‡è¤‡çµ„æ•¸: 0

çµè«–ï¼š
- Watermark è¶Šé•·ï¼Œå»é‡æ•ˆæœè¶Šå¥½
- ä½† state store æœƒæ›´å¤§ï¼Œæ¶ˆè€—æ›´å¤šè¨˜æ†¶é«”
"""
```

**3. ç›£æ§ State Store å¤§å°ï¼š**

```python
# ç›£æ§ state store çš„å¢é•·

from pyspark.sql.streaming import StreamingQuery

def monitor_state_store(query: StreamingQuery):
    """ç›£æ§ streaming query çš„ state store"""
    
    # å–å¾— query çš„é€²åº¦
    progress = query.lastProgress
    
    if progress:
        print("\nState Store Metrics:")
        print(f"  Number of State Stores: {progress['numStateStores']}")
        
        if 'stateOperators' in progress:
            for i, state_op in enumerate(progress['stateOperators']):
                print(f"\n  State Operator {i}:")
                print(f"    Num Row Updated: {state_op.get('numRowsUpdated', 0)}")
                print(f"    Num Row Removed: {state_op.get('numRowsRemoved', 0)}")
                print(f"    Memory Used (bytes): {state_op.get('memoryUsedBytes', 0)}")
                
                # ä¼°ç®— state æ•¸é‡
                custom_metrics = state_op.get('customMetrics', {})
                if 'loadedMapCacheHitCount' in custom_metrics:
                    print(f"    State Entries: ~{custom_metrics['loadedMapCacheHitCount']}")

# ä½¿ç”¨
query = (spark.readStream
    .format("parquet")
    .load("/tmp/raw_orders/")
    .withWatermark("time", "2 hours")
    .dropDuplicates(["customer_id", "order_id"])
    .writeStream
    .format("console")
    .trigger(processingTime="1 minute")
    .start()
)

# å®šæœŸç›£æ§
import time
for i in range(5):
    time.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
    monitor_state_store(query)

query.stop()
```

**4. å¯¦å‹™ä¸­çš„å®Œæ•´è§£æ±ºæ–¹æ¡ˆï¼š**

```python
# ç”Ÿç”¢ç’°å¢ƒçš„æœ€ä½³å¯¦è¸

from pyspark.sql.functions import col, current_timestamp, sha2, concat_ws

# æ–¹æ¡ˆ 1: ä½¿ç”¨è¤‡åˆ Watermark ç­–ç•¥
def robust_deduplication():
    """æ›´ç©©å¥çš„å»é‡ç­–ç•¥"""
    
    # å¢åŠ å”¯ä¸€æ€§æª¢æŸ¥æ¬„ä½
    df = (spark.readStream
        .format("parquet")
        .load("/mnt/raw_orders/")
        .withColumn(
            "dedup_key",  # å»ºç«‹è¤‡åˆéµçš„ hash
            sha2(concat_ws("_", col("customer_id"), col("order_id")), 256)
        )
    )
    
    # æ ¹æ“šæ¥­å‹™éœ€æ±‚èª¿æ•´ watermark
    watermark_duration = "24 hours"  # å»¶é•·åˆ° 24 å°æ™‚ä»¥æ•æ‰æ›´å¤šé‡è¤‡
    
    deduplicated = (df
        .withWatermark("time", watermark_duration)
        .dropDuplicates(["dedup_key"])  # ä½¿ç”¨ hash key å»é‡
    )
    
    query = (deduplicated
        .writeStream
        .format("delta")  # ä½¿ç”¨ Delta Lake
        .option("checkpointLocation", "/mnt/checkpoints/orders_dedup")
        .trigger(once=True)
        .table("orders")
    )
    
    return query

# æ–¹æ¡ˆ 2: åˆ†å±¤å»é‡ç­–ç•¥
def tiered_deduplication():
    """åˆ†å±¤å»é‡ï¼šè¿‘æœŸç”¨ streamingï¼Œé æœŸç”¨ batch"""
    
    # Streaming è™•ç†æœ€è¿‘ 24 å°æ™‚
    streaming_df = (spark.readStream
        .format("parquet")
        .load("/mnt/raw_orders/")
        .withWatermark("time", "24 hours")
        .dropDuplicates(["customer_id", "order_id"])
    )
    
    streaming_df.writeStream.table("orders_staging")
    
    # Batch è™•ç†æ­·å²è³‡æ–™å»é‡
    from pyspark.sql import Window
    from pyspark.sql.functions import row_number
    
    window_spec = Window.partitionBy("customer_id", "order_id").orderBy("time")
    
    historical_deduplicated = (spark.table("orders_staging")
        .withColumn("row_num", row_number().over(window_spec))
        .filter("row_num = 1")
        .drop("row_num")
    )
    
    historical_deduplicated.write.mode("overwrite").table("orders_final")

# æ–¹æ¡ˆ 3: ä½¿ç”¨ Delta Lake çš„ MERGE
def delta_merge_deduplication():
    """ä½¿ç”¨ Delta MERGE å¯¦ç¾ç²¾ç¢ºå»é‡"""
    
    # Streaming è®€å–
    new_orders = (spark.readStream
        .format("parquet")
        .load("/mnt/raw_orders/")
    )
    
    # ä½¿ç”¨ foreachBatch åŸ·è¡Œ MERGE
    def merge_batch(batch_df, batch_id):
        batch_df.createOrReplaceTempView("new_orders")
        
        spark.sql("""
            MERGE INTO orders AS target
            USING new_orders AS source
            ON target.customer_id = source.customer_id 
               AND target.order_id = source.order_id
            WHEN NOT MATCHED THEN INSERT *
        """)
        
        print(f"Batch {batch_id} processed")
    
    query = (new_orders.writeStream
        .foreachBatch(merge_batch)
        .option("checkpointLocation", "/mnt/checkpoints/orders_merge")
        .trigger(once=True)
        .start()
    )
    
    return query

"""
æ–¹æ¡ˆæ¯”è¼ƒï¼š

æ–¹æ¡ˆ 1 (å»¶é•· Watermark):
å„ªé»ï¼šç°¡å–®ï¼Œæ•æ‰æ›´å¤šé‡è¤‡
ç¼ºé»ï¼šState store é¾å¤§ï¼Œè¨˜æ†¶é«”æ¶ˆè€—é«˜

æ–¹æ¡ˆ 2 (åˆ†å±¤å»é‡):
å„ªé»ï¼šå¹³è¡¡æ•ˆèƒ½èˆ‡æº–ç¢ºæ€§
ç¼ºé»ï¼šéœ€è¦é¡å¤–çš„ batch job

æ–¹æ¡ˆ 3 (Delta MERGE):
å„ªé»ï¼šå®Œå…¨å»é‡ï¼Œç„¡ watermark é™åˆ¶
ç¼ºé»ï¼šæ•ˆèƒ½è¼ƒæ…¢ï¼Œéœ€è¦ Delta Lake
"""
```

**5. è¨ºæ–·é‡è¤‡å•é¡Œï¼š**

```python
# æª¢æ¸¬å’Œåˆ†æé‡è¤‡è¨˜éŒ„

def analyze_duplicates(table_name="orders"):
    """åˆ†æè¡¨æ ¼ä¸­çš„é‡è¤‡è¨˜éŒ„"""
    
    df = spark.table(table_name)
    
    # æ‰¾å‡ºé‡è¤‡è¨˜éŒ„
    duplicates = (df
        .groupBy("customer_id", "order_id")
        .agg(
            count("*").alias("count"),
            min("time").alias("first_seen"),
            max("time").alias("last_seen")
        )
        .filter("count > 1")
    )
    
    print(f"\né‡è¤‡è¨˜éŒ„åˆ†æ ({table_name}):")
    print(f"  é‡è¤‡çµ„æ•¸: {duplicates.count()}")
    
    # åˆ†æé‡è¤‡é–“éš”
    from pyspark.sql.functions import unix_timestamp, expr
    
    duplicates_with_interval = duplicates.withColumn(
        "interval_hours",
        (unix_timestamp("last_seen") - unix_timestamp("first_seen")) / 3600
    )
    
    print("\né‡è¤‡é–“éš”åˆ†å¸ƒ:")
    duplicates_with_interval.groupBy(
        expr("CASE " +
             "WHEN interval_hours < 1 THEN '< 1 hour' " +
             "WHEN interval_hours < 2 THEN '1-2 hours' " +
             "WHEN interval_hours < 6 THEN '2-6 hours' " +
             "WHEN interval_hours < 24 THEN '6-24 hours' " +
             "ELSE '> 24 hours' END AS interval_bucket")
    ).count().orderBy("interval_bucket").show()
    
    # é¡¯ç¤ºç¯„ä¾‹é‡è¤‡è¨˜éŒ„
    print("\né‡è¤‡è¨˜éŒ„ç¯„ä¾‹:")
    duplicate_keys = duplicates.select("customer_id", "order_id").limit(5)
    
    for row in duplicate_keys.collect():
        print(f"\nCustomer: {row.customer_id}, Order: {row.order_id}")
        (df.filter(
            (col("customer_id") == row.customer_id) &
            (col("order_id") == row.order_id)
        )
        .orderBy("time")
        .show(truncate=False))

# ä½¿ç”¨
analyze_duplicates("orders")

"""
ç¯„ä¾‹è¼¸å‡ºï¼š

é‡è¤‡è¨˜éŒ„åˆ†æ (orders):
  é‡è¤‡çµ„æ•¸: 5

é‡è¤‡é–“éš”åˆ†å¸ƒ:
+--------------+-----+
|interval_bucket|count|
+--------------+-----+
|< 1 hour       |    0|
|1-2 hours      |    2|
|2-6 hours      |    3| â† é€™äº›æ˜¯ watermark ç„¡æ³•è™•ç†çš„
|6-24 hours     |    0|
|> 24 hours     |    0|
+--------------+-----+

â†’ çµè«–ï¼šå¤šæ•¸é‡è¤‡é–“éš”åœ¨ 2-6 å°æ™‚ï¼Œè¶…é 2 å°æ™‚çš„ watermark
â†’ é©—è­‰é¸é … A çš„æ­£ç¢ºæ€§
"""
```

---

## ğŸ”— ç›¸é—œé¡Œç›®
- Q-XXX: Structured Streaming Trigger Modes
- Q-XXX: State Store Management
- Q-XXX: Window Functions in Streaming
- Q-XXX: Late-arriving Data Handling
