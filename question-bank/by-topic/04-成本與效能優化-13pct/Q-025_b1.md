# Question #025

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-025`

### ä¾†æº
**ä¾†æº:** Sample / Batch 1

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
A data engineering team is running a complex analytical query on a large dataset, but they notice that the query execution results in a significant disk spill, which drastically slows down performance.

Which of the following approaches would NOT be an effective solution to minimize this issue?

### é¸é …
- **A.** Reduce the size of Spark partitions

- **B.** Increase memory size

- **C.** Increase the number of CPU cores

- **D.** Increase the number of shuffle partitions

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Spark-Performance`

### Trap Tags (é™·é˜±é¡žåž‹æ¨™ç±¤)
**Traps:** `Logical-Trap`, `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Performance Optimization

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** C
- **ç¤¾ç¾¤å…±è­˜:** C

---

# é¡Œç›®è§£æž

---

## ðŸ“ è€ƒé»žè­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Spark Disk Spill å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

**é—œéµæ¦‚å¿µ:**
- Disk Spill çš„æˆå› 
- è¨˜æ†¶é«”èˆ‡åˆ†å€çš„é—œä¿‚
- CPU æ ¸å¿ƒæ•¸èˆ‡è¨˜æ†¶é«”çš„ç¨ç«‹æ€§

**é¡Œç›®é—œéµå­—ï¼š**
- **disk spill**: ç£ç¢Ÿæº¢å‡ºï¼Œä¸­é–“è³‡æ–™å¯«å…¥ç£ç¢Ÿ
- **NOT be an effective solution**: åå‘é¡Œï¼Œæ‰¾ç„¡æ•ˆçš„è§£æ±ºæ–¹æ¡ˆ
- **memory**: è¨˜æ†¶é«”
- **CPU cores**: CPU æ ¸å¿ƒæ•¸

---

## âœ… æ­£è§£èªªæ˜Ž

### ç‚ºä»€éº¼ C æ˜¯æ­£ç¢ºç­”æ¡ˆï¼ˆç„¡æ•ˆè§£æ±ºæ–¹æ¡ˆï¼‰ï¼Ÿ

**æŠ€è¡“åŽŸç†ï¼š**

Disk Spill ç™¼ç”Ÿåœ¨**å–®ä¸€ Task è™•ç†çš„è³‡æ–™é‡è¶…éŽå¯ç”¨è¨˜æ†¶é«”**æ™‚ã€‚å¢žåŠ  CPU æ ¸å¿ƒæ•¸åªæœƒå¢žåŠ **å¹³è¡ŒåŸ·è¡Œçš„ Task æ•¸é‡**ï¼Œä½†**ä¸æœƒæ¸›å°‘æ¯å€‹ Task éœ€è¦çš„è¨˜æ†¶é«”**ã€‚

#### Disk Spill çš„æˆå› 

```
Task åŸ·è¡ŒéŽç¨‹ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Task 1 è™•ç† 500MB è³‡æ–™                   â”‚
â”‚ â”œâ”€â”€ å¯ç”¨è¨˜æ†¶é«”ï¼š300MB                    â”‚
â”‚ â””â”€â”€ è¶…å‡ºéƒ¨åˆ†ï¼š200MB â†’ å¯«å…¥ç£ç¢Ÿï¼ˆSpillï¼‰  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å¢žåŠ  CPU æ ¸å¿ƒæ•¸çš„æ•ˆæžœ

```
å¢žåŠ  CPU æ ¸å¿ƒå‰ï¼ˆ4 coresï¼‰ï¼š
â”œâ”€â”€ Task 1: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 2: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 3: 500MB è³‡æ–™ï¼ŒSpill 200MB
â””â”€â”€ Task 4: 500MB è³‡æ–™ï¼ŒSpill 200MB

å¢žåŠ  CPU æ ¸å¿ƒå¾Œï¼ˆ8 coresï¼‰ï¼š
â”œâ”€â”€ Task 1: 500MB è³‡æ–™ï¼ŒSpill 200MB  â† é‚„æ˜¯ Spillï¼
â”œâ”€â”€ Task 2: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 3: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 4: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 5: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 6: 500MB è³‡æ–™ï¼ŒSpill 200MB
â”œâ”€â”€ Task 7: 500MB è³‡æ–™ï¼ŒSpill 200MB
â””â”€â”€ Task 8: 500MB è³‡æ–™ï¼ŒSpill 200MB

çµè«–ï¼šæ›´å¤š Task åŒæ™‚åŸ·è¡Œï¼Œä½†æ¯å€‹ Task çš„ Spill å•é¡Œæ²’è§£æ±ºï¼
ç”šè‡³å¯èƒ½æ›´ç³Ÿï¼šæ›´å¤š Task ç«¶çˆ­ç›¸åŒçš„è¨˜æ†¶é«”è³‡æºã€‚
```

---

## âŒ ç‚ºä»€éº¼å…¶ä»–é¸é …æ˜¯æœ‰æ•ˆè§£æ±ºæ–¹æ¡ˆ

### é¸é … A âœ…
**Reduce the size of Spark partitions**

**ç‚ºä»€éº¼æœ‰æ•ˆï¼Ÿ**

1. **æ›´å°çš„ partition = æ¯å€‹ Task è™•ç†æ›´å°‘è³‡æ–™**ï¼š
```python
# å¢žåŠ  partition æ•¸é‡ï¼Œæ¸›å°‘æ¯å€‹ partition çš„å¤§å°
df.repartition(1000)  # åŽŸæœ¬ 100 å€‹ partitionï¼Œç¾åœ¨ 1000 å€‹

# æ¯å€‹ Task è™•ç†çš„è³‡æ–™é‡æ¸›å°‘
# 500MB / 100 partitions = 5MB per task â†’ å¯èƒ½ Spill
# 500MB / 1000 partitions = 0.5MB per task â†’ ä¸æœƒ Spill
```

2. **é©ç”¨å ´æ™¯**ï¼š
   - è³‡æ–™å‚¾æ–œï¼ˆæŸäº› partition ç‰¹åˆ¥å¤§ï¼‰
   - è¨˜æ†¶é«”å›ºå®šï¼Œç„¡æ³•å¢žåŠ 

---

### é¸é … B âœ…
**Increase memory size**

**ç‚ºä»€éº¼æœ‰æ•ˆï¼Ÿ**

1. **æ›´å¤šè¨˜æ†¶é«” = æ¯å€‹ Task å¯ç”¨è¨˜æ†¶é«”å¢žåŠ **ï¼š
```
å¢žåŠ è¨˜æ†¶é«”å‰ï¼š
â”œâ”€â”€ Task è™•ç† 500MB è³‡æ–™
â”œâ”€â”€ å¯ç”¨è¨˜æ†¶é«”ï¼š300MB
â””â”€â”€ Spillï¼š200MB

å¢žåŠ è¨˜æ†¶é«”å¾Œï¼š
â”œâ”€â”€ Task è™•ç† 500MB è³‡æ–™
â”œâ”€â”€ å¯ç”¨è¨˜æ†¶é«”ï¼š600MB
â””â”€â”€ Spillï¼š0MB âœ…
```

2. **é…ç½®æ–¹å¼**ï¼š
```python
# Cluster é…ç½®æ™‚é¸æ“‡æ›´å¤§è¨˜æ†¶é«”çš„ instance type
# æˆ–èª¿æ•´ Spark memory é…ç½®
spark.conf.set("spark.executor.memory", "8g")
```

---

### é¸é … D âœ…
**Increase the number of shuffle partitions**

**ç‚ºä»€éº¼æœ‰æ•ˆï¼Ÿ**

1. **Shuffle æ“ä½œæ˜¯ Spill çš„å¸¸è¦‹ä¾†æº**ï¼š
```python
# é è¨­ shuffle partitions = 200
spark.conf.set("spark.sql.shuffle.partitions", 200)

# å¦‚æžœè³‡æ–™é‡å¤§ï¼Œæ¯å€‹ partition å¯èƒ½å¾ˆå¤§
# 10GB è³‡æ–™ / 200 partitions = 50MB per partition

# å¢žåŠ  shuffle partitions
spark.conf.set("spark.sql.shuffle.partitions", 1000)
# 10GB è³‡æ–™ / 1000 partitions = 10MB per partition â†’ æ¸›å°‘ Spill
```

2. **é©ç”¨å ´æ™¯**ï¼š
   - JOIN æ“ä½œç”¢ç”Ÿå¤§é‡ shuffle
   - GROUP BY æ“ä½œ
   - ä»»ä½•æ¶‰åŠè³‡æ–™é‡æ–°åˆ†ä½ˆçš„æ“ä½œ

---

## ðŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€ŒSpill æ‰¾è¨˜æ†¶é«”ï¼ŒCPU ç®¡å¹³è¡Œåº¦ã€**
â†’ Disk Spill æ˜¯è¨˜æ†¶é«”å•é¡Œï¼Œå¢žåŠ  CPU æ ¸å¿ƒåªå¢žåŠ å¹³è¡Œåº¦ï¼Œä¸è§£æ±ºè¨˜æ†¶é«”ä¸è¶³

### Disk Spill è§£æ±ºæ–¹æ¡ˆç¸½çµ

| æ–¹æ¡ˆ | åŽŸç† | æœ‰æ•ˆæ€§ |
|------|------|--------|
| **æ¸›å°‘ partition å¤§å°** | æ¯å€‹ Task è™•ç†æ›´å°‘è³‡æ–™ | âœ… |
| **å¢žåŠ è¨˜æ†¶é«”** | æ¯å€‹ Task æœ‰æ›´å¤šè¨˜æ†¶é«”å¯ç”¨ | âœ… |
| **å¢žåŠ  shuffle partitions** | Shuffle å¾Œæ¯å€‹ partition æ›´å° | âœ… |
| **å¢žåŠ  CPU æ ¸å¿ƒ** | åªå¢žåŠ å¹³è¡Œåº¦ï¼Œä¸æ¸›å°‘å–® Task è¨˜æ†¶é«”éœ€æ±‚ | âŒ |

### Spark è¨˜æ†¶é«”æž¶æ§‹

```
Executor Memory åˆ†é…ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reserved Memory (300MB fixed)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User Memory (ç¨‹å¼ç¢¼ã€UDF ç­‰)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Execution Memory (Shuffle, Sort)    â”‚ â† Spill ä¾†æº
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Storage Memory (Cache, Broadcast)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç•¶ Execution Memory ä¸è¶³æ™‚ â†’ Disk Spill
```

### è¨ºæ–· Disk Spill

```python
# åœ¨ Spark UI ä¸­æŸ¥çœ‹ï¼š
# Stages Tab â†’ é¸æ“‡ Stage â†’ Task Metrics
# æŸ¥çœ‹ "Spill (Memory)" å’Œ "Spill (Disk)" æ¬„ä½

# æˆ–ä½¿ç”¨ SQL æŸ¥è©¢æ­·å²
SELECT * FROM spark_catalog.default.query_history
WHERE spill_to_disk_bytes > 0;
```

---

## ðŸ“š å®˜æ–¹æ–‡ä»¶

- [Spark Memory Management](https://spark.apache.org/docs/latest/tuning.html#memory-management-overview)
- [Debugging Disk Spill](https://docs.databricks.com/optimizations/disk-spill.html)
- [Spark Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

---

**[è¿”å›žé¡Œç›®](#question-025)**
