# Question #022

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-022`

### ä¾†æº
**ä¾†æº:** Community

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L1-Basic`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A data engineer wants to optimize the following join operation by allowing the smaller DataFrame to be sent to all executor nodes in the cluster:

```python
largeDF.join(smallerDF, ["key"], "inner")
```

Which of the following functions can be used to mark a DataFrame as small enough to fit in memory on all executors?

### é¸é …

- **A.** `pyspark.sql.functions.distribute`
- **B.** `pyspark.sql.functions.broadcast`
- **C.** `pyspark.sql.functions.explode`
- **D.** `pyspark.sql.functions.shuffle`

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Spark-Performance`, `Spark-Joins`, `PySpark`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Similar-Function`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Performance Optimization

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** B
- **ç¤¾ç¾¤å…±è­˜:** B

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Spark / Broadcast Join
**é—œéµæ¦‚å¿µ:** broadcast(), BroadcastHashJoin, Shuffle avoidance

**é¡Œç›®é—œéµå­—ï¼š**
- **smaller DataFrame**: è¼ƒå°çš„ DataFrameï¼Œé©åˆ broadcastã€‚
- **sent to all executor nodes**: å»£æ’­åˆ°æ‰€æœ‰ç¯€é»ã€‚
- **optimize join**: å„ªåŒ– Join æ•ˆèƒ½ã€‚

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ B æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

1. **Broadcast Join çš„æ¦‚å¿µ**:
   - å°‡è¼ƒå°çš„ DataFrame å®Œæ•´è¤‡è£½åˆ°æ¯å€‹ executor
   - é¿å…å¤§è¦æ¨¡çš„ shuffle æ“ä½œ
   - å¤§å¹…æå‡ Join æ•ˆèƒ½

2. **ä½¿ç”¨æ–¹å¼**:
   ```python
   from pyspark.sql.functions import broadcast

   result = largeDF.join(broadcast(smallerDF), ["key"], "inner")
   ```

3. **æ•ˆèƒ½æå‡åŸç†**:
   ```
   å‚³çµ± Shuffle Join:
   largeDF (partition) â”€â”€shuffleâ”€â”€â”
                                   â”œâ”€â”€ Join
   smallerDF (partition) â”€shuffleâ”€â”˜

   Broadcast Join:
   largeDF (partition) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€ Join (local)
   smallerDF (broadcast to all) â”€â”€â”€â”˜
   ```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A - distribute

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **å‡½æ•¸ä¸å­˜åœ¨**: PySpark ä¸­æ²’æœ‰ `pyspark.sql.functions.distribute`ã€‚

### C - explode

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ç”¨é€”ä¸åŒ**: `explode` ç”¨æ–¼å°‡é™£åˆ—æ¬„ä½å±•é–‹æˆå¤šè¡Œã€‚
- **èˆ‡ Join å„ªåŒ–ç„¡é—œ**: ä¸æ˜¯ç”¨æ–¼ Join å„ªåŒ–çš„å‡½æ•¸ã€‚

### D - shuffle

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **å‡½æ•¸ä¸å­˜åœ¨**: PySpark ä¸­æ²’æœ‰ `pyspark.sql.functions.shuffle`ã€‚
- **é‚è¼¯ç›¸å**: broadcast çš„ç›®çš„æ˜¯é¿å… shuffleã€‚

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€Œå°è¡¨ broadcastï¼Œå¤§è¡¨ä¸å‹•å½ˆã€**

### Broadcast Join ä½¿ç”¨æ¢ä»¶

| æ¢ä»¶ | èªªæ˜ |
|------|------|
| å°è¡¨å¤§å° | å»ºè­° < 10MBï¼ˆå¯èª¿æ•´ï¼‰ |
| è¨˜æ†¶é«” | éœ€è¦èƒ½æ”¾é€²æ¯å€‹ executor |
| è¨­å®š | `spark.sql.autoBroadcastJoinThreshold` |

### Join é¡å‹æ¯”è¼ƒ

| é¡å‹ | é©ç”¨å ´æ™¯ | Shuffle |
|------|----------|---------|
| **Broadcast Hash Join** | ä¸€æ–¹å¾ˆå° | **ç„¡** |
| Shuffle Hash Join | ä¸­ç­‰å¤§å° | æœ‰ |
| Sort Merge Join | å…©æ–¹éƒ½å¤§ | æœ‰ |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Broadcast Join](https://spark.apache.org/docs/latest/sql-performance-tuning.html#broadcast-hint-for-sql-queries)
- [broadcast function](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html)

---

**[è¿”å›é¡Œç›®](#question-022)**
