# Question #17

---

## 題目資訊

### 題目編號
**ID:** `Q-01-017`

### 來源
**來源:** Real Exam Recall

### 難度等級
**難度:** `L3-Advanced`

---

## 題目內容

### 題幹

A production workload incrementally applies updates from an external Change Data Capture feed to a Delta Lake table as an always-on Structured Stream job.

When data was initially migrated for this table, OPTIMIZE was executed and most data files were resized to 1 GB.

Auto Optimize and Auto Compaction were both turned on for the streaming production job.

Recent review of data files shows that most data files are under 64 MB, although each partition in the table contains at least 1 GB of data and the total table size is over 10 TB.

Which of the following likely explains these smaller file sizes?

### 選項

- **A.** Databricks has autotuned to a smaller target file size to reduce duration of MERGE operations
- **B.** Z-order indices calculated on the table are preventing file compaction
- **C.** Bloom filter indices calculated on the table are preventing file compaction
- **D.** Databricks has autotuned to a smaller target file size based on the overall size of data in the table
- **E.** Databricks has autotuned to a smaller target file size based on the amount of data in each partition

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Auto-Optimize`, `Auto-Compaction`, `File-Sizing`, `Streaming`, `MERGE`

### Trap Tags (陷阱類型標籤)
**Traps:** `Auto-Tuning-Logic`, `File-Size-Factors`

### Knowledge Domain (知識領域)
**Domain:** `Delta Lake`

---

## 答案與解析連結

### 正確答案
**正解:** `A`

### 解析檔案
**詳細解析:** [點此查看解析](../analysis/Q-01-017-analysis.md)

---

## 相關資源

### 官方文件
- [Auto Optimize](https://docs.databricks.com/delta/optimizations/auto-optimize.html)
- [Auto Compaction](https://docs.databricks.com/delta/optimizations/auto-optimize.html#auto-compaction)
- [MERGE Performance](https://docs.databricks.com/delta/merge.html#performance-tuning)
