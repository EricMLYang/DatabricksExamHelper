# Question #052

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-052`
### ä¾†æº
**ä¾†æº:** Community
### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
<!-- âš ï¸ ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è­¯ -->
A data engineer at a retail company is tasked with analyzing daily sales trends across hundreds of stores. The raw PySpark DataFrame contains columns store_id, date, and sales_amount. The engineer wants to calculate a 7-day rolling average of sales per store, leveraging Pandas for its convenient rolling window operations. To achieve this task, the engineer created a custom Python function called "calculate_sales" where the input and output of the function are both pandas.DataFrame.

Which of the following allows applying this function to each group of store_id while preserving Spark efficiency?

### é¸é …
<!-- âš ï¸ ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è­¯ -->
- **A.** pandas_udf(calculate_sales, returnType=Series)
- **B.** df.groupBy("store_id").applyInPandas(calculate_sales, schema)
- **C.** df.selectExpr("calculate_sales(store_id)")
- **D.** df.mapInPandas("store_id", calculate_sales, schema)

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `PySpark`, `Spark-UDF`, `DataFrames`
### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Similar-Function`
### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Data Transformation

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B`
### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** B
- **ç¤¾ç¾¤å…±è­˜:** B

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥
**æ ¸å¿ƒæŠ€è¡“:** PySpark applyInPandas
**é—œéµæ¦‚å¿µ:** GroupedDataã€Pandas æ•´åˆã€åˆ†çµ„ DataFrame è™•ç†
**é¡Œç›®é—œéµå­—ï¼š**
- **rolling window**: æ»¾å‹•è¦–çª—è¨ˆç®—
- **input and output are both pandas.DataFrame**: å‡½æ•¸æ¥æ”¶å’Œè¿”å›éƒ½æ˜¯ Pandas DataFrame
- **each group of store_id**: å°æ¯å€‹åˆ†çµ„å¥—ç”¨å‡½æ•¸

---

## âœ… æ­£è§£èªªæ˜
### ç‚ºä»€éº¼ B æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

`applyInPandas` æ˜¯å°ˆé–€è¨­è¨ˆç”¨æ–¼ã€Œå°æ¯å€‹åˆ†çµ„å¥—ç”¨ Pandas DataFrame å‡½æ•¸ã€çš„ APIã€‚

#### ä½¿ç”¨æƒ…å¢ƒå°ç…§
| éœ€æ±‚ | è§£æ±ºæ–¹æ¡ˆ |
|------|---------|
| å°æ¯å€‹åˆ†çµ„å¥—ç”¨ Pandas DataFrame å‡½æ•¸ | `groupBy().applyInPandas()` |
| å°æ•´å€‹ DataFrame å¥—ç”¨ Pandas å‡½æ•¸ | `mapInPandas()` |
| å°å–®ä¸€æ¬„ä½å¥—ç”¨ç´”é‡å‡½æ•¸ | `pandas_udf` |

#### å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹
```python
from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType

# å®šç¾© schemaï¼ˆè¼¸å‡ºçš„ DataFrame çµæ§‹ï¼‰
output_schema = StructType([
    StructField("store_id", StringType()),
    StructField("date", DateType()),
    StructField("sales_amount", DoubleType()),
    StructField("rolling_avg", DoubleType())
])

# å®šç¾© Pandas å‡½æ•¸
def calculate_sales(pdf: pd.DataFrame) -> pd.DataFrame:
    # pdf æ˜¯è©² store_id çš„æ‰€æœ‰è³‡æ–™ï¼ˆPandas DataFrameï¼‰
    pdf = pdf.sort_values("date")
    pdf["rolling_avg"] = pdf["sales_amount"].rolling(window=7).mean()
    return pdf

# å¥—ç”¨åˆ°æ¯å€‹ store_id åˆ†çµ„
result = (df
    .groupBy("store_id")
    .applyInPandas(calculate_sales, schema=output_schema)
)
```

#### applyInPandas åŸ·è¡Œæµç¨‹
```
Spark DataFrame
      â†“
groupBy("store_id")
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ åˆ†çµ„ 1 (store_id='A')                    â”‚
â”‚   â†’ è½‰æ›ç‚º Pandas DataFrame             â”‚
â”‚   â†’ åŸ·è¡Œ calculate_sales(pdf)           â”‚
â”‚   â†’ è¿”å› Pandas DataFrame               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åˆ†çµ„ 2 (store_id='B')                    â”‚
â”‚   â†’ è½‰æ›ç‚º Pandas DataFrame             â”‚
â”‚   â†’ åŸ·è¡Œ calculate_sales(pdf)           â”‚
â”‚   â†’ è¿”å› Pandas DataFrame               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ... (ä¸¦è¡Œè™•ç†æ‰€æœ‰åˆ†çµ„)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“
åˆä½µç‚º Spark DataFrame
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A. pandas_udf(calculate_sales, returnType=Series)
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- `pandas_udf` ç”¨æ–¼å»ºç«‹ Pandas UDFï¼Œè™•ç†çš„æ˜¯ **Series**ï¼ˆå–®ä¸€æ¬„ä½ï¼‰
- é¡Œç›®èªªå‡½æ•¸çš„è¼¸å…¥è¼¸å‡ºæ˜¯ **DataFrame**ï¼Œä¸æ˜¯ Series
- `pandas_udf` ä¸æ”¯æ´ DataFrame è¼¸å…¥è¼¸å‡º

### C. df.selectExpr("calculate_sales(store_id)")
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- `selectExpr` ç”¨æ–¼ SQL è¡¨é”å¼
- `calculate_sales` æ˜¯ Python å‡½æ•¸ï¼Œä¸æ˜¯ SQL å‡½æ•¸
- ç„¡æ³•åœ¨ SQL è¡¨é”å¼ä¸­ç›´æ¥å‘¼å« Python å‡½æ•¸

### D. df.mapInPandas("store_id", calculate_sales, schema)
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- `mapInPandas` çš„èªæ³•éŒ¯èª¤
- æ­£ç¢ºèªæ³•æ˜¯ `df.mapInPandas(func, schema)`ï¼Œæ²’æœ‰åˆ†çµ„æ¬„ä½åƒæ•¸
- `mapInPandas` è™•ç†æ•´å€‹ DataFrameï¼Œä¸åšåˆ†çµ„
- è‹¥è¦åˆ†çµ„è™•ç†ï¼Œå¿…é ˆä½¿ç”¨ `groupBy().applyInPandas()`

---

## ğŸ§  è¨˜æ†¶æ³•
### å£è¨£
**ã€Œåˆ†çµ„ç”¨ applyInPandasï¼Œå…¨è¡¨ç”¨ mapInPandasã€**

### Pandas æ•´åˆ API å°ç…§è¡¨
| API | è¼¸å…¥ | è¼¸å‡º | é©ç”¨å ´æ™¯ |
|-----|------|------|---------|
| `pandas_udf` | Series | Series | æ¬„ä½ç´šè½‰æ› |
| `mapInPandas` | Iterator[DataFrame] | Iterator[DataFrame] | å…¨è¡¨è™•ç† |
| `applyInPandas` | DataFrame | DataFrame | åˆ†çµ„è™•ç† |
| `applyInArrow` | PyArrow Table | PyArrow Table | é«˜æ•ˆèƒ½åˆ†çµ„è™•ç† |

### å¸¸è¦‹ä½¿ç”¨æ¨¡å¼
```python
# æ¨¡å¼ 1: æ¬„ä½ç´š UDF
@pandas_udf(DoubleType())
def my_udf(s: pd.Series) -> pd.Series:
    return s * 2

df.select(my_udf(col("value")))

# æ¨¡å¼ 2: åˆ†çµ„ DataFrame è™•ç†
df.groupBy("key").applyInPandas(func, schema)

# æ¨¡å¼ 3: å…¨è¡¨ DataFrame è™•ç†
df.mapInPandas(func, schema)
```

### ä½•æ™‚é¸æ“‡ applyInPandasï¼Ÿ
| ç‰¹å¾µ | é©åˆ applyInPandas |
|------|-------------------|
| éœ€è¦åˆ†çµ„è™•ç† | âœ… |
| ä½¿ç”¨ Pandas è¤‡é›œæ“ä½œï¼ˆrolling, resampleï¼‰| âœ… |
| å‡½æ•¸éœ€è¦æ•´çµ„è³‡æ–™ | âœ… |
| è¼¸å‡ºçµæ§‹èˆ‡è¼¸å…¥ä¸åŒ | âœ… |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶
- [applyInPandas](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.applyInPandas.html)
- [Pandas UDFs](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html)

---

**[è¿”å›é¡Œç›®](#question-052)**
