# Question #026

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-026`

### ä¾†æº
**ä¾†æº:** Community

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L3-Advanced`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A junior data engineer is using the following code to de-duplicate raw streaming data and insert them in a target Delta table:

```python
spark.readStream
    .table("orders_raw")
    .dropDuplicates(["order_id", "order_timestamp"])
.writeStream
    .option("checkpointLocation", "dbfs:/checkpoints")
    .table("orders_unique")
```

A senior data engineer pointed out that this approach is not enough for having distinct records in the target table when there are late-arriving, duplicate records.

Which of the following could explain the senior data engineer's remark?

### é¸é …

- **A.** A window function is also needed to apply deduplication for each non-overlapping interval.
- **B.** A ranking function is also needed to ensure processing only the most recent records.
- **C.** Watermarking is also needed to only track state information for a window of time in which we expect records could be delayed.
- **D.** The new records need also to be deduplicated against previously inserted data into the table.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Structured-Streaming`, `Delta-Merge`, `Deduplication`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Execution-Behavior`, `Scope-Misunderstanding`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Development

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** D
- **ç¤¾ç¾¤å…±è­˜:** D

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming / Deduplication Patterns
**é—œéµæ¦‚å¿µ:** dropDuplicates scope, Insert-only merge, Historical deduplication

**é¡Œç›®é—œéµå­—ï¼š**
- **dropDuplicates**: ä¸²æµå»é‡ã€‚
- **late-arriving, duplicate records**: é²åˆ°çš„é‡è¤‡è¨˜éŒ„ã€‚
- **not enough**: ä¸è¶³ä»¥é”æˆç›®æ¨™ã€‚

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

1. **dropDuplicates çš„ç¯„åœé™åˆ¶**:
   ```
   Micro-batch 1: [A, B, A] â†’ dropDuplicates â†’ [A, B]
   Micro-batch 2: [A, C]   â†’ dropDuplicates â†’ [A, C]

   çµæœï¼šA å‡ºç¾å…©æ¬¡ï¼ï¼ˆåˆ†åˆ¥ä¾†è‡ªä¸åŒ batchï¼‰
   ```

2. **å•é¡Œæƒ…å¢ƒ**:
   - `dropDuplicates` åªèƒ½åœ¨**ç•¶å‰ micro-batch å…§**å»é‡
   - ç„¡æ³•çŸ¥é“è©²è¨˜éŒ„æ˜¯å¦**å·²ç¶“å­˜åœ¨æ–¼ç›®æ¨™è¡¨**
   - é²åˆ°çš„é‡è¤‡è¨˜éŒ„æœƒè¢«ç•¶ä½œæ–°è¨˜éŒ„æ’å…¥

3. **è§£æ±ºæ–¹æ¡ˆï¼šInsert-Only Merge**:
   ```python
   def upsert_to_delta(batch_df, batch_id):
       target = DeltaTable.forName(spark, "orders_unique")
       target.alias("t").merge(
           batch_df.alias("s"),
           "t.order_id = s.order_id"
       ).whenNotMatchedInsertAll().execute()

   stream.writeStream.foreachBatch(upsert_to_delta).start()
   ```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A - Window function

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ç”¨é€”ä¸åŒ**: Window å‡½æ•¸ç”¨æ–¼åˆ†å€è¨ˆç®—ï¼Œä¸æ˜¯å»é‡çš„æ ¸å¿ƒæ–¹æ¡ˆã€‚
- **ä¸è§£æ±ºæ­·å²å•é¡Œ**: å³ä½¿ç”¨ window åˆ†å€ï¼Œä»ç„¡æ³•å°æ¯”ç›®æ¨™è¡¨ä¸­çš„æ­·å²è³‡æ–™ã€‚

### B - Ranking function

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ç”¨é€”ä¸åŒ**: Rankingï¼ˆå¦‚ row_numberï¼‰ç”¨æ–¼é¸å–æœ€æ–°/æœ€èˆŠè¨˜éŒ„ã€‚
- **ä¸è§£æ±ºè·¨ batch å•é¡Œ**: æ’ååªåœ¨ç•¶å‰è³‡æ–™é›†å…§æœ‰æ•ˆã€‚

### C - Watermarking

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ç”¨é€”ä¸åŒ**: Watermark ç”¨æ–¼**ç‹€æ…‹ç®¡ç†**ï¼Œé™åˆ¶è¿½è¹¤çš„æ™‚é–“ç¯„åœä»¥é¿å…ç„¡é™ç‹€æ…‹ã€‚
- **ä»æœ‰é™åˆ¶**: å³ä½¿åŠ äº† watermarkï¼ŒdropDuplicates ä»åªåœ¨ state å…§å»é‡ï¼Œç„¡æ³•å°æ¯”ç›®æ¨™è¡¨ã€‚
- **å¸¸è¦‹æ··æ·†**: Watermark + dropDuplicates ç¢ºå¯¦æ˜¯å¸¸è¦‹çµ„åˆï¼Œä½†è§£æ±ºçš„æ˜¯ä¸åŒå•é¡Œã€‚

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€ŒdropDuplicates ç®¡ batchï¼Œæ­·å²å»é‡é  mergeã€**

### ä¸²æµå»é‡æ–¹æ¡ˆæ¯”è¼ƒ

| æ–¹æ¡ˆ | å»é‡ç¯„åœ | é©ç”¨å ´æ™¯ |
|------|----------|----------|
| `dropDuplicates()` | ç•¶å‰ micro-batch | å¿«é€Ÿéæ¿¾æ˜é¡¯é‡è¤‡ |
| `dropDuplicates() + watermark` | ç‹€æ…‹æ™‚é–“çª—å£å…§ | æœ‰ç•Œçš„ late data è™•ç† |
| **Insert-only Merge** | **å…¨éƒ¨æ­·å²è³‡æ–™** | **å®Œæ•´å»é‡** |

### å®Œæ•´å»é‡æµç¨‹

```
Raw Data â†’ dropDuplicates (batch å…§å»é‡)
         â†’ foreachBatch + MERGE (æ­·å²å»é‡)
         â†’ Target Table (å®Œå…¨ä¸é‡è¤‡)
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Streaming Deduplication](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#streaming-deduplication)
- [Delta Merge for Deduplication](https://docs.databricks.com/delta/merge.html#data-deduplication-when-writing-into-delta-tables)

---

**[è¿”å›é¡Œç›®](#question-026)**
