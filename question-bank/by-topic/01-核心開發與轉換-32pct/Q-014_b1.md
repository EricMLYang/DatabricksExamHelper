# Question #014

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-014`

### ä¾†æº
**ä¾†æº:** Sample / Batch 1

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
The data engineering team wants to build a pipeline that receives customers data as change data capture (CDC) feed from a source system. The CDC events logged at the source contain the data of the records along with metadata information. This metadata indicates whether the specified record was inserted, updated, or deleted. In addition to a timestamp column identified by the field `update_time` indicating the order in which the changes happened. Each record has a primary key identified by the field `customer_id`.

In the same batch, multiple changes for the same customer could be received with different `update_time`. The team wants to store only the most recent information for each customer in the target Delta Lake table.

Which of the following solutions meets these requirements?

### é¸é …
- **A.** Use MERGE INTO to upsert the most recent entry for each customer_id into the table

- **B.** Use dropDuplicates function to remove duplicates by customer_id, then merge the duplicate records into the table

- **C.** Use MERGE INTO with SEQUENCE BY clause on the update_time for ordering how operations should be applied

- **D.** Enable Delta Lake's Change Data Feed (CDF) on the target table to automatically merge the received CDC feed

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Delta-CDC`, `Delta-MERGE`, `ETL-Patterns`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Syntax-Confusion`, `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Data Transformation

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `A`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** A
- **ç¤¾ç¾¤å…±è­˜:** A

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** CDC (Change Data Capture) è³‡æ–™è™•ç†èˆ‡ Delta Lake MERGE æ“ä½œ

**é—œéµæ¦‚å¿µ:** 
- MERGE INTO çš„ Upsert èªç¾©
- CDC è³‡æ–™å»é‡ç­–ç•¥
- æ™‚åºè³‡æ–™è™•ç†
- Delta Lake çš„è®Šæ›´è³‡æ–™åˆä½µ

**é¡Œç›®é—œéµå­—ï¼š**
- **CDC feed**: è®Šæ›´è³‡æ–™æ•ç²ä¸²æµï¼ŒåŒ…å« INSERT/UPDATE/DELETE äº‹ä»¶
- **update_time**: æ™‚é–“æˆ³è¨˜æ¬„ä½ï¼Œæ¨™ç¤ºè®Šæ›´é †åº
- **customer_id**: ä¸»éµï¼Œå”¯ä¸€è­˜åˆ¥å®¢æˆ¶
- **most recent information**: éœ€ä¿ç•™æœ€æ–°çš„è³‡æ–™ç‰ˆæœ¬ï¼ˆåŸºæ–¼ update_timeï¼‰
- **multiple changes in the same batch**: åŒä¸€æ‰¹æ¬¡å¯èƒ½æœ‰åŒä¸€å®¢æˆ¶çš„å¤šç­†è®Šæ›´

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

`MERGE INTO` æ˜¯ Delta Lake çš„æ ¸å¿ƒ Upsert æŒ‡ä»¤ï¼Œå°ˆç‚ºè™•ç† CDC å ´æ™¯è¨­è¨ˆã€‚å®ƒå¯ä»¥åœ¨å–®ä¸€äº‹å‹™ä¸­åŸ·è¡Œæ’å…¥ã€æ›´æ–°èˆ‡åˆªé™¤æ“ä½œï¼Œå®Œç¾ç¬¦åˆé¡Œç›®éœ€æ±‚ã€‚

#### å®Œæ•´è§£æ±ºæ–¹æ¡ˆ

**æ­¥é©Ÿ 1: å‰ç½®è™•ç† - å»é‡ä¸¦ä¿ç•™æœ€æ–°è¨˜éŒ„**

åœ¨åŸ·è¡Œ MERGE å‰ï¼Œå…ˆå°ä¾†æºè³‡æ–™é€²è¡Œå»é‡ï¼Œåªä¿ç•™æ¯å€‹ `customer_id` çš„æœ€æ–°è¨˜éŒ„ï¼š

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number, col

# å®šç¾©è¦–çª—ï¼šæŒ‰ customer_id åˆ†çµ„ï¼Œä¾ update_time é™åºæ’åˆ—
window_spec = Window.partitionBy("customer_id").orderBy(col("update_time").desc())

# å°æ¯å€‹ customer_id æ¨™è¨˜æœ€æ–°è¨˜éŒ„ï¼ˆrow_number = 1ï¼‰
cdc_deduped = (cdc_df
    .withColumn("row_num", row_number().over(window_spec))
    .filter(col("row_num") == 1)
    .drop("row_num")
)
```

**æ­¥é©Ÿ 2: ä½¿ç”¨ MERGE INTO åˆä½µè³‡æ–™**

```sql
MERGE INTO target_table AS target
USING cdc_deduped AS source
ON target.customer_id = source.customer_id
WHEN MATCHED AND source.operation = 'DELETE' THEN
  DELETE
WHEN MATCHED THEN
  UPDATE SET *
WHEN NOT MATCHED AND source.operation != 'DELETE' THEN
  INSERT *
```

æˆ–ä½¿ç”¨ PySpark APIï¼š

```python
from delta.tables import DeltaTable

delta_table = DeltaTable.forName(spark, "target_table")

(delta_table.alias("target")
    .merge(
        cdc_deduped.alias("source"),
        "target.customer_id = source.customer_id"
    )
    .whenMatchedDelete(condition="source.operation = 'DELETE'")
    .whenMatchedUpdateAll()
    .whenNotMatchedInsertAll(condition="source.operation != 'DELETE'")
    .execute()
)
```

#### ç‚ºä»€éº¼é€™å€‹æ–¹æ¡ˆæœ‰æ•ˆï¼Ÿ

1. **å»é‡é‚è¼¯**: Window Function ç¢ºä¿åŒä¸€ `customer_id` åªä¿ç•™æœ€æ–°çš„ `update_time` è¨˜éŒ„
2. **åŸå­æ€§**: MERGE æ˜¯å–®ä¸€äº‹å‹™ï¼Œä¿è­‰è³‡æ–™ä¸€è‡´æ€§
3. **æ”¯æ´æ‰€æœ‰ CDC æ“ä½œ**: INSERTã€UPDATEã€DELETE éƒ½èƒ½æ­£ç¢ºè™•ç†
4. **æ•ˆèƒ½å„ªåŒ–**: Delta Lake çš„ MERGE å…§å»ºå„ªåŒ–ï¼Œè™•ç†å¤§è¦æ¨¡è³‡æ–™é«˜æ•ˆ

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … B
**Use dropDuplicates function to remove duplicates by customer_id, then merge the duplicate records into the table**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **`dropDuplicates()` ç„¡æ³•æ§åˆ¶ä¿ç•™å“ªç­†è¨˜éŒ„**:
   - `dropDuplicates("customer_id")` æœƒéš¨æ©Ÿä¿ç•™ä¸€ç­†è¨˜éŒ„ï¼Œç„¡æ³•ä¿è­‰æ˜¯ `update_time` æœ€æ–°çš„
   - Spark ä¸ä¿è­‰å“ªç­†è¨˜éŒ„æœƒè¢«ä¿ç•™ï¼Œå¯èƒ½å°è‡´èˆŠè³‡æ–™è¦†è“‹æ–°è³‡æ–™

2. **èªæ„çŸ›ç›¾**:
   - é¸é …èªªã€Œremove duplicatesã€ç„¶å¾Œã€Œmerge the duplicate recordsã€é‚è¼¯ä¸é€š
   - å»é‡å¾Œå°±æ²’æœ‰é‡è¤‡è¨˜éŒ„äº†ï¼Œé‚„è¦ merge ä»€éº¼ï¼Ÿ

3. **æ­£ç¢ºç”¨æ³•å°æ¯”**:
```python
# âŒ éŒ¯èª¤ï¼šç„¡æ³•ä¿è­‰ä¿ç•™æœ€æ–°è¨˜éŒ„
cdc_df.dropDuplicates(["customer_id"])

# âœ… æ­£ç¢ºï¼šä½¿ç”¨ Window Function æŒ‡å®šä¿ç•™æœ€æ–°è¨˜éŒ„
cdc_df.withColumn("row_num", row_number().over(
    Window.partitionBy("customer_id").orderBy(col("update_time").desc())
)).filter(col("row_num") == 1)
```

---

### é¸é … C
**Use MERGE INTO with SEQUENCE BY clause on the update_time for ordering how operations should be applied**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **`SEQUENCE BY` ä¸é©ç”¨æ–¼ `MERGE INTO`**:
   - `SEQUENCE BY` æ˜¯ **`APPLY CHANGES INTO`** çš„å°ˆå±¬å­å¥ï¼Œä¸èƒ½ç”¨æ–¼ `MERGE INTO`
   - ä½¿ç”¨ `MERGE INTO ... SEQUENCE BY` æœƒç”¢ç”Ÿèªæ³•éŒ¯èª¤

2. **æ­£ç¢ºçš„èªæ³•å°æ¯”**:

```sql
-- âŒ éŒ¯èª¤ï¼šMERGE INTO ä¸æ”¯æ´ SEQUENCE BY
MERGE INTO target_table
USING source_table
ON target.id = source.id
SEQUENCE BY source.update_time  -- èªæ³•éŒ¯èª¤ï¼
WHEN MATCHED THEN UPDATE SET *

-- âœ… æ­£ç¢ºï¼šä½¿ç”¨ APPLY CHANGES INTOï¼ˆDelta Live Tablesï¼‰
APPLY CHANGES INTO target_table
FROM source_stream
KEYS (customer_id)
SEQUENCE BY update_time  -- é€™è£¡æ‰èƒ½ç”¨ï¼
```

3. **`APPLY CHANGES INTO` vs `MERGE INTO` æ¯”è¼ƒ**:

| ç‰¹æ€§ | APPLY CHANGES INTO | MERGE INTO |
|------|-------------------|-----------|
| **ä½¿ç”¨ç’°å¢ƒ** | Delta Live Tables (DLT) | æ¨™æº– Delta Lake |
| **SEQUENCE BY** | âœ… æ”¯æ´ | âŒ ä¸æ”¯æ´ |
| **è‡ªå‹•è™•ç† CDC** | âœ… è‡ªå‹•è™•ç†æ™‚åº | âš ï¸ éœ€æ‰‹å‹•å»é‡ |
| **é©ç”¨å ´æ™¯** | DLT Pipeline | ä¸€èˆ¬ Spark ç¨‹å¼ |

---

### é¸é … D
**Enable Delta Lake's Change Data Feed (CDF) on the target table to automatically merge the received CDC feed**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **CDF çš„ç”¨é€”å®Œå…¨ç›¸å**:
   - **CDF (Change Data Feed)** æ˜¯ç”¨ä¾†**ç”¢ç”Ÿè®Šæ›´è¨˜éŒ„**ï¼Œè€Œé**æ¶ˆè²»è®Šæ›´è¨˜éŒ„**
   - CDF è®“ä½ è¿½è¹¤ Delta è¡¨æ ¼çš„è®Šæ›´æ­·å²ï¼Œä¸æ˜¯ç”¨ä¾†åˆä½µå¤–éƒ¨ CDC è³‡æ–™

2. **åŠŸèƒ½æ··æ·†**:
```python
# âŒ éŒ¯èª¤ç†è§£ï¼šCDF ä¸æœƒè‡ªå‹•åˆä½µå¤–éƒ¨ CDC è³‡æ–™
spark.sql("""
    ALTER TABLE target_table 
    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)
""")
# é€™åªæ˜¯å•Ÿç”¨è®Šæ›´è¿½è¹¤ï¼Œä¸æœƒè™•ç†ä¾†æº CDC

# âœ… æ­£ç¢ºç†è§£ï¼šCDF ç”¨æ–¼è®€å–è¡¨æ ¼çš„è®Šæ›´æ­·å²
changes_df = spark.read.format("delta") \
    .option("readChangeFeed", "true") \
    .option("startingVersion", 0) \
    .table("target_table")
```

3. **CDF vs å¤–éƒ¨ CDC æ¯”è¼ƒ**:

| åŠŸèƒ½ | Change Data Feed (CDF) | å¤–éƒ¨ CDC Feed |
|------|----------------------|--------------|
| **æ–¹å‘** | Delta Lake â†’ ä¸‹æ¸¸ç³»çµ± | ä¾†æºç³»çµ± â†’ Delta Lake |
| **ç”¨é€”** | **ç”¢ç”Ÿ**è®Šæ›´è¨˜éŒ„ | **æ¶ˆè²»**è®Šæ›´è¨˜éŒ„ |
| **å•Ÿç”¨æ–¹å¼** | `delta.enableChangeDataFeed` | MERGE INTO / APPLY CHANGES |
| **è³‡æ–™æµå‘** | ğŸ“¤ è¼¸å‡ºè®Šæ›´ | ğŸ“¥ æ¥æ”¶è®Šæ›´ |

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€ŒCDC å…ˆå»é‡ï¼ŒMERGE æ‰å®Œç¾ã€**  
â†’ CDC è³‡æ–™å…ˆç”¨ Window Function å»é‡ï¼Œå†ç”¨ MERGE INTO åˆä½µ

**ã€ŒSEQUENCE æ˜¯ APPLY çš„ï¼ŒMERGE ç”¨ä¸äº†ã€**  
â†’ SEQUENCE BY åªèƒ½ç”¨æ–¼ APPLY CHANGES INTOï¼Œä¸èƒ½ç”¨æ–¼ MERGE INTO

### å°æ¯”è¡¨ï¼šCDC è™•ç†æ–¹æ¡ˆæ¯”è¼ƒ

| æ–¹æ¡ˆ | é©ç”¨å ´æ™¯ | å»é‡æ–¹å¼ | æ™‚åºè™•ç† | ç’°å¢ƒéœ€æ±‚ |
|------|---------|---------|---------|---------|
| **MERGE INTO** | ä¸€èˆ¬ Spark/Delta | æ‰‹å‹• Window Function | å‰ç½®è™•ç† | æ¨™æº– Delta Lake |
| **APPLY CHANGES INTO** | Delta Live Tables | è‡ªå‹•ï¼ˆSEQUENCE BYï¼‰ | è‡ªå‹•è™•ç† | DLT Pipeline |
| **dropDuplicates** | âŒ ä¸é©åˆ CDC | éš¨æ©Ÿä¿ç•™ | ç„¡æ³•æ§åˆ¶ | - |
| **CDF** | âŒ ç›¸åæ–¹å‘ | N/Aï¼ˆç”¨æ–¼è¼¸å‡ºï¼‰ | N/A | - |

### å¯¦ä¾‹è¨˜æ†¶

**æƒ…å¢ƒï¼šéŠ€è¡Œå¸³æˆ¶äº¤æ˜“æ­·å²**
- åŒä¸€å€‹å¸³æˆ¶åœ¨åŒä¸€æ‰¹æ¬¡æœ‰å¤šç­†äº¤æ˜“ï¼ˆå­˜æ¬¾ã€ææ¬¾ã€è½‰å¸³ï¼‰
- éœ€è¦ä¿ç•™æœ€å¾Œä¸€ç­†äº¤æ˜“çš„çµæœï¼ˆæœ€æ–°é¤˜é¡ï¼‰
- **è§£æ±ºæ–¹æ¡ˆ**ï¼š
  1. æŒ‰ `account_id` å’Œ `transaction_time` æ’åº
  2. å–æ¯å€‹å¸³æˆ¶çš„æœ€æ–°äº¤æ˜“ï¼ˆWindow Functionï¼‰
  3. ç”¨ MERGE INTO æ›´æ–°å¸³æˆ¶è¡¨

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Delta Lake MERGE - Databricks Documentation](https://docs.databricks.com/delta/merge.html)
- [Change Data Capture with Delta Lake](https://docs.databricks.com/delta/delta-change-data-feed.html)
- [APPLY CHANGES INTO - Delta Live Tables](https://docs.databricks.com/delta-live-tables/cdc.html)
- [Window Functions in PySpark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html)

---

**[è¿”å›é¡Œç›®](#question-014)**
