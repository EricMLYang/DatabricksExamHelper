# Q-087

## é¡Œç›®è³‡è¨Š

**ID:** `Q-087`

**ä¾†æº:** Mock Exam

**é›£åº¦:** `L1-Basic`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

Which of the following technologies can be used to identify key areas of text when parsing Spark Driver log4j output?

### é¸é …

- **A.** Regex
- **B.** Julia
- **C.** pyspsark.ml.feature
- **D.** Scala Datasets
- **E.** C++

---

## æ¨™ç±¤ç³»çµ±

**Topics:** `Debugging`, `Log-Analysis`, `Spark-Core`

**Traps:** `Concept-Confusion`, `Language-vs-Tool`

**Domain:** `é–‹ç™¼èˆ‡è½‰æ› (Development & Transformation)`

---

## ç­”æ¡ˆèˆ‡åˆ†æ

### æ­£ç¢ºç­”æ¡ˆ

**æ­£è§£:** `A`

**ç¤¾ç¾¤æŠ•ç¥¨:** A (87%)
**ä¾†æºæ¨™è¨»:** E (C++)

**æ·±å…¥æ´å¯Ÿ:** ä¾†æºæ¨™è¨»ç­”æ¡ˆ E è¢«ç¤¾ç¾¤æ™®éèªç‚ºæ˜¯éŒ¯èª¤çš„ã€‚Regex æ˜¯è§£ææ–‡å­—æ—¥èªŒçš„æ¨™æº–æŠ€è¡“ï¼ŒC++ æ˜¯ç¨‹å¼èªè¨€è€Œéè§£ææŠ€è¡“ã€‚

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** Regex (Regular Expressions) æ­£å‰‡è¡¨é”å¼
**çŸ¥è­˜é ˜åŸŸ:** é–‹ç™¼èˆ‡è½‰æ› - é™¤éŒ¯èˆ‡æ—¥èªŒåˆ†æ
**é—œéµæ¦‚å¿µ:** 
- æ–‡å­—æ¨¡å¼åŒ¹é…ï¼ˆText Pattern Matchingï¼‰
- æ—¥èªŒè§£ææŠ€è¡“
- Spark Driver log4j è¼¸å‡ºæ ¼å¼

### æ¬¡è¦è€ƒé»
- å€åˆ†ã€Œè§£ææŠ€è¡“ã€èˆ‡ã€Œç¨‹å¼èªè¨€ã€
- ç†è§£ log4j æ—¥èªŒçµæ§‹

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A (Regex) æ˜¯æ­£ç¢ºçš„ï¼Ÿ

**æŠ€è¡“åŸç†:**

Regex (Regular Expressionsï¼Œæ­£å‰‡è¡¨é”å¼) æ˜¯**æ–‡å­—æ¨¡å¼åŒ¹é…çš„æ¨™æº–å·¥å…·**ï¼Œå°ˆé–€ç”¨æ–¼ï¼š

1. **è­˜åˆ¥æ–‡å­—æ¨¡å¼** - ä½¿ç”¨æ¨¡å¼å®šç¾©ä¾†åŒ¹é…ç‰¹å®šæ ¼å¼çš„æ–‡å­—
2. **æå–é—œéµè³‡è¨Š** - å¾éçµæ§‹åŒ–æ–‡å­—ä¸­æ“·å–çµæ§‹åŒ–è³‡æ–™
3. **æ—¥èªŒè§£æ** - log4j è¼¸å‡ºæ˜¯ç´”æ–‡å­—æ ¼å¼ï¼ŒRegex æ˜¯æœ€å¸¸ç”¨çš„è§£ææ–¹æ³•

**ç¬¦åˆéœ€æ±‚:**

Spark Driver log4j è¼¸å‡ºç¯„ä¾‹ï¼š
```
24/01/16 10:23:45 INFO SparkContext: Starting job: count at <console>:1
24/01/16 10:23:45 ERROR TaskSchedulerImpl: Lost executor 2 on node123
24/01/16 10:23:46 WARN MemoryStore: Not enough space to cache rdd_1_0
```

ä½¿ç”¨ Regex è§£æé—œéµè³‡è¨Šï¼š
```python
import re

# æå–æ—¥èªŒç­‰ç´š (INFO, ERROR, WARN)
log_level_pattern = r'(INFO|ERROR|WARN|DEBUG)'

# æå–æ™‚é–“æˆ³è¨˜
timestamp_pattern = r'\d{2}/\d{2}/\d{2}\s+\d{2}:\d{2}:\d{2}'

# æå–é¡åˆ¥åç¨±
class_pattern = r'(INFO|ERROR|WARN)\s+([^:]+):'

# ç¯„ä¾‹ï¼šè§£æéŒ¯èª¤è¨Šæ¯
error_pattern = r'ERROR\s+(\w+):\s+(.+)'
match = re.search(error_pattern, log_line)
if match:
    component = match.group(1)  # TaskSchedulerImpl
    message = match.group(2)    # Lost executor 2 on node123
```

**å¯¦å‹™æ‡‰ç”¨:**

åœ¨ Databricks/Spark é™¤éŒ¯æµç¨‹ä¸­ï¼š

```python
# è®€å– Spark Driver æ—¥èªŒ
logs = spark.read.text("/databricks/driver/logs/")

# ä½¿ç”¨ Regex éæ¿¾ ERROR è¨Šæ¯
from pyspark.sql.functions import regexp_extract

errors = logs.select(
    regexp_extract("value", r"(\d{2}/\d{2}/\d{2}\s+\d{2}:\d{2}:\d{2})", 1).alias("timestamp"),
    regexp_extract("value", r"ERROR\s+([^:]+)", 1).alias("component"),
    regexp_extract("value", r"ERROR\s+[^:]+:\s+(.+)", 1).alias("message")
).filter("component != ''")
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … B - Julia

**éŒ¯èª¤åŸå› :** Julia æ˜¯**ç¨‹å¼èªè¨€**ï¼Œä¸æ˜¯æ–‡å­—è§£ææŠ€è¡“

**è©³ç´°åˆ†æ:**
- Julia æ˜¯ä¸€ç¨®é«˜æ•ˆèƒ½ç§‘å­¸è¨ˆç®—èªè¨€ï¼ˆé¡ä¼¼ Python/Rï¼‰
- é›–ç„¶ Julia ä¹Ÿæœ‰ Regex å‡½å¼åº«ï¼Œä½† Julia **æœ¬èº«**ä¸æ˜¯è§£æå·¥å…·
- é¡Œç›®å•çš„æ˜¯ã€Œtechnologiesã€ï¼ˆæŠ€è¡“ï¼‰ï¼Œè€Œéã€Œprogramming languagesã€
- Spark ç”Ÿæ…‹ç³»çµ±ä¸­ä¸ä½¿ç”¨ Julia é€²è¡Œæ—¥èªŒè§£æ

**æ˜“æ··æ·†é»:**
æ··æ·†ã€Œèªè¨€ã€èˆ‡ã€ŒæŠ€è¡“ã€ï¼ŒJulia å¯ä»¥**ä½¿ç”¨** Regexï¼Œä½†å®ƒæœ¬èº«ä¸æ˜¯ Regex

---

### é¸é … C - pyspsark.ml.feature

**éŒ¯èª¤åŸå› :** é€™æ˜¯**æ©Ÿå™¨å­¸ç¿’ç‰¹å¾µå·¥ç¨‹æ¨¡çµ„**ï¼Œä¸æ˜¯æ–‡å­—è§£æå·¥å…·

**è©³ç´°åˆ†æ:**
- `pyspark.ml.feature` ç”¨æ–¼æ©Ÿå™¨å­¸ç¿’çš„ç‰¹å¾µè½‰æ›ï¼ˆå¦‚ tokenization, TF-IDF, word2vecï¼‰
- ç”¨æ–¼**çµæ§‹åŒ–è³‡æ–™çš„ç‰¹å¾µæå–**ï¼Œè€Œé**éçµæ§‹åŒ–æ—¥èªŒçš„æ¨¡å¼åŒ¹é…**
- åŠŸèƒ½ç¯„ä¾‹ï¼š
  - `Tokenizer`: å°‡æ–‡æœ¬åˆ†å‰²ç‚ºå–®è©
  - `HashingTF`: è¨ˆç®—è©é »
  - `Word2Vec`: è©å‘é‡åŒ–
- **ä¸é©ç”¨æ–¼æ—¥èªŒè§£æ**ï¼Œå› ç‚ºæ—¥èªŒè§£æéœ€è¦ç²¾ç¢ºçš„æ¨¡å¼åŒ¹é…ï¼Œè€Œéçµ±è¨ˆç‰¹å¾µæå–

**æ˜“æ··æ·†é»:**
é›–ç„¶ `pyspark.ml.feature` å¯ä»¥è™•ç†æ–‡å­—ï¼Œä½†å®ƒæ˜¯ç‚º ML è¨­è¨ˆçš„ï¼Œä¸æ˜¯ç‚ºäº†ã€Œè­˜åˆ¥é—œéµå€åŸŸã€çš„æ—¥èªŒè§£æ

---

### é¸é … D - Scala Datasets

**éŒ¯èª¤åŸå› :** Scala Datasets æ˜¯**è³‡æ–™çµæ§‹**ï¼Œä¸æ˜¯è§£ææŠ€è¡“

**è©³ç´°åˆ†æ:**
- `Dataset[T]` æ˜¯ Spark çš„å¼·å‹åˆ¥åˆ†æ•£å¼è³‡æ–™é›†ï¼ˆScala APIï¼‰
- å®ƒæ˜¯**è³‡æ–™å®¹å™¨**ï¼Œç”¨æ–¼å„²å­˜å’Œæ“ä½œè³‡æ–™ï¼Œè€Œéè§£æå·¥å…·
- é›–ç„¶å¯ä»¥åœ¨ Dataset ä¸Šä½¿ç”¨ Regexï¼ˆé€éå­—ä¸²æ“ä½œå‡½æ•¸ï¼‰ï¼Œä½† Dataset **æœ¬èº«**ä¸æ˜¯è§£ææŠ€è¡“
- é¡æ¯”ï¼šDataset å°±åƒã€Œå®¹å™¨ã€ï¼ŒRegex æ‰æ˜¯ã€Œå·¥å…·ã€

**æ˜“æ··æ·†é»:**
æ··æ·†è³‡æ–™çµæ§‹èˆ‡æ“ä½œå·¥å…·ï¼ŒDataset å¯ä»¥**åŒ…å«**æ—¥èªŒè³‡æ–™ï¼Œä½†ä¸æ˜¯è§£ææŠ€è¡“

---

### é¸é … E - C++ (ä¾†æºæ¨™è¨»ç­”æ¡ˆ)

**éŒ¯èª¤åŸå› :** C++ æ˜¯**ç¨‹å¼èªè¨€**ï¼Œä¸æ˜¯æ–‡å­—è§£ææŠ€è¡“

**è©³ç´°åˆ†æ:**
- C++ æ˜¯ä½éšç³»çµ±ç¨‹å¼èªè¨€
- é›–ç„¶ C++ æœ‰ Regex å‡½å¼åº«ï¼ˆå¦‚ `std::regex`ï¼‰ï¼Œä½†ï¼š
  1. C++ **æœ¬èº«**ä¸æ˜¯è§£ææŠ€è¡“
  2. Spark æ˜¯åŸºæ–¼ JVM (Java/Scala)ï¼ŒDriver æ—¥èªŒè™•ç†é€šå¸¸ç”¨ Python/Scala
  3. åœ¨ Databricks/Spark ç’°å¢ƒä¸­ï¼Œ**ä¸æœƒç”¨ C++ ä¾†è§£ææ—¥èªŒ**
- C++ èˆ‡ Spark Driver log4j è¼¸å‡º**å®Œå…¨ç„¡é—œ**

**ç‚ºä½•ä¾†æºæ¨™è¨»ç­”æ¡ˆæ˜¯ Eï¼Ÿ**
é€™æ˜¯æ˜é¡¯çš„ç­”æ¡ˆéŒ¯èª¤ï¼š
- å¯èƒ½æ˜¯é¡Œç›®è£½ä½œè€…çš„å¤±èª¤
- å¯èƒ½æ··æ·†äº†ã€Œç¨‹å¼èªè¨€ã€èˆ‡ã€ŒæŠ€è¡“/æ–¹æ³•ã€
- ç¤¾ç¾¤ 87% æŠ•ç¥¨ Aï¼Œè­‰å¯¦ Regex æ‰æ˜¯æ­£ç¢ºç­”æ¡ˆ

**æ˜“æ··æ·†é»:**
æ··æ·†ã€Œå¯ä»¥ç”¨æŸèªè¨€å¯« Regexã€èˆ‡ã€Œè©²èªè¨€æœ¬èº«æ˜¯è§£ææŠ€è¡“ã€

---

## ğŸ§  è¨˜æ†¶æ³•èˆ‡æŠ€å·§

### å£è¨£
**ã€Œæ—¥èªŒè§£ææ‰¾ Regexï¼Œæ¨¡å¼åŒ¹é…ç¬¬ä¸€é¸ã€**

### é—œéµå°æ¯”è¡¨

| é¸é … | é¡å‹ | ç”¨é€” | é©åˆæ—¥èªŒè§£æï¼Ÿ |
|------|------|------|---------------|
| **A. Regex** | æ–‡å­—è§£ææŠ€è¡“ | æ¨¡å¼åŒ¹é…ã€æ–‡å­—æ“·å– | âœ… å®Œç¾é©åˆ |
| B. Julia | ç¨‹å¼èªè¨€ | ç§‘å­¸è¨ˆç®— | âŒ èªè¨€éæŠ€è¡“ |
| C. pyspark.ml.feature | ML ç‰¹å¾µå·¥ç¨‹ | TF-IDF, Word2Vec | âŒ ç”¨æ–¼ MLï¼Œéè§£æ |
| D. Scala Datasets | è³‡æ–™çµæ§‹ | å„²å­˜åˆ†æ•£å¼è³‡æ–™ | âŒ å®¹å™¨éå·¥å…· |
| E. C++ | ç¨‹å¼èªè¨€ | ç³»çµ±ç¨‹å¼è¨­è¨ˆ | âŒ èˆ‡ Spark æ—¥èªŒç„¡é—œ |

### æ–‡å­—è§£ææŠ€è¡“ vs ç¨‹å¼èªè¨€

```
âœ… æ­£ç¢ºæ€ç¶­ï¼š
å•é¡Œï¼šã€Œç”¨ä»€éº¼æŠ€è¡“è§£ææ–‡å­—ï¼Ÿã€
ç­”æ¡ˆï¼šã€ŒRegexï¼ˆæ­£å‰‡è¡¨é”å¼ï¼‰ã€

âŒ éŒ¯èª¤æ€ç¶­ï¼š
å•é¡Œï¼šã€Œç”¨ä»€éº¼æŠ€è¡“è§£ææ–‡å­—ï¼Ÿã€
ç­”æ¡ˆï¼šã€ŒPython/C++/Juliaï¼ˆèªè¨€ï¼‰ã€

æ­£ç¢ºé—œä¿‚ï¼š
ç¨‹å¼èªè¨€ï¼ˆPython/Scala/Julia/C++ï¼‰
    â†“ å¯ä»¥ä½¿ç”¨
æ–‡å­—è§£ææŠ€è¡“ï¼ˆRegexï¼‰
    â†“ ç”¨æ–¼
è§£ææ—¥èªŒï¼ˆlog4j outputï¼‰
```

### log4j æ—¥èªŒè§£æå¯¦æˆ°ç¯„ä¾‹

```python
import re

log_line = "24/01/16 10:23:45 ERROR TaskSchedulerImpl: Lost executor 2 on node123"

# Regex æ¨¡å¼
pattern = r"(?P<timestamp>\d{2}/\d{2}/\d{2}\s+\d{2}:\d{2}:\d{2})\s+(?P<level>\w+)\s+(?P<class>[\w.]+):\s+(?P<message>.+)"

match = re.match(pattern, log_line)
if match:
    print(f"æ™‚é–“: {match.group('timestamp')}")      # 24/01/16 10:23:45
    print(f"ç­‰ç´š: {match.group('level')}")          # ERROR
    print(f"é¡åˆ¥: {match.group('class')}")          # TaskSchedulerImpl
    print(f"è¨Šæ¯: {match.group('message')}")        # Lost executor 2 on node123
```

**é—œéµæ´å¯Ÿï¼š**
- **Regex æ˜¯æŠ€è¡“** â†’ ç›´æ¥å›ç­”ã€Œå¦‚ä½•è§£æã€
- **ç¨‹å¼èªè¨€æ˜¯å·¥å…·** â†’ åªæ˜¯åŸ·è¡Œ Regex çš„ç’°å¢ƒ
- **ML æ¨¡çµ„æ˜¯ç‰¹å¾µ** â†’ ç”¨æ–¼çµ±è¨ˆåˆ†æï¼Œéç²¾ç¢ºè§£æ
- **è³‡æ–™çµæ§‹æ˜¯å®¹å™¨** â†’ ç”¨æ–¼å„²å­˜ï¼Œéè§£æ

---

## ğŸ“š å»¶ä¼¸é–±è®€

### å®˜æ–¹æ–‡ä»¶
- [Databricks - View logs](https://docs.databricks.com/clusters/clusters-manage.html#view-logs)
- [Apache Spark - Debugging Guide](https://spark.apache.org/docs/latest/sql-performance-tuning.html#other-configuration-options)
- [Python re module - Regular Expressions](https://docs.python.org/3/library/re.html)

### ç›¸é—œæ¦‚å¿µ
- log4j æ—¥èªŒæ ¼å¼èˆ‡ç­‰ç´šï¼ˆINFO, WARN, ERROR, DEBUGï¼‰
- PySpark ä¸­ä½¿ç”¨ `regexp_extract()` å‡½æ•¸è§£ææ—¥èªŒ
- Spark UI èˆ‡æ—¥èªŒæŸ¥çœ‹æœ€ä½³å¯¦è¸

### å¯¦å‹™æŠ€å·§
```python
# åœ¨ Databricks ä¸­æª¢è¦– Driver æ—¥èªŒ
# æ–¹æ³• 1: Cluster UI â†’ Driver Logs
# æ–¹æ³• 2: ç¨‹å¼åŒ–è®€å–
driver_logs = spark.read.text("/databricks/driver/logs/")

# ä½¿ç”¨ Regex éæ¿¾ç‰¹å®šéŒ¯èª¤
from pyspark.sql.functions import regexp_extract, col

errors = driver_logs.filter(col("value").contains("ERROR"))
parsed = errors.select(
    regexp_extract("value", r"ERROR\s+(\w+)", 1).alias("component"),
    regexp_extract("value", r":\s+(.+)", 1).alias("message")
)
parsed.show(truncate=False)
```

---

## ğŸ”— ç›¸é—œé¡Œç›®
- Q-XXX: Spark UI ä½¿ç”¨èˆ‡é™¤éŒ¯
- Q-XXX: log4j è¨­å®šèˆ‡æ—¥èªŒç­‰ç´šèª¿æ•´
- Q-XXX: PySpark å­—ä¸²æ“ä½œå‡½æ•¸
