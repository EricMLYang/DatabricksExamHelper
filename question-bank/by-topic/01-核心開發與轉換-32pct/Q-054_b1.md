# Question #054

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-054`
### ä¾†æº
**ä¾†æº:** Community
### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
<!-- âš ï¸ ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è­¯ -->
Given the following query on the Delta table 'customers' on which Change Data Feed is enabled:

```python
spark.readStream
        .option("readChangeFeed", "true")
        .option("startingVersion", 0)
        .table("customers")
        .filter(col("_change_type").isin(["update_postimage"]))
    .writeStream
        .option("checkpointLocation", "dbfs:/checkpoints")
        .trigger(availableNow=True)
        .table("customers_updates")
```

Which statement describes the results of this query each time it is executed?

### é¸é …
<!-- âš ï¸ ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è­¯ -->
- **A.** Newly updated records will overwrite the target table.
- **B.** The entire history of updated records will be appended to the target table at each execution, which leads to duplicate entries.
- **C.** Newly updated records will be appended to the target table.
- **D.** The entire history of updated records will overwrite the target table at each execution.

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Delta-CDC`, `Streaming`
### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Execution-Behavior`
### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Change Data Capture

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`
### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** C
- **ç¤¾ç¾¤å…±è­˜:** C

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥
**æ ¸å¿ƒæŠ€è¡“:** Delta Lake Change Data Feed (CDF) + Structured Streaming
**é—œéµæ¦‚å¿µ:** Checkpointã€å¢é‡è™•ç†ã€é è¨­å¯«å…¥æ¨¡å¼
**é¡Œç›®é—œéµå­—ï¼š**
- **readChangeFeed**: å•Ÿç”¨ CDF è®€å–
- **checkpointLocation**: é€²åº¦è¿½è¹¤ä½ç½®
- **availableNow**: è™•ç†æ‰€æœ‰å¯ç”¨è³‡æ–™å¾Œåœæ­¢
- **_change_type**: CDF è®Šæ›´é¡å‹æ¬„ä½

---

## âœ… æ­£è§£èªªæ˜
### ç‚ºä»€éº¼ C æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

é€™å€‹æŸ¥è©¢çµåˆäº†å…©å€‹é—œéµæ©Ÿåˆ¶ï¼š

#### 1. Checkpoint æ©Ÿåˆ¶ç¢ºä¿å¢é‡è™•ç†
```
ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼š
â”œâ”€â”€ è®€å– version 0 åˆ°æœ€æ–°ç‰ˆæœ¬çš„æ‰€æœ‰è®Šæ›´
â”œâ”€â”€ è™•ç† update_postimage è¨˜éŒ„
â”œâ”€â”€ å¯«å…¥ customers_updates
â””â”€â”€ è¨˜éŒ„é€²åº¦åˆ° checkpoint

ç¬¬äºŒæ¬¡åŸ·è¡Œï¼š
â”œâ”€â”€ å¾ checkpoint è®€å–ä¸Šæ¬¡é€²åº¦
â”œâ”€â”€ åªè®€å–ã€Œæ–°å¢çš„ã€è®Šæ›´ï¼ˆä¸Šæ¬¡ä¹‹å¾Œçš„ï¼‰
â”œâ”€â”€ è™•ç† update_postimage è¨˜éŒ„
â””â”€â”€ Append åˆ° customers_updates

çµæœï¼šä¸æœƒé‡è¤‡è™•ç†å·²è™•ç†éçš„è³‡æ–™
```

#### 2. é è¨­å¯«å…¥æ¨¡å¼æ˜¯ Append
```python
.writeStream
    .table("customers_updates")  # é è¨­ä½¿ç”¨ append æ¨¡å¼
```
- æ²’æœ‰æŒ‡å®š `outputMode`ï¼Œé è¨­æ˜¯ **append**
- æ–°è³‡æ–™æœƒã€Œè¿½åŠ ã€åˆ°ç›®æ¨™è¡¨æ ¼ï¼Œä¸æœƒè¦†è“‹

#### CDF _change_type å€¼èªªæ˜
| _change_type | èªªæ˜ |
|--------------|------|
| `insert` | æ–°æ’å…¥çš„è¨˜éŒ„ |
| `update_preimage` | æ›´æ–°å‰çš„è¨˜éŒ„ç‹€æ…‹ |
| `update_postimage` | æ›´æ–°å¾Œçš„è¨˜éŒ„ç‹€æ…‹ |
| `delete` | è¢«åˆªé™¤çš„è¨˜éŒ„ |

#### å®Œæ•´åŸ·è¡Œæµç¨‹
```
customers è¡¨æ ¼è®Šæ›´æ­·å²ï¼š
â”œâ”€â”€ Version 1: INSERT row_a
â”œâ”€â”€ Version 2: UPDATE row_a â†’ row_a'
â”œâ”€â”€ Version 3: UPDATE row_a' â†’ row_a''
â””â”€â”€ Version 4: INSERT row_b

ç¬¬ä¸€æ¬¡åŸ·è¡Œï¼ˆå¾ version 0 é–‹å§‹ï¼‰ï¼š
â”œâ”€â”€ è®€å– CDFï¼šversion 1-4 çš„æ‰€æœ‰è®Šæ›´
â”œâ”€â”€ éæ¿¾ update_postimageï¼šrow_a', row_a''
â”œâ”€â”€ Append åˆ° customers_updates
â””â”€â”€ Checkpoint è¨˜éŒ„ï¼šå·²è™•ç†åˆ° version 4

ç¬¬äºŒæ¬¡åŸ·è¡Œï¼š
â”œâ”€â”€ å¾ checkpoint è®€å–ï¼šå·²è™•ç†åˆ° version 4
â”œâ”€â”€ è‹¥æœ‰ version 5 çš„è®Šæ›´ï¼Œåªè™•ç† version 5
â”œâ”€â”€ éæ¿¾ update_postimage
â””â”€â”€ Append æ–°è¨˜éŒ„ï¼ˆä¸é‡è¤‡ version 1-4 çš„è³‡æ–™ï¼‰
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A. Newly updated records will overwrite the target table.
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- æ²’æœ‰æŒ‡å®š `outputMode("complete")` æˆ–ä½¿ç”¨ `mode("overwrite")`
- é è¨­æ˜¯ **append** æ¨¡å¼ï¼Œä¸æœƒè¦†è“‹
- Streaming å¯«å…¥ä¸æ”¯æ´ç›´æ¥ overwrite

### B. The entire history of updated records will be appended...duplicate entries.
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **Checkpoint æœƒè¿½è¹¤è™•ç†é€²åº¦**
- æ¯æ¬¡åŸ·è¡Œåªè™•ç†ã€Œæœªè™•ç†éçš„ã€æ–°è®Šæ›´
- ä¸æœƒé‡è¤‡è®€å–å·²è™•ç†çš„æ­·å²è¨˜éŒ„

### D. The entire history of updated records will overwrite...
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- åŒæ¨£éŒ¯èª¤ç†è§£äº† checkpoint æ©Ÿåˆ¶
- ä¸æœƒæ¯æ¬¡éƒ½è®€å–å®Œæ•´æ­·å²
- ä¸”æ²’æœ‰ overwrite æ¨¡å¼

---

## ğŸ§  è¨˜æ†¶æ³•
### å£è¨£
**ã€ŒCheckpoint è¨˜é€²åº¦ï¼ŒAppend è¿½åŠ ä¸è¦†è“‹ï¼›æ¯æ¬¡åªè™•ç†æ–°çš„ï¼Œä¸æœƒé‡è¤‡èˆŠè³‡æ–™ã€**

### Streaming å¯«å…¥æ¨¡å¼æ¯”è¼ƒ
| æ¨¡å¼ | èªªæ˜ | é©ç”¨å ´æ™¯ |
|------|------|---------|
| **append** (é è¨­) | è¿½åŠ æ–°è¨˜éŒ„ | å¢é‡è¼‰å…¥ |
| **complete** | å®Œæ•´è¦†è“‹ | èšåˆçµæœ |
| **update** | æ›´æ–°å·²è®Šæ›´çš„è¨˜éŒ„ | å»é‡æˆ–æ›´æ–° |

### CDF + Streaming é—œéµè¦ç´ 
```python
spark.readStream
    .option("readChangeFeed", "true")  # å•Ÿç”¨ CDF
    .option("startingVersion", 0)       # èµ·å§‹ç‰ˆæœ¬
    .table("source")
    .filter(...)                        # éæ¿¾ change_type
.writeStream
    .option("checkpointLocation", "...")  # é—œéµï¼è¿½è¹¤é€²åº¦
    .trigger(availableNow=True)           # è™•ç†å®Œåœæ­¢
    .table("target")
```

### å¸¸è¦‹èª¤è§£æ¾„æ¸…
| èª¤è§£ | å¯¦éš›æƒ…æ³ |
|------|---------|
| æ¯æ¬¡éƒ½å¾é ­è®€å– | Checkpoint ç¢ºä¿å¢é‡è®€å– |
| startingVersion æ¯æ¬¡éƒ½ç”Ÿæ•ˆ | åªæœ‰ç¬¬ä¸€æ¬¡ç”Ÿæ•ˆï¼Œä¹‹å¾Œç”¨ checkpoint |
| æ²’æœ‰ checkpoint ä¹Ÿèƒ½å¢é‡ | æ²’æœ‰ checkpoint æœƒé‡è¤‡è™•ç† |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶
- [Change Data Feed](https://docs.databricks.com/en/delta/delta-change-data-feed.html)
- [Structured Streaming checkpointing](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing)

---

**[è¿”å›é¡Œç›®](#question-054)**
