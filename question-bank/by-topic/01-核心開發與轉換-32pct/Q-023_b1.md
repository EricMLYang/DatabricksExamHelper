# Question #023

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-023`

### ä¾†æº
**ä¾†æº:** Sample / Batch 1

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
A data engineer is cleaning a DataFrame orders and wants to calculate new columns using chained transformations:

```python
def normalize_email(df):
    return df.withColumn("email", col("email").lower())

def calculate_total(df):
    return df.withColumn("total_amount", col("quantity") * col("unit_price"))

orders_transformed = orders.transform(normalize_email).transform(calculate_total)
```

What is the key advantage of using the transform function in this scenario?

### é¸é …
- **A.** It converts the DataFrame to an RDD before transformations.

- **B.** It ensures transformations run in parallel on multiple nodes automatically.

- **C.** It allows for modular, composable, and testable transformations.

- **D.** It automatically caches the DataFrame after each transformation.

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `PySpark`, `DataFrames`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Data Transformation

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** C
- **ç¤¾ç¾¤å…±è­˜:** C

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** PySpark DataFrame transform() æ–¹æ³•

**é—œéµæ¦‚å¿µ:**
- transform() çš„è¨­è¨ˆæ¨¡å¼å„ªå‹¢
- æ¨¡çµ„åŒ–ç¨‹å¼è¨­è¨ˆ
- å‡½æ•¸çµ„åˆï¼ˆFunction Compositionï¼‰

**é¡Œç›®é—œéµå­—ï¼š**
- **transform()**: DataFrame çš„éˆå¼è½‰æ›æ–¹æ³•
- **chained transformations**: é€£é–è½‰æ›
- **modular, composable, testable**: æ¨¡çµ„åŒ–ã€å¯çµ„åˆã€å¯æ¸¬è©¦

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ C æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

`transform()` æ–¹æ³•çš„ä¸»è¦å„ªå‹¢åœ¨æ–¼**ç¨‹å¼ç¢¼çµ„ç¹”**å’Œ**è»Ÿé«”å·¥ç¨‹æœ€ä½³å¯¦è¸**ï¼Œè€ŒéåŸ·è¡Œæ•ˆèƒ½ã€‚å®ƒè®“ä½ å¯ä»¥å°‡è½‰æ›é‚è¼¯å°è£æˆç¨ç«‹å‡½æ•¸ï¼Œå¯¦ç¾æ¨¡çµ„åŒ–ã€å¯çµ„åˆã€å¯æ¸¬è©¦çš„ç¨‹å¼ç¢¼çµæ§‹ã€‚

#### transform() çš„ä¸‰å¤§å„ªå‹¢

| å„ªå‹¢ | èªªæ˜ |
|------|------|
| **æ¨¡çµ„åŒ– (Modular)** | æ¯å€‹è½‰æ›å‡½æ•¸ç¨ç«‹å°è£ï¼Œå–®ä¸€è·è²¬ |
| **å¯çµ„åˆ (Composable)** | å‡½æ•¸å¯ä»¥ä»»æ„çµ„åˆã€é‡æ’åº |
| **å¯æ¸¬è©¦ (Testable)** | æ¯å€‹å‡½æ•¸å¯ä»¥ç¨ç«‹å–®å…ƒæ¸¬è©¦ |

#### å°æ¯”ï¼šæœ‰ transform() vs æ²’æœ‰ transform()

```python
# âŒ æ²’æœ‰ transform() - ä¸å¥½ç¶­è­·
orders_transformed = (orders
    .withColumn("email", col("email").lower())
    .withColumn("total_amount", col("quantity") * col("unit_price"))
    .withColumn("tax", col("total_amount") * 0.1)
    .withColumn("final_total", col("total_amount") + col("tax"))
    # æ›´å¤šè½‰æ›...
)

# âœ… ä½¿ç”¨ transform() - æ¨¡çµ„åŒ–ã€å¯é‡ç”¨
def normalize_email(df):
    return df.withColumn("email", col("email").lower())

def calculate_total(df):
    return df.withColumn("total_amount", col("quantity") * col("unit_price"))

def add_tax(df, rate=0.1):
    return df.withColumn("tax", col("total_amount") * rate)

def calculate_final(df):
    return df.withColumn("final_total", col("total_amount") + col("tax"))

# æ¸…æ™°çš„è½‰æ›æµç¨‹
orders_transformed = (orders
    .transform(normalize_email)
    .transform(calculate_total)
    .transform(add_tax)
    .transform(calculate_final)
)
```

#### å¯æ¸¬è©¦æ€§ç¯„ä¾‹

```python
import pytest
from pyspark.sql import SparkSession

def test_normalize_email():
    spark = SparkSession.builder.getOrCreate()

    # æº–å‚™æ¸¬è©¦è³‡æ–™
    test_df = spark.createDataFrame([
        ("John", "JOHN@EMAIL.COM"),
        ("Jane", "Jane@Email.com")
    ], ["name", "email"])

    # åŸ·è¡Œè½‰æ›
    result_df = test_df.transform(normalize_email)

    # é©—è­‰çµæœ
    results = result_df.collect()
    assert results[0]["email"] == "john@email.com"
    assert results[1]["email"] == "jane@email.com"
```

#### å¯çµ„åˆæ€§ç¯„ä¾‹

```python
# ä¸åŒå ´æ™¯å¯ä»¥çµ„åˆä¸åŒçš„è½‰æ›
def process_us_orders(df):
    return (df
        .transform(normalize_email)
        .transform(calculate_total)
        .transform(add_us_tax)
    )

def process_eu_orders(df):
    return (df
        .transform(normalize_email)
        .transform(calculate_total)
        .transform(add_eu_vat)
    )
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A
**It converts the DataFrame to an RDD before transformations.**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **transform() ä¸æœƒè½‰æ›ç‚º RDD**ï¼š
   - transform() æ¥å—ä¸€å€‹å‡½æ•¸ï¼Œè©²å‡½æ•¸æ¥æ”¶ DataFrame ä¸¦è¿”å› DataFrame
   - å…¨ç¨‹éƒ½æ˜¯ DataFrame æ“ä½œ

2. **DataFrame vs RDD**ï¼š
```python
# transform() ä¿æŒ DataFrame é¡å‹
result = df.transform(my_func)  # result ä»ç„¶æ˜¯ DataFrame

# è½‰æ›ç‚º RDD éœ€è¦æ˜ç¢ºèª¿ç”¨
rdd = df.rdd  # é€™æ‰æœƒè½‰æ›ç‚º RDD
```

---

### é¸é … B
**It ensures transformations run in parallel on multiple nodes automatically.**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **å¹³è¡ŒåŸ·è¡Œæ˜¯ Spark çš„åŸºæœ¬ç‰¹æ€§ï¼Œä¸æ˜¯ transform() ç‰¹æœ‰çš„**ï¼š
   - æ‰€æœ‰ DataFrame æ“ä½œéƒ½æœƒè‡ªå‹•å¹³è¡ŒåŸ·è¡Œ
   - ç„¡è«–æ˜¯å¦ä½¿ç”¨ transform()ï¼ŒSpark éƒ½æœƒåˆ†æ•£åˆ°å¤šå€‹ç¯€é»

2. **åŸ·è¡Œè¨ˆç•«ç›¸åŒ**ï¼š
```python
# é€™å…©ç¨®å¯«æ³•çš„åŸ·è¡Œè¨ˆç•«å®Œå…¨ç›¸åŒ
df.withColumn("a", col("b") * 2)
df.transform(lambda df: df.withColumn("a", col("b") * 2))
```

---

### é¸é … D
**It automatically caches the DataFrame after each transformation.**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **transform() ä¸æœƒè‡ªå‹•å¿«å–**ï¼š
   - Spark éµå¾ª Lazy Evaluation
   - å¿«å–éœ€è¦æ˜ç¢ºèª¿ç”¨ `.cache()` æˆ– `.persist()`

2. **æ­£ç¢ºçš„å¿«å–æ–¹å¼**ï¼š
```python
# transform() ä¸æœƒè‡ªå‹•å¿«å–
result = df.transform(my_func)  # æ²’æœ‰å¿«å–

# éœ€è¦æ˜ç¢ºå¿«å–
result = df.transform(my_func).cache()  # æ˜ç¢ºå¿«å–
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€Œtransform ç®¡çµ„ç¹”ï¼ŒåŸ·è¡Œé  Spark è‡ªå·±ã€**
â†’ transform() æ˜¯ç¨‹å¼ç¢¼çµ„ç¹”å·¥å…·ï¼ŒåŸ·è¡Œæ•ˆèƒ½ç”± Spark å¼•æ“è™•ç†

### transform() vs ç›´æ¥èª¿ç”¨æ–¹æ³•

| ç‰¹æ€§ | transform() | ç›´æ¥èª¿ç”¨ |
|------|-------------|---------|
| **åŸ·è¡Œæ•ˆèƒ½** | ç›¸åŒ | ç›¸åŒ |
| **å¹³è¡Œè™•ç†** | ç›¸åŒ | ç›¸åŒ |
| **ç¨‹å¼ç¢¼çµ„ç¹”** | âœ… æ¨¡çµ„åŒ– | âš ï¸ å¯èƒ½æ··äº‚ |
| **å¯æ¸¬è©¦æ€§** | âœ… å®¹æ˜“æ¸¬è©¦ | âš ï¸ é›£ä»¥éš”é›¢ |
| **å¯é‡ç”¨æ€§** | âœ… é«˜ | âš ï¸ ä½ |

### æœ€ä½³å¯¦è¸

```python
# å»ºè­°çš„å‡½æ•¸ç°½å
def my_transformation(df: DataFrame) -> DataFrame:
    """
    è½‰æ›å‡½æ•¸æ‡‰è©²ï¼š
    1. æ¥å— DataFrame ä½œç‚ºè¼¸å…¥
    2. è¿”å› DataFrame ä½œç‚ºè¼¸å‡º
    3. ä¸ç”¢ç”Ÿå‰¯ä½œç”¨ï¼ˆç´”å‡½æ•¸ï¼‰
    4. æœ‰æ¸…æ™°çš„å‘½åè¡¨é”ç”¨é€”
    """
    return df.withColumn(...)

# å¯ä»¥æ·»åŠ åƒæ•¸
def add_tax(df: DataFrame, rate: float = 0.1) -> DataFrame:
    return df.withColumn("tax", col("amount") * rate)

# ä½¿ç”¨ lambda å‚³éåƒæ•¸
df.transform(lambda df: add_tax(df, rate=0.15))
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [PySpark DataFrame.transform()](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.transform.html)
- [Chained Transformations](https://spark.apache.org/docs/latest/sql-programming-guide.html)

---

**[è¿”å›é¡Œç›®](#question-023)**
