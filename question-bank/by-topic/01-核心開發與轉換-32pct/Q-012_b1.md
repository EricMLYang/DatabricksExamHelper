# Question #012

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-012`
### ä¾†æº
**ä¾†æº:** Sample
### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
A data engineer has the following streaming query with a blank:

```python
spark.readStream
    .table("orders_cleaned")
    ____________________________
    .groupBy(
        "order_timestamp",
        "author")
    .agg(
        count("order_id").alias("orders_count"),
        avg("quantity").alias("avg_quantity"))
    .writeStream
    .option("checkpointLocation", "dbfs:/path/checkpoint")
    .table("orders_stats")
```

For handling late-arriving data, they want to maintain the streaming state information for 30 minutes.

Which option correctly fills in the blank to meet this requirement?

### é¸é …
- **A.** `.trigger(processingTime="30 minutes")`
- **B.** `.withWatermark("order_timestamp", "30 minutes")`
- **C.** `.awaitWatermark("order_timestamp", "30 minutes")`
- **D.** `.window("order_timestamp", "30 minutes")`

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `Streaming-Stateful`
### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Similar-Function`, `Command-Purpose`
### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Streaming

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B`
### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** B
- **ç¤¾ç¾¤å…±è­˜:** B

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥
**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming Watermark Mechanism
**é—œéµæ¦‚å¿µ:** Late-arriving Data, Stateful Processing, Watermark
**é¡Œç›®é—œéµå­—ï¼š**
- **Late-arriving data**: å»¶é²åˆ°é”çš„è³‡æ–™
- **Maintain streaming state**: ç¶­è­·ä¸²æµç‹€æ…‹è³‡è¨Šï¼ˆç”¨æ–¼ aggregationï¼‰
- **30 minutes**: Watermark çš„å®¹å¿æ™‚é–“çª—å£
- **groupBy + agg**: ç‹€æ…‹èšåˆæ“ä½œï¼Œéœ€è¦ Watermark ç®¡ç†ç‹€æ…‹

---

## âœ… æ­£è§£èªªæ˜
### ç‚ºä»€éº¼ B æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

åœ¨ Structured Streaming çš„**ç‹€æ…‹èšåˆ**ï¼ˆstateful aggregationï¼‰ä¸­ï¼Œç³»çµ±éœ€è¦è¿½è¹¤æ­·å²ç‹€æ…‹ä»¥è¨ˆç®—èšåˆçµæœã€‚ä½†ç„¡é™åˆ¶åœ°ä¿ç•™ç‹€æ…‹æœƒå°è‡´è¨˜æ†¶é«”çˆ†ç‚¸ã€‚

**Watermark çš„ä½œç”¨ï¼š**

```python
.withWatermark("order_timestamp", "30 minutes")
```

æ­¤è¨­å®šè¡¨ç¤ºï¼š
1. **å®¹å¿å»¶é² 30 åˆ†é˜**ï¼šç³»çµ±æœƒä¿ç•™ 30 åˆ†é˜å…§å¯èƒ½åˆ°é”çš„è³‡æ–™ç‹€æ…‹
2. **è‡ªå‹•æ¸…ç†éæœŸç‹€æ…‹**ï¼šè¶…é 30 åˆ†é˜çš„èˆŠè³‡æ–™ç‹€æ…‹æœƒè¢«æ¸…é™¤
3. **åŸºæ–¼äº‹ä»¶æ™‚é–“**ï¼šä½¿ç”¨ `order_timestamp`ï¼ˆevent timeï¼‰è€Œéè™•ç†æ™‚é–“

**é‹ä½œæ©Ÿåˆ¶ï¼š**

```
ç•¶å‰æœ€å¤§äº‹ä»¶æ™‚é–“ï¼š12:30
Watermark = 12:30 - 30åˆ†é˜ = 12:00

ä¿ç•™ç‹€æ…‹ï¼š12:00 ~ 12:30 çš„è³‡æ–™
ä¸Ÿæ£„è³‡æ–™ï¼š< 12:00 çš„é²åˆ°è³‡æ–™å°‡è¢«å¿½ç•¥
```

**å®Œæ•´ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š**

```python
from pyspark.sql.functions import count, avg

spark.readStream \
    .table("orders_cleaned") \
    .withWatermark("order_timestamp", "30 minutes") \
    .groupBy(
        "order_timestamp",
        "author"
    ) \
    .agg(
        count("order_id").alias("orders_count"),
        avg("quantity").alias("avg_quantity")
    ) \
    .writeStream \
    .option("checkpointLocation", "dbfs:/path/checkpoint") \
    .outputMode("append")  # æˆ– "update"
    .table("orders_stats")
```

**ç‚ºä»€éº¼å¿…é ˆä½¿ç”¨ Watermarkï¼Ÿ**
- âœ… **ç‹€æ…‹ç®¡ç†**ï¼šé™åˆ¶éœ€è¦ä¿ç•™çš„ç‹€æ…‹æ•¸é‡
- âœ… **è¨˜æ†¶é«”æ§åˆ¶**ï¼šé˜²æ­¢ç‹€æ…‹ç„¡é™å¢é•·
- âœ… **è™•ç†å»¶é²**ï¼šå…è¨±ä¸€å®šæ™‚é–“å…§çš„é²åˆ°è³‡æ–™
- âœ… **è‡ªå‹•æ¸…ç†**ï¼šéæœŸè³‡æ–™è‡ªå‹•å¾ç‹€æ…‹ä¸­ç§»é™¤

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A. `.trigger(processingTime="30 minutes")`
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **ç”¨é€”ä¸åŒ**ï¼š`trigger` æ§åˆ¶çš„æ˜¯**æ‰¹æ¬¡è™•ç†é »ç‡**ï¼Œè€Œéç‹€æ…‹ä¿ç•™æ™‚é–“
- **æ¦‚å¿µæ··æ·†**ï¼šè™•ç†é »ç‡ â‰  å»¶é²å®¹å¿åº¦

```python
# trigger çš„å¯¦éš›ç”¨é€”
.trigger(processingTime="30 minutes")  # æ¯ 30 åˆ†é˜è§¸ç™¼ä¸€æ¬¡æ‰¹æ¬¡è™•ç†
```

**è§¸ç™¼é¡å‹æ¯”è¼ƒï¼š**
- `trigger(processingTime="30 minutes")` - æ¯ 30 åˆ†é˜è™•ç†ä¸€æ¬¡
- `trigger(once=True)` - åªè™•ç†ä¸€æ¬¡
- `trigger(availableNow=True)` - è™•ç†æ‰€æœ‰å¯ç”¨è³‡æ–™å¾Œåœæ­¢

### C. `.awaitWatermark("order_timestamp", "30 minutes")`
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **ä¸å­˜åœ¨çš„æ–¹æ³•**ï¼šSpark æ²’æœ‰ `awaitWatermark` é€™å€‹ API
- **é™·é˜±è¨­è¨ˆ**ï¼šæ•…æ„é€ ä¸€å€‹çœ‹èµ·ä¾†åˆç†ä½†ä¸å­˜åœ¨çš„æ–¹æ³•å

### D. `.window("order_timestamp", "30 minutes")`
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **ç”¨é€”ä¸åŒ**ï¼š`window()` ç”¨æ–¼å®šç¾©**æ™‚é–“è¦–çª—èšåˆ**ï¼ˆwindowing aggregationï¼‰ï¼Œè€Œéç‹€æ…‹ä¿ç•™æ™‚é–“
- **ä½ç½®éŒ¯èª¤**ï¼š`window()` æ‡‰è©²åœ¨ `groupBy()` å…§ä½¿ç”¨

```python
# window() çš„æ­£ç¢ºç”¨æ³•
.groupBy(
    window("order_timestamp", "30 minutes"),  # å®šç¾© 30 åˆ†é˜çš„æ™‚é–“çª—å£
    "author"
)
```

**é—œéµå·®ç•°ï¼š**
- `window()` - å®šç¾©èšåˆçš„æ™‚é–“çª—å£å¤§å°
- `withWatermark()` - å®šç¾©å®¹å¿å»¶é²çš„æ™‚é–“

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€ŒWatermark ç®¡ç‹€æ…‹ï¼ŒTrigger ç®¡é »ç‡ï¼ŒWindow ç®¡çª—å£ã€**

### æ¯”è¼ƒè¡¨ï¼šStreaming æ™‚é–“ç›¸é—œæ¦‚å¿µ

| æ–¹æ³• | ç”¨é€” | å½±éŸ¿å°è±¡ | åƒæ•¸å«ç¾© | ä½¿ç”¨ä½ç½® |
|------|------|---------|---------|---------|
| **withWatermark()** | ğŸ¯ **ç‹€æ…‹ä¿ç•™æ™‚é–“** | è¨˜æ†¶é«”ç®¡ç† | å®¹å¿å»¶é²çš„æ™‚é–“ | readStream ä¹‹å¾Œ |
| **trigger()** | æ‰¹æ¬¡åŸ·è¡Œé »ç‡ | è™•ç†é€Ÿåº¦ | å¤šä¹…è™•ç†ä¸€æ¬¡ | writeStream ä¹‹å‰ |
| **window()** | æ™‚é–“çª—å£å®šç¾© | èšåˆç¯„åœ | çª—å£çš„å¤§å° | groupBy() å…§ |

### è¦–è¦ºåŒ–å°æ¯”

```
æ™‚é–“è»¸: 12:00 â”€â”€â†’ 12:30 â”€â”€â†’ 13:00

withWatermark("timestamp", "30min"):
    ä¿ç•™ç‹€æ…‹ [12:00 ~ 13:00]
    ä¸Ÿæ£„ < 12:00 çš„é²åˆ°è³‡æ–™

trigger(processingTime="30min"):
    12:00 åŸ·è¡Œ â†’ 12:30 åŸ·è¡Œ â†’ 13:00 åŸ·è¡Œ
    
window("timestamp", "30min"):
    [12:00-12:30] [12:30-13:00] [13:00-13:30]
    æ¯å€‹çª—å£ç¨ç«‹èšåˆ
```

### è¨˜æ†¶é»
- **Watermark = æ°´ä½ç·š**ï¼šæƒ³åƒæ°´ä½ç·šä¸‹çš„è³‡æ–™æœƒè¢«æ²–èµ°ï¼ˆæ¸…é™¤ç‹€æ…‹ï¼‰
- **Trigger = è§¸ç™¼å™¨**ï¼šè¨­å®šå¤šä¹…ã€Œæ‹‰ä¸€æ¬¡æ‰³æ©Ÿã€åŸ·è¡Œè™•ç†
- **Window = çª—æˆ¶**ï¼šæŠŠæ™‚é–“åˆ‡æˆä¸€å€‹å€‹çª—å£ä¾†çœ‹

### å®Œæ•´ç¯„ä¾‹å°æ¯”

```python
# æ­£ç¢ºçµ„åˆ
spark.readStream \
    .table("orders") \
    .withWatermark("timestamp", "30 minutes") \  # å®¹å¿ 30 åˆ†é˜å»¶é²
    .groupBy(
        window("timestamp", "10 minutes"),       # æ¯ 10 åˆ†é˜ä¸€å€‹çª—å£
        "author"
    ) \
    .agg(count("*")) \
    .writeStream \
    .trigger(processingTime="5 minutes") \       # æ¯ 5 åˆ†é˜è™•ç†ä¸€æ¬¡
    .start()
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶
- [withWatermark - PySpark Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withWatermark.html)
- [Handling Late Data and Watermarking](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)

---

**[è¿”å›é¡Œç›®](#question-012)**
