# é¡Œç›® Q-078

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-078`

### ä¾†æº
**ä¾†æº:** Mock Exam (ç¬¬ä¸‰æ–¹æ¨¡æ“¬è©¦é¡Œ)

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

### æ¨™ç±¤
**Topics:** `PySpark`, `DataFrames`, `ETL-Patterns`  
**Traps:** `Concept-Confusion`, `Execution-Behavior`  
**Level:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

**Question #78** examines the behavior of a standard PySpark batch aggregation and overwrite operation within a data pipeline.

The data engineering team maintains the following code:

```python
import pyspark.sql.functions as F

(spark.table("silver_customer_sales")
    .groupBy("customer_id")
    .agg(
        F.min("sale_date").alias("first_transaction_date"),
        F.max("sale_date").alias("last_transaction_date"),
        F.mean("sale_total").alias("average_sales"),
        F.countDistinct("order_id").alias("total_orders"),
        F.sum("sale_total").alias("lifetime_value")
    ).write
    .mode("overwrite")
    .table("gold_customer_lifetime_sales_summary")
)
```

Assuming that this code produces logically correct results and the data in the source table has been de-duplicated and validated, which statement describes what will occur when this code is executed?

### é¸é …

**A.** The `silver_customer_sales` table will be overwritten by aggregated values calculated from all records in the `gold_customer_lifetime_sales_summary` table as a batch job.

**B.** A batch job will update the `gold_customer_lifetime_sales_summary` table, replacing only those rows that have different values than the current version of the table, using `customer_id` as the primary key.

**C.** The `gold_customer_lifetime_sales_summary` table will be overwritten by aggregated values calculated from all records in the `silver_customer_sales` table as a batch job.

**D.** An incremental job will leverage running information in the state store to update aggregate values in the `gold_customer_lifetime_sales_summary` table.

**E.** An incremental job will detect if new rows have been written to the `silver_customer_sales` table; if new rows are detected, all aggregates will be recalculated and used to overwrite the `gold_customer_lifetime_sales_summary` table.

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`

---

## ğŸ“ è€ƒé»è­˜åˆ¥

### ä¸»è¦è€ƒé»
**æ ¸å¿ƒæŠ€è¡“:** PySpark DataFrame API + Batch Processing  
**çŸ¥è­˜é ˜åŸŸ:** æ ¸å¿ƒé–‹ç™¼èˆ‡è½‰æ› - è³‡æ–™èšåˆèˆ‡è½‰æ›  
**é—œéµæ¦‚å¿µ:** batch vs streamingã€overwrite modeã€Medallion Architecture

### æ¬¡è¦è€ƒé»
- PySpark èšåˆå‡½æ•¸ä½¿ç”¨ (min, max, mean, countDistinct, sum)
- DataFrame write modes çš„è¡Œç‚ºå·®ç•°
- Silver to Gold layer è³‡æ–™æµè½‰

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼é¸æ“‡ C

**Option C æ­£ç¢ºçš„åŸå› ï¼š**

1. **æ‰¹æ¬¡è™•ç† (Batch Processing)**: 
   - ç¨‹å¼ç¢¼ä½¿ç”¨ `spark.table("silver_customer_sales")` é€²è¡Œ**éœæ…‹/æ‰¹æ¬¡è®€å–**
   - å¦‚æœæ˜¯ä¸²æµæˆ–å¢é‡ä½œæ¥­ï¼Œèªæ³•éœ€è¦ä½¿ç”¨ `spark.readStream`

2. **å®Œå…¨è¦†å¯« (Complete Overwrite)**:
   - ä½¿ç”¨ **`.mode("overwrite")`** æ¨¡å¼
   - åœ¨ Databricks å’Œ Spark ä¸­ï¼Œæ­¤æ¨¡å¼è¡¨ç¤ºç›®æ¨™è¡¨æ ¼ (`gold_customer_lifetime_sales_summary`) æœƒè¢«ç•¶å‰æŸ¥è©¢çµæœ**å®Œå…¨å–ä»£**

3. **ç„¡å¢é‡ç‰¹æ€§**:
   - æ²’æœ‰æåˆ°ä¸²æµè§¸ç™¼å™¨ (streaming trigger) æˆ–ç‹€æ…‹å­˜å„² (state store)
   - ä¸æœƒã€Œåµæ¸¬ã€æ–°è³‡æ–™åˆ—ï¼Œè€Œæ˜¯å°**æ•´å€‹ä¾†æºè¡¨æ ¼**é‡æ–°è¨ˆç®—èšåˆ

4. **è³‡æ–™åˆ†å±¤æ¶æ§‹**:
   - ä»£è¡¨ Medallion Architecture ä¸­å¾ **Silver å±¤**ï¼ˆå·²æ¸…ç†/é©—è­‰è³‡æ–™ï¼‰åˆ° **Gold å±¤**ï¼ˆèšåˆæ¥­å‹™æŒ‡æ¨™ï¼‰çš„è½‰æ›
   - Gold å±¤é€šå¸¸ç‚ºã€Œèšåˆè¡¨æ ¼ã€ï¼Œç”¨æ–¼æœå‹™ BI å„€è¡¨æ¿çš„é è¨ˆç®— KPI

### æŠ€è¡“åŸ·è¡Œæµç¨‹

```python
# å¯¦éš›åŸ·è¡Œæ­¥é©Ÿï¼š
1. è®€å–æ•´å€‹ silver_customer_sales è¡¨æ ¼ (æ‰¹æ¬¡è®€å–)
2. æŒ‰ customer_id åˆ†çµ„ä¸¦è¨ˆç®—å„ç¨®èšåˆæŒ‡æ¨™
3. å®Œå…¨æ›¿æ› gold_customer_lifetime_sales_summary è¡¨æ ¼å…§å®¹
4. æ¯æ¬¡åŸ·è¡Œéƒ½æœƒé‡æ–°è¨ˆç®—æ‰€æœ‰å®¢æˆ¶çš„çµ‚ç”Ÿåƒ¹å€¼çµ±è¨ˆ
```

---

## âŒ éŒ¯èª¤é¸é …åˆ†æ

### Option A - è³‡æ–™æµå‘éŒ¯èª¤
```
silver_customer_sales table will be overwritten...
```
**å•é¡Œï¼š**
- æéŒ¯äº†è³‡æ–™æµå‘ï¼šæ˜¯ Silver â†’ Goldï¼Œä¸æ˜¯ Gold â†’ Silver
- `silver_customer_sales` æ˜¯ä¾†æºè¡¨æ ¼ï¼Œä¸æœƒè¢«è¦†å¯«

### Option B - èª¤è§£ç‚ºéƒ¨åˆ†æ›´æ–°
```
replacing only those rows that have different values...
```
**å•é¡Œï¼š**
- `.mode("overwrite")` æ˜¯**å®Œå…¨è¦†å¯«**ï¼Œä¸æ˜¯éƒ¨åˆ†æ›´æ–°
- å¦‚æœè¦éƒ¨åˆ†æ›´æ–°ï¼Œéœ€è¦ä½¿ç”¨ `.mode("append")` æˆ– MERGE æ“ä½œ

### Option D - èª¤è§£ç‚ºç‹€æ…‹åŒ–ä¸²æµ
```
incremental job will leverage running information in the state store...
```
**å•é¡Œï¼š**
- é€™æ˜¯æ‰¹æ¬¡ä½œæ¥­ï¼Œæ²’æœ‰ç‹€æ…‹å­˜å„² (state store) æ¦‚å¿µ
- ç‹€æ…‹å­˜å„²åªç”¨æ–¼ä¸²æµè™•ç†ä¸­çš„ç‹€æ…‹åŒ–æ“ä½œ

### Option E - èª¤è§£ç‚ºå¢é‡åµæ¸¬
```
incremental job will detect if new rows have been written...
```
**å•é¡Œï¼š**
- é€™ä¸æ˜¯å¢é‡ä½œæ¥­ï¼Œæ¯æ¬¡éƒ½è™•ç†æ•´å€‹ä¾†æºè¡¨æ ¼
- æ²’æœ‰ã€Œåµæ¸¬æ–°è³‡æ–™åˆ—ã€çš„æ©Ÿåˆ¶ï¼Œè€Œæ˜¯å®Œæ•´é‡æ–°è¨ˆç®—

---

## ğŸ§  è¨˜æ†¶æŠ€å·§

### Batch vs Streaming è­˜åˆ¥å£è¨£
```
spark.table()     â†’ æ‰¹æ¬¡è®€å– (ä¸€æ¬¡æ€§)
spark.readStream â†’ ä¸²æµè®€å– (æŒçºŒæ€§)

.write           â†’ æ‰¹æ¬¡å¯«å…¥
.writeStream     â†’ ä¸²æµå¯«å…¥
```

### Write Modes å°æ¯”è¡¨

| æ¨¡å¼ | è¡Œç‚º | é©ç”¨æƒ…å¢ƒ |
|------|------|---------|
| `overwrite` | **å®Œå…¨æ›¿æ›**ç›®æ¨™è¡¨æ ¼ | é‡æ–°è¨ˆç®—æ•´å€‹æ‘˜è¦è¡¨ |
| `append` | **è¿½åŠ **æ–°è³‡æ–™åˆ°ç¾æœ‰è¡¨æ ¼ | ç´¯ç©æ­·å²è³‡æ–™ |
| `upsert/merge` | **æ›´æ–°+æ’å…¥**ç‰¹å®šè³‡æ–™åˆ— | å¢é‡æ›´æ–°ç‰¹å®šè¨˜éŒ„ |

### Medallion Architecture å±¤ç´š
```
Bronze (Raw) â†’ Silver (Cleaned) â†’ Gold (Aggregated)
              â†— è³‡æ–™æ¸…ç†        â†— æ¥­å‹™æŒ‡æ¨™è¨ˆç®—
```

---

## ğŸ“š å»¶ä¼¸å­¸ç¿’

### PySpark èšåˆå‡½æ•¸è©³è§£

1. **æ™‚é–“èšåˆ**
```python
F.min("sale_date")  # æœ€æ—©äº¤æ˜“æ—¥æœŸ
F.max("sale_date")  # æœ€æ™šäº¤æ˜“æ—¥æœŸ
```

2. **æ•¸å€¼èšåˆ**
```python
F.mean("sale_total")        # å¹³å‡éŠ·å”®é¡
F.sum("sale_total")         # ç¸½éŠ·å”®é¡ (çµ‚ç”Ÿåƒ¹å€¼)
F.countDistinct("order_id") # ä¸é‡è¤‡è¨‚å–®æ•¸é‡
```

3. **é€²éšèšåˆ**
```python
# ç™¾åˆ†ä½æ•¸
F.percentile_approx("sale_total", 0.5).alias("median_sales")

# æ¢ä»¶èšåˆ
F.sum(F.when(F.col("sale_total") > 1000, 1).otherwise(0)).alias("big_orders")
```

### Write Mode æœ€ä½³å¯¦è¸

1. **Overwrite ä½¿ç”¨æ™‚æ©Ÿ**
```python
# é©ç”¨ï¼šé‡æ–°è¨ˆç®—æ•´å€‹æ‘˜è¦è¡¨
.mode("overwrite")  # æ¯æ—¥é‡ç®—å®¢æˆ¶çµ±è¨ˆ
```

2. **Append ä½¿ç”¨æ™‚æ©Ÿ**
```python
# é©ç”¨ï¼šç´¯ç©æ­·å²äº¤æ˜“è¨˜éŒ„
.mode("append")     # æ–°å¢ç•¶æ—¥äº¤æ˜“
```

3. **éŒ¯èª¤è™•ç†**
```python
# å»ºè­°åŠ ä¸Šåˆ†å‰²æ§åˆ¶
.partitionBy("year", "month")  # é¿å…å°æª”æ¡ˆå•é¡Œ
.option("overwriteSchema", "true")  # å…è¨± schema æ¼”åŒ–
```

### æ•ˆèƒ½å„ªåŒ–å»ºè­°

1. **é©ç•¶çš„åˆ†å‰²ç­–ç•¥**
```python
# æŒ‰æ™‚é–“åˆ†å‰²ä»¥æå‡æŸ¥è©¢æ•ˆèƒ½
.partitionBy("year", "month")
```

2. **å¿«å–ä¸­é–“çµæœ**
```python
# è¤‡é›œèšåˆå‰å…ˆå¿«å–
df_silver = spark.table("silver_customer_sales").cache()
```

### å®˜æ–¹æ–‡ä»¶é€£çµ

- [PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)
- [Spark SQL Write Modes](https://spark.apache.org/docs/latest/sql-data-sources-load-save-functions.html#save-modes)
- [Medallion Architecture](https://docs.databricks.com/lakehouse/medallion.html)