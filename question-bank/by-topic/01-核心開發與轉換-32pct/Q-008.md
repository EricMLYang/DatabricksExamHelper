# Question #8

---

## 題目資訊

### 題目編號
**ID:** `Q-01-008`

### 來源
**來源:** Real Exam Recall

### 難度等級
**難度:** `L3-Advanced`

---

## 題目內容

### 題幹

An upstream source writes Parquet data as hourly batches to directories named with the current date. A nightly batch job runs the following code to ingest all data from the previous day as indicated by the date variable:

```python
(spark.read
    .format("parquet")
    .load(f"/mnt/raw_orders/{date}")
    .dropDuplicates(["customer_id", "order_id"])
    .write
    .mode("append")
    .saveAsTable("orders")
)
```

Assume that the fields customer_id and order_id serve as a composite key to uniquely identify each order. If the upstream system is known to occasionally produce duplicate entries for a single order hours apart, which statement is correct?

### 選項

- **A.** Each write to the orders table will only contain unique records, and only those records without duplicates in the target table will be written.
- **B.** Each write to the orders table will only contain unique records, but newly written records may have duplicates already present in the target table.
- **C.** Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, these records will be overwritten.
- **D.** Each write to the orders table will only contain unique records; if existing records with the same key are present in the target table, the operation will fail.
- **E.** Each write to the orders table will run deduplication over the union of new and existing records, ensuring no duplicate records are present.

---

## 標籤系統

### Topic Tags (技術主題標籤)
**Topics:** `Deduplication`, `Delta-Lake`, `Data-Quality`, `ETL`

### Trap Tags (陷阱類型標籤)
**Traps:** `Scope-of-Operation`, `dropDuplicates-Behavior`

### Knowledge Domain (知識領域)
**Domain:** `Data Engineering`

---

## 答案與解析連結

### 正確答案
**正解:** `B`

### 解析檔案
**詳細解析:** [點此查看解析](../analysis/Q-01-008-analysis.md)

---

## 相關資源

### 官方文件
- [PySpark dropDuplicates](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.dropDuplicates.html)
- [Delta Lake Merge](https://docs.databricks.com/delta/merge.html)
