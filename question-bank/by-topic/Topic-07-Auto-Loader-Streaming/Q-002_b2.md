# Question #002

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-002`

### ä¾†æº
**ä¾†æº:** Community

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A data engineer has the following streaming query with a blank:

```python
spark.readStream
       .table("orders_cleaned")
       .groupBy(
           ___________________________,
           "author")
       .agg(
           count("order_id").alias("orders_count"),
           avg("quantity").alias("avg_quantity"))
    .writeStream
       .option("checkpointLocation", "dbfs:/path/checkpoint")
       .table("orders_stats")
```

They want to calculate the orders count and average quantity for each non-overlapping 15-minute interval.

Which option correctly fills in the blank to meet this requirement?

### é¸é …

- **A.** `window("order_timestamp", "15 minutes")`
- **B.** `trigger(processingTime="15 minutes")`
- **C.** `withWatermark("order_timestamp", "15 minutes")`
- **D.** `withWindow("order_timestamp", "15 minutes")`

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `Streaming-Windowing`, `Spark-SQL`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Similar-Function`, `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Development

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `A`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** A
- **ç¤¾ç¾¤å…±è­˜:** A

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming / Window Functions
**é—œéµæ¦‚å¿µ:** Tumbling Window, Time-based Aggregation, window() å‡½æ•¸

**é¡Œç›®é—œéµå­—ï¼š**
- **non-overlapping 15-minute interval**: æŒ‡çš„æ˜¯ Tumbling Windowï¼ˆæ»¾å‹•è¦–çª—ï¼‰ï¼Œæ¯å€‹è³‡æ–™é»åªå±¬æ–¼ä¸€å€‹è¦–çª—ã€‚
- **groupBy**: éœ€è¦åœ¨ groupBy ä¸­ä½¿ç”¨è¦–çª—å‡½æ•¸ã€‚
- **Streaming query**: é€™æ˜¯ä¸²æµè™•ç†ï¼Œä½†è¦–çª—å‡½æ•¸çš„èªæ³•èˆ‡æ‰¹æ¬¡è™•ç†ç›¸åŒã€‚

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

1. **window() å‡½æ•¸çš„ç”¨é€”**:
   `pyspark.sql.functions.window` å‡½æ•¸ç”¨æ–¼å°‡æ™‚é–“æˆ³æ¬„ä½åˆ‡åˆ†æˆå›ºå®šå¤§å°çš„æ™‚é–“å€é–“ï¼ˆbucketsï¼‰ã€‚é€™æ˜¯åœ¨ Spark SQL ä¸­å¯¦ç¾æ™‚é–“è¦–çª—èšåˆçš„æ¨™æº–æ–¹å¼ã€‚

2. **Tumbling Windowï¼ˆæ»¾å‹•è¦–çª—ï¼‰**:
   ç•¶åªæŒ‡å®šä¸€å€‹æ™‚é–“åƒæ•¸æ™‚ï¼Œ`window()` æœƒå»ºç«‹ä¸é‡ç–Šçš„æ»¾å‹•è¦–çª—ã€‚ä¾‹å¦‚ `window("timestamp", "15 minutes")` æœƒç”¢ç”Ÿï¼š
   - 00:00 - 00:15
   - 00:15 - 00:30
   - 00:30 - 00:45
   - ...

**æ­£ç¢ºç¨‹å¼ç¢¼ï¼š**

```python
from pyspark.sql.functions import window, count, avg

spark.readStream
    .table("orders_cleaned")
    .groupBy(
        window("order_timestamp", "15 minutes"),  # âœ… å»ºç«‹ 15 åˆ†é˜æ»¾å‹•è¦–çª—
        "author")
    .agg(
        count("order_id").alias("orders_count"),
        avg("quantity").alias("avg_quantity"))
    .writeStream
    .option("checkpointLocation", "dbfs:/path/checkpoint")
    .table("orders_stats")
```

**window() å‡½æ•¸ç°½åï¼š**

```python
window(timeColumn, windowDuration, slideDuration=None, startTime=None)
```

- `timeColumn`: æ™‚é–“æˆ³æ¬„ä½
- `windowDuration`: è¦–çª—å¤§å°
- `slideDuration`: æ»‘å‹•é–“éš”ï¼ˆå¯é¸ï¼Œè‹¥çœç•¥å‰‡ç­‰æ–¼è¦–çª—å¤§å°ï¼Œå½¢æˆ Tumbling Windowï¼‰

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### B - `trigger(processingTime="15 minutes")`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ç”¨é€”ä¸åŒ**: `trigger` æ˜¯ `writeStream` çš„é¸é …ï¼Œç”¨æ–¼æ§åˆ¶å¾®æ‰¹æ¬¡çš„åŸ·è¡Œé »ç‡ï¼Œè€Œä¸æ˜¯è³‡æ–™çš„è¦–çª—åˆ†çµ„ã€‚
- **èªæ³•ä½ç½®éŒ¯èª¤**: `trigger` ä¸èƒ½æ”¾åœ¨ `groupBy` ä¸­ã€‚
- **åŠŸèƒ½å·®ç•°**:
  - `trigger`: æ§åˆ¶ã€Œå¤šä¹…åŸ·è¡Œä¸€æ¬¡ã€å¾®æ‰¹æ¬¡è™•ç†
  - `window`: æ§åˆ¶ã€Œå¦‚ä½•åˆ†çµ„ã€è³‡æ–™

### C - `withWatermark("order_timestamp", "15 minutes")`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ç”¨é€”ä¸åŒ**: `withWatermark` ç”¨æ–¼å®šç¾©é²åˆ°è³‡æ–™çš„å®¹å¿é–¾å€¼ï¼Œè®“ Spark çŸ¥é“ä½•æ™‚å¯ä»¥ä¸Ÿæ£„éæœŸçš„ç‹€æ…‹è³‡æ–™ã€‚
- **ä¸æ˜¯åˆ†çµ„å‡½æ•¸**: `withWatermark` æ˜¯ DataFrame çš„æ–¹æ³•ï¼Œä¸èƒ½æ”¾åœ¨ `groupBy` åƒæ•¸ä¸­ã€‚
- **å…¸å‹ç”¨æ³•**:
  ```python
  df.withWatermark("timestamp", "10 minutes")
    .groupBy(window("timestamp", "5 minutes"))
    .count()
  ```

### D - `withWindow("order_timestamp", "15 minutes")`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **å‡½æ•¸ä¸å­˜åœ¨**: Spark ä¸­æ²’æœ‰ `withWindow` é€™å€‹å‡½æ•¸ã€‚
- **æ­£ç¢ºå‡½æ•¸åç¨±**: æ‡‰è©²æ˜¯ `window()`ï¼Œä¸æ˜¯ `withWindow`ã€‚

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€ŒWindow åˆ†æ™‚æ®µï¼ŒTrigger å®šé »ç‡ï¼ŒWatermark ç®¡é²åˆ°ã€**

- **window()**: åˆ†æ™‚é–“å€æ®µï¼ˆè¦–çª—ï¼‰
- **trigger()**: è¨­å®šåŸ·è¡Œé »ç‡
- **withWatermark()**: è™•ç†é²åˆ°è³‡æ–™

### æ¯”è¼ƒè¡¨

| å‡½æ•¸ | ç”¨é€” | ä½ç½® | ç¯„ä¾‹ |
|------|------|------|------|
| `window()` | æ™‚é–“è¦–çª—åˆ†çµ„ | `groupBy()` å…§ | `window("ts", "15 min")` |
| `trigger()` | å¾®æ‰¹æ¬¡åŸ·è¡Œé »ç‡ | `writeStream` å¾Œ | `.trigger(processingTime="15 min")` |
| `withWatermark()` | é²åˆ°è³‡æ–™é–¾å€¼ | DataFrame æ–¹æ³• | `df.withWatermark("ts", "10 min")` |

### è¦–çª—é¡å‹

| è¦–çª—é¡å‹ | èªªæ˜ | window() ç”¨æ³• |
|----------|------|---------------|
| Tumbling | ä¸é‡ç–Š | `window("ts", "15 min")` |
| Sliding | é‡ç–Š | `window("ts", "15 min", "5 min")` |
| Session | åŸºæ–¼æ´»å‹• | éœ€ä½¿ç”¨ `session_window()` |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Spark Window Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.window.html)
- [Structured Streaming Windows](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time)
- [Watermarking](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#handling-late-data-and-watermarking)

---

**[è¿”å›é¡Œç›®](#question-002)**
