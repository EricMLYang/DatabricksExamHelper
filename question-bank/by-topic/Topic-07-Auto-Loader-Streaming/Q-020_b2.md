# Question #020

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-020`

### ä¾†æº
**ä¾†æº:** Community

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L3-Advanced`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A data engineer is using a foreachBatch logic to upsert data in a target Delta table.

The function to be called at each new microbatch processing is displayed below with a blank:

```python
def upsert_data(microBatchDF, batch_id):
    microBatchDF.createOrReplaceTempView("sales_microbatch")

    sql_query = """
      MERGE INTO sales_silver a
      USING sales_microbatch b
      ON a.item_id=b.item_id AND a.item_timestamp=b.item_timestamp
      WHEN NOT MATCHED THEN INSERT *
    """

    ________________
```

Which option correctly fills in the blank to execute the sql query in the function on a cluster with recent Databricks Runtime above 10.5?

### é¸é …

- **A.** `microBatchDF._jdf.sparkSession().sql(sql_query)`
- **B.** `microBatchDF.sparkSession.sql(sql_query)`
- **C.** `microBatchDF.sql(sql_query)`
- **D.** `spark.sql(sql_query)`

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `Delta-MERGE`, `PySpark`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Syntax-Confusion`, `Execution-Behavior`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Development

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `B`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** B
- **ç¤¾ç¾¤å…±è­˜:** B

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming / foreachBatch
**é—œéµæ¦‚å¿µ:** SparkSession access in foreachBatch, microBatchDF.sparkSession

**é¡Œç›®é—œéµå­—ï¼š**
- **foreachBatch**: ä¸²æµè™•ç†ä¸­çš„å¾®æ‰¹æ¬¡å›å‘¼å‡½æ•¸ã€‚
- **microBatchDF**: å¾®æ‰¹æ¬¡çš„ DataFrameã€‚
- **Databricks Runtime above 10.5**: ç‰ˆæœ¬ç›¸é—œçš„èªæ³•ã€‚

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ B æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

1. **foreachBatch ä¸­å­˜å– SparkSession çš„å•é¡Œ**:
   - åœ¨ `foreachBatch` å‡½æ•¸å…§éƒ¨ï¼Œå…¨åŸŸçš„ `spark` è®Šæ•¸å¯èƒ½ç„¡æ³•ç›´æ¥ä½¿ç”¨
   - é€™æ˜¯å› ç‚ºå‡½æ•¸åœ¨ executor ä¸ŠåŸ·è¡Œï¼Œè€Œ `spark` æ˜¯ driver ä¸Šçš„è®Šæ•¸

2. **æ­£ç¢ºçš„å­˜å–æ–¹å¼ï¼ˆDBR 10.5+ï¼‰**:
   ```python
   microBatchDF.sparkSession.sql(sql_query)
   ```
   - å¾ microBatchDF å–å¾—èˆ‡ä¹‹é—œè¯çš„ SparkSession
   - é€™åœ¨ Databricks Runtime 10.5 ä»¥ä¸Šç‰ˆæœ¬æ”¯æ´

3. **å®Œæ•´çš„ foreachBatch ç¯„ä¾‹**:
   ```python
   def upsert_data(microBatchDF, batch_id):
       microBatchDF.createOrReplaceTempView("sales_microbatch")

       sql_query = """
         MERGE INTO sales_silver a
         USING sales_microbatch b
         ON a.item_id=b.item_id AND a.item_timestamp=b.item_timestamp
         WHEN NOT MATCHED THEN INSERT *
       """

       microBatchDF.sparkSession.sql(sql_query)  # âœ… æ­£ç¢º

   # ä½¿ç”¨ foreachBatch
   (df.writeStream
       .foreachBatch(upsert_data)
       .start())
   ```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A - `microBatchDF._jdf.sparkSession().sql(sql_query)`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **èˆŠç‰ˆèªæ³•**: é€™æ˜¯ DBR 10.5 ä¹‹å‰çš„åšæ³•ï¼Œé€é Java API å­˜å–ã€‚
- **å·²è¢«ç°¡åŒ–**: æ–°ç‰ˆå¯ä»¥ç›´æ¥ä½¿ç”¨ `.sparkSession` å±¬æ€§ã€‚
- **ä¸æ¨è–¦**: ä½¿ç”¨ç§æœ‰ API (`_jdf`) ä¸æ˜¯æœ€ä½³å¯¦å‹™ã€‚

### C - `microBatchDF.sql(sql_query)`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **DataFrame æ²’æœ‰ sql æ–¹æ³•**: `sql()` æ˜¯ SparkSession çš„æ–¹æ³•ï¼Œä¸æ˜¯ DataFrame çš„ã€‚
- **æœƒå ±éŒ¯**: `AttributeError: 'DataFrame' object has no attribute 'sql'`

### D - `spark.sql(sql_query)`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **ä½œç”¨åŸŸå•é¡Œ**: åœ¨ foreachBatch å‡½æ•¸å…§ï¼Œå…¨åŸŸ `spark` è®Šæ•¸å¯èƒ½ä¸å¯ç”¨ã€‚
- **åŸ·è¡Œä½ç½®**: å‡½æ•¸åœ¨ executor åŸ·è¡Œæ™‚ï¼Œdriver ä¸Šå®šç¾©çš„è®Šæ•¸ç„¡æ³•ç›´æ¥å­˜å–ã€‚
- **å¯èƒ½å¤±æ•—**: å–æ±ºæ–¼åŸ·è¡Œç’°å¢ƒï¼Œä¸ä¿è­‰å¯ç”¨ã€‚

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€Œå¾®æ‰¹æ¬¡å– Sessionï¼Œå¾ DF æ‹¿æœ€ç©©ã€**

### foreachBatch ä¸­å­˜å– SparkSession

| DBR ç‰ˆæœ¬ | èªæ³• |
|----------|------|
| < 10.5 | `microBatchDF._jdf.sparkSession().sql(...)` |
| **â‰¥ 10.5** | **`microBatchDF.sparkSession.sql(...)`** |

### foreachBatch å¸¸è¦‹æ¨¡å¼

```python
def process_batch(df, batch_id):
    # 1. å»ºç«‹ temp view
    df.createOrReplaceTempView("temp_batch")

    # 2. å–å¾— SparkSession
    spark_session = df.sparkSession

    # 3. åŸ·è¡Œ SQL
    spark_session.sql("MERGE INTO target USING temp_batch ...")

    # 4. æˆ–ä½¿ç”¨ DataFrame API
    # df.write.format("delta").mode("append").saveAsTable("target")
```

### foreachBatch vs å…¶ä»– Sink

| æ–¹å¼ | é©ç”¨å ´æ™¯ | è¤‡é›œåº¦ |
|------|----------|--------|
| `foreachBatch` | MERGEã€è¤‡é›œé‚è¼¯ | é«˜ |
| `.table()` | ç°¡å–® append | ä½ |
| `.toTable()` | ç°¡å–® append | ä½ |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [foreachBatch](https://docs.databricks.com/structured-streaming/foreach.html#foreachbatch)
- [Streaming MERGE](https://docs.databricks.com/structured-streaming/delta-lake.html#upsert-from-streaming-queries-using-foreachbatch)
- [SparkSession from DataFrame](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.sparkSession.html)

---

**[è¿”å›é¡Œç›®](#question-020)**
