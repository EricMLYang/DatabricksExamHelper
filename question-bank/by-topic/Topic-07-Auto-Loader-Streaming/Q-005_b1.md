# Question #005

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-005`

### ä¾†æº
**ä¾†æº:** Sample / Community

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

The data engineering team has a singleplex bronze table called `orders_raw` where new orders data is appended every night. They created a new Silver table called `orders_cleaned` in order to provide a more refined view of the orders data.

The team wants to create a batch processing pipeline to process **all new records** inserted in the `orders_raw` table and propagate them to the `orders_cleaned` table.

Which solution **minimizes the compute costs** to propagate this batch of data?

### é¸é …

- **A.** Use Spark Structured Streaming's `foreachBatch` logic to process the new records from `orders_raw` using `trigger(processingTime="24 hours")`.
- **B.** Use batch overwrite logic to reprocess all records in `orders_raw` and overwrite the `orders_cleaned` table.
- **C.** Use time travel capabilities in Delta Lake to compare the latest version of `orders_raw` with one version prior, then write the difference to the `orders_cleaned` table.
- **D.** Use Spark Structured Streaming to process the new records from `orders_raw` in batch mode using the trigger `availableNow` option.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `ETL-Patterns`, `Spark-Performance`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Command-Purpose`, `Performance-Misconception`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Optimization

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** D
- **ç¤¾ç¾¤å…±è­˜:** D

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming Triggers  
**é—œéµæ¦‚å¿µ:** Incremental Batch Processing (`trigger(availableNow=True)`) vs Continuous Processing

**é¡Œç›®é—œéµå­—ï¼š**
- **Process all new records**: éœ€è¦å¢é‡è™•ç† (Incremental)ã€‚
- **Batch processing pipeline**: æ‰¹æ¬¡ä½œæ¥­æ¨¡å¼ï¼ˆè·‘å®Œå³åœï¼‰ã€‚
- **Minimize compute costs**: æœ€å°åŒ–è¨ˆç®—æˆæœ¬ã€‚

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D (`trigger(availableNow=True)`) æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

`trigger(availableNow=True)` æ˜¯ Databricks æ¨è–¦ç”¨æ–¼ã€Œå®šæœŸæ‰¹æ¬¡è™•ç†å¢é‡è³‡æ–™ã€çš„æœ€ä½³æ¨¡å¼ã€‚

å®ƒçš„è¡Œç‚ºçµåˆäº† Streaming èˆ‡ Batch çš„å„ªé»ï¼š
1.  **å¢é‡è®€å– (Streaming)**: åˆ©ç”¨ Checkpoint è‡ªå‹•è¿½è¹¤é€²åº¦ï¼Œåªè®€å–ä¸Šæ¬¡è™•ç†å¾Œçš„æ–°è³‡æ–™ (New records)ã€‚
2.  **æ‰¹æ¬¡åŸ·è¡Œ (Batch)**: å•Ÿå‹•å¾Œï¼Œä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰å°šæœªè™•ç†çš„è³‡æ–™ï¼ˆå¯èƒ½æ‹†åˆ†ç‚ºå¤šå€‹ Micro-batches ä»¥é¿å… OOMï¼‰ï¼Œè™•ç†å®Œç•¢å¾Œ**è‡ªå‹•åœæ­¢**ã€‚

**æˆæœ¬å„ªå‹¢ï¼š**
ç”±æ–¼è™•ç†å®Œå°±åœæ­¢ï¼Œé€™å…è¨±æˆ‘å€‘å°‡æ­¤é‚è¼¯æ”¾å…¥ Databricks Job ä¸­ï¼Œä½¿ç”¨ **Job Cluster** (æ¯” All-purpose cluster ä¾¿å®œ)ã€‚Cluster åªåœ¨è³‡æ–™è™•ç†æœŸé–“é‹è¡Œï¼Œä¸éœ€ 24 å°æ™‚å¾…å‘½ï¼Œå› æ­¤æˆæœ¬æœ€ä½ã€‚

**ç¨‹å¼ç¢¼ç¯„ä¾‹ï¼š**

```python
(spark.readStream
    .table("orders_raw")
    .writeStream
    .option("checkpointLocation", "/path/to/checkpoint")
    .trigger(availableNow=True)  # âœ… é—œéµï¼šè™•ç†å®Œç¾æœ‰è³‡æ–™å°±åœæ­¢
    .table("orders_cleaned")
)
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A - `trigger(processingTime="24 hours")`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **åŸ·è¡Œæ¨¡å¼**: `processingTime` è§¸ç™¼å™¨é€šå¸¸ç”¨æ–¼**é•·æœŸé‹è¡Œçš„ä¸²æµ (Long-running query)**ã€‚å³ä½¿è¨­ç‚º 24 å°æ™‚ï¼ŒSpark æ‡‰ç”¨ç¨‹å¼é æœŸæœƒä¿æŒ Active ç‹€æ…‹ç­‰å¾…ä¸‹ä¸€æ¬¡è§¸ç™¼ã€‚
- **æˆæœ¬é«˜æ˜‚**: é€™æ„å‘³è‘— Cluster éœ€è¦ 24 å°æ™‚é‹ä½œï¼ˆIdle æ™‚é–“ä¹Ÿè¨ˆè²»ï¼‰ï¼Œæˆ–è€…éœ€è¦è¤‡é›œçš„æ’ç¨‹ä¾†å¼·åˆ¶èˆ‡ Cluster ç”Ÿå‘½å‘¨æœŸé…åˆã€‚ç›¸è¼ƒæ–¼ D é¸é …çš„ã€Œè·‘å®Œå³åœã€ï¼Œæˆæœ¬é¡¯è‘—è¼ƒé«˜ã€‚

### B - Batch overwrite

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **å…¨é‡è™•ç†**: æ¯æ¬¡éƒ½é‡æ–°è™•ç† `orders_raw` çš„**æ‰€æœ‰æ­·å²è³‡æ–™**ã€‚
- **æ•ˆç‡ä½**: éš¨è‘—è³‡æ–™é‡å¢é•·ï¼Œè™•ç†æ™‚é–“èˆ‡æˆæœ¬æœƒç·šæ€§ï¼ˆç”šè‡³è¶…ç·šæ€§ï¼‰å¢åŠ ã€‚ä¸ç¬¦åˆ "process **new** records" çš„å¢é‡éœ€æ±‚ï¼Œä¸”æˆæœ¬æ¥µé«˜ã€‚

### C - Time Travel Comparison

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **è¤‡é›œåº¦é«˜**: éœ€è¦æ‰‹å‹•ç®¡ç†ç‰ˆæœ¬è™Ÿï¼Œè‡ªè¡Œç·¨å¯«é‚è¼¯æ‰¾å‡ºå·®ç•° (`Left Anti Join` æˆ– `Except`)ã€‚
- **ä¸å¯é **: å¦‚æœä¸€å¤©å…§æœ‰å¤šå€‹å¯«å…¥æ“ä½œï¼Œå–®ç´”æ¯”è¼ƒ "Latest" èˆ‡ "Previous" å¯èƒ½æœƒéºæ¼ä¸­é–“çš„ç‰ˆæœ¬ã€‚Structured Streaming çš„ Checkpoint æ©Ÿåˆ¶æ‰æ˜¯è¿½è¹¤è³‡æ–™è®Šæ›´çš„æ¨™æº–ä¸”å¯é åšæ³•ã€‚

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€ŒçœéŒ¢æ‰¹æ¬¡è·‘å¢é‡ï¼ŒAvailableNow æœ€æ£’ã€**

- **AvailableNow**: æŠŠ Stream ç•¶ Batch è·‘ï¼Œè™•ç†å®Œå°±ä¸‹ç­ (Stop)ã€‚
- **ProcessingTime**: ä¸€ç›´ä¸Šç­ç­‰æ™‚é–“ (Continuous)ã€‚

### Trigger æ¨¡å¼å°æ¯”

| Trigger æ¨¡å¼ | è¡Œç‚º | é©ç”¨å ´æ™¯ | æˆæœ¬ |
|--------------|------|----------|------|
| `processingTime='X'` | æ¯éš” X æ™‚é–“è§¸ç™¼ï¼ŒæŒçºŒé‹è¡Œ | å¯¦æ™‚/æº–å¯¦æ™‚ç›£æ§ | é«˜ (24/7) |
| `availableNow=True` | è™•ç†å®Œæ‰€æœ‰ç´¯ç©è³‡æ–™å¾Œå·²åœæ­¢ | å®šæœŸæ’ç¨‹ ETL (Daily/Hourly) | ä½ (Pay per use) |
| `once=True` | (èˆŠç‰ˆ) é¡ä¼¼ availableNow | èˆŠç‰ˆç›¸å®¹ | ä½ |

*(è¨»ï¼š`trigger(once=True)` æ˜¯èˆŠç‰ˆå¯«æ³•ï¼Œ`availableNow=True` æ˜¯å…¶å¢å¼·ç‰ˆï¼Œæ”¯æ´æ›´å¥½çš„å¾®æ‰¹æ¬¡æ‹†åˆ†)*

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Configure Structured Streaming triggers](https://docs.databricks.com/structured-streaming/triggers.html)
- [Best practices for incremental batch processing](https://docs.databricks.com/structured-streaming/production.html#incremental-batch-processing)

---

**[è¿”å›é¡Œç›®](#question-005)**
