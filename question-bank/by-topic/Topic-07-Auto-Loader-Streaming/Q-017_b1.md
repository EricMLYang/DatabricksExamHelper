# Question #017

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-017`

### ä¾†æº
**ä¾†æº:** Sample / Batch 1

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
A junior data engineer is testing the following code block to get the newest entry for each item added in the 'sales' table since the last table update.

```python
from pyspark.sql import functions as F
from pyspark.sql.window import Window

window = Window.partitionBy("item_id").orderBy(F.col("item_time").desc())

ranked_df = (spark.readStream
                    .table("sales")
                    .withColumn("rank", F.rank().over(window))
                    .filter("rank == 1")
                    .drop("rank")
            )

display(ranked_df)
```

However, the command fails when executed.

Which statement explains the cause of this failure?

### é¸é …
- **A.** Non-time-based window operations are not supported on streaming DataFrames. They need to be implemented inside a foreachBatch logic instead.

- **B.** The query output can not be displayed. They should use spark.writeStream to persist the query result.

- **C.** The item_id field is not unique. Records must be de-duplicated on the item_id using dropDuplicates function.

- **D.** Watermarking is missing. It should be added to allow tracking state information for the window of time.

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `Spark-SQL`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Streaming Processing

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `A`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** A
- **ç¤¾ç¾¤å…±è­˜:** A

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Structured Streaming çš„ Window æ“ä½œé™åˆ¶

**é—œéµæ¦‚å¿µ:**
- Streaming DataFrames çš„æ“ä½œé™åˆ¶
- Non-time-based window vs Time-based window
- foreachBatch ä½œç‚ºæ›¿ä»£æ–¹æ¡ˆ

**é¡Œç›®é—œéµå­—ï¼š**
- **spark.readStream**: ä¸²æµè®€å–ï¼Œç”¢ç”Ÿ Streaming DataFrame
- **Window.partitionBy().orderBy()**: éæ™‚é–“åŸºç¤çš„è¦–çª—å‡½æ•¸
- **rank().over(window)**: æ’åå‡½æ•¸ï¼Œå±¬æ–¼éæ™‚é–“åŸºç¤æ“ä½œ
- **foreachBatch**: ä¸²æµçš„å¾®æ‰¹æ¬¡è™•ç†å›å‘¼å‡½æ•¸

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

Structured Streaming å°æ–¼å¯ä»¥åŸ·è¡Œçš„æ“ä½œæœ‰åš´æ ¼é™åˆ¶ã€‚**éæ™‚é–“åŸºç¤çš„è¦–çª—å‡½æ•¸**ï¼ˆå¦‚ `rank()`, `row_number()`, `dense_rank()` ç­‰ï¼‰åœ¨ Streaming DataFrame ä¸Šæ˜¯**ä¸æ”¯æ´**çš„ã€‚

#### ç‚ºä»€éº¼ä¸æ”¯æ´ï¼Ÿ

1. **ç„¡ç•Œè³‡æ–™çš„æœ¬è³ª**ï¼š
   - Streaming æ˜¯ç„¡ç•Œï¼ˆunboundedï¼‰è³‡æ–™æµ
   - `rank()` éœ€è¦çŸ¥é“æ•´å€‹ partition çš„è³‡æ–™æ‰èƒ½è¨ˆç®—æ’å
   - ä¸²æµè³‡æ–™ä¸æ–·åˆ°é”ï¼Œç„¡æ³•ç¢ºå®šã€Œå…¨éƒ¨è³‡æ–™ã€

2. **ç‹€æ…‹ç®¡ç†å•é¡Œ**ï¼š
   - å¦‚æœæ¯ç­†æ–°è³‡æ–™éƒ½è¦é‡æ–°è¨ˆç®—æ‰€æœ‰æ­·å²è³‡æ–™çš„æ’åï¼Œç‹€æ…‹æœƒç„¡é™å¢é•·
   - é€™æœƒå°è‡´è¨˜æ†¶é«”æº¢å‡º

#### éŒ¯èª¤è¨Šæ¯

ç•¶åŸ·è¡Œä¸Šè¿°ç¨‹å¼ç¢¼æ™‚ï¼Œæœƒç”¢ç”Ÿé¡ä¼¼ä»¥ä¸‹éŒ¯èª¤ï¼š

```
AnalysisException: Non-time-based windows are not supported on streaming DataFrames
```

#### æ­£ç¢ºè§£æ³•ï¼šä½¿ç”¨ foreachBatch

`foreachBatch` å…è¨±ä½ åœ¨æ¯å€‹å¾®æ‰¹æ¬¡ï¼ˆmicro-batchï¼‰ä¸ŠåŸ·è¡Œæ‰¹æ¬¡æ“ä½œï¼ŒåŒ…æ‹¬éæ™‚é–“åŸºç¤çš„è¦–çª—å‡½æ•¸ï¼š

```python
from pyspark.sql import functions as F
from pyspark.sql.window import Window

def process_batch(batch_df, batch_id):
    window = Window.partitionBy("item_id").orderBy(F.col("item_time").desc())

    result_df = (batch_df
        .withColumn("rank", F.rank().over(window))
        .filter("rank == 1")
        .drop("rank")
    )

    # å°‡çµæœå¯«å…¥ç›®æ¨™è¡¨
    result_df.write.mode("append").saveAsTable("sales_latest")

# ä½¿ç”¨ foreachBatch è™•ç†ä¸²æµ
(spark.readStream
    .table("sales")
    .writeStream
    .foreachBatch(process_batch)
    .outputMode("update")
    .start()
)
```

#### Streaming æ”¯æ´çš„æ“ä½œé¡å‹

| æ“ä½œé¡å‹ | Streaming æ”¯æ´ | èªªæ˜ |
|---------|---------------|------|
| **Selection/Projection** | âœ… | select, filter, where |
| **Aggregation** | âœ… | groupBy + aggï¼ˆéœ€ watermarkï¼‰ |
| **Time-based Window** | âœ… | window() å‡½æ•¸ï¼ˆåŸºæ–¼ event timeï¼‰ |
| **Non-time-based Window** | âŒ | rank, row_number, lead, lag |
| **Joins** | âš ï¸ éƒ¨åˆ†æ”¯æ´ | Stream-Stream å’Œ Stream-Static |
| **Distinct/dropDuplicates** | âš ï¸ éƒ¨åˆ†æ”¯æ´ | éœ€è¦ watermark |

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … B
**The query output can not be displayed. They should use spark.writeStream to persist the query result.**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **display() åœ¨ Databricks ä¸­æ”¯æ´ä¸²æµ**ï¼š
   - Databricks çš„ `display()` å‡½æ•¸å¯ä»¥ç›´æ¥é¡¯ç¤º Streaming DataFrame
   - å®ƒæœƒè‡ªå‹•å•Ÿå‹•ä¸²æµæŸ¥è©¢ä¸¦åœ¨ notebook ä¸­é¡¯ç¤ºçµæœ

2. **éŒ¯èª¤ç™¼ç”Ÿåœ¨ window æ“ä½œï¼Œä¸æ˜¯ display**ï¼š
```python
# é€™å€‹æ˜¯æœ‰æ•ˆçš„ï¼š
display(spark.readStream.table("sales"))  # âœ… å¯ä»¥é¡¯ç¤º

# éŒ¯èª¤ç™¼ç”Ÿåœ¨é€™è£¡ï¼š
.withColumn("rank", F.rank().over(window))  # âŒ é€™è£¡å¤±æ•—
```

---

### é¸é … C
**The item_id field is not unique. Records must be de-duplicated on the item_id using dropDuplicates function.**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **é€™ä¸æ˜¯éŒ¯èª¤çš„åŸå› **ï¼š
   - ç¨‹å¼ç¢¼çš„ç›®çš„å°±æ˜¯è¦è™•ç†é‡è¤‡çš„ `item_id`ï¼Œæ‰¾å‡ºæ¯å€‹ item çš„æœ€æ–°è¨˜éŒ„
   - `item_id` ä¸å”¯ä¸€æ˜¯é æœŸçš„æƒ…æ³ï¼Œä¸æ˜¯éŒ¯èª¤

2. **dropDuplicates ç„¡æ³•æ›¿ä»£æ­¤é‚è¼¯**ï¼š
```python
# dropDuplicates ç„¡æ³•ä¿è­‰ä¿ç•™æœ€æ–°è¨˜éŒ„
df.dropDuplicates(["item_id"])  # éš¨æ©Ÿä¿ç•™ä¸€ç­†ï¼Œä¸æ˜¯æœ€æ–°çš„

# éœ€è¦ rank + filter æ‰èƒ½ä¿è­‰å–æœ€æ–°
```

---

### é¸é … D
**Watermarking is missing. It should be added to allow tracking state information for the window of time.**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **Watermark ç”¨æ–¼æ™‚é–“åŸºç¤æ“ä½œï¼Œä¸æ˜¯ rank()**ï¼š
   - Watermark ç”¨æ–¼å®šç¾©äº‹ä»¶æ™‚é–“çš„å»¶é²å®¹å¿åº¦
   - å®ƒé©ç”¨æ–¼ `groupBy().agg()` å’Œæ™‚é–“è¦–çª—èšåˆ
   - **å³ä½¿åŠ äº† watermarkï¼Œrank() ä»ç„¶ä¸æ”¯æ´**

2. **Watermark çš„æ­£ç¢ºç”¨é€”**ï¼š
```python
# Watermark ç”¨æ–¼æ™‚é–“åŸºç¤çš„èšåˆ
(spark.readStream
    .table("sales")
    .withWatermark("item_time", "10 minutes")  # å®šç¾©å»¶é²å®¹å¿
    .groupBy(
        F.window("item_time", "1 hour"),  # æ™‚é–“è¦–çª— âœ…
        "item_id"
    )
    .agg(F.max("value"))
)
```

3. **å³ä½¿æœ‰ watermarkï¼Œrank ä»æœƒå¤±æ•—**ï¼š
```python
# é€™ä»ç„¶æœƒå¤±æ•—ï¼
(spark.readStream
    .table("sales")
    .withWatermark("item_time", "10 minutes")
    .withColumn("rank", F.rank().over(window))  # âŒ ä»ç„¶ä¸æ”¯æ´
)
```

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€Œä¸²æµ rank åˆ¥æƒ³ç”¨ï¼ŒforeachBatch ä¾†æå®šã€**
â†’ Streaming DataFrame ä¸æ”¯æ´ rank/row_numberï¼Œç”¨ foreachBatch æ›¿ä»£

### Streaming æ“ä½œæ”¯æ´å¿«é€Ÿåˆ¤æ–·

| æ“ä½œ | é—œéµå­— | Streaming æ”¯æ´ï¼Ÿ |
|------|--------|-----------------|
| `rank()` | Non-time window | âŒ |
| `row_number()` | Non-time window | âŒ |
| `lead()` / `lag()` | Non-time window | âŒ |
| `window("time", "1 hour")` | Time-based window | âœ… |
| `groupBy().agg()` | Aggregation | âœ…ï¼ˆéœ€ watermarkï¼‰ |

### foreachBatch ä½¿ç”¨å ´æ™¯

1. **éœ€è¦éæ™‚é–“åŸºç¤è¦–çª—å‡½æ•¸** â†’ foreachBatch
2. **éœ€è¦ MERGE INTO æ“ä½œ** â†’ foreachBatch
3. **éœ€è¦å¯«å…¥å¤šå€‹ç›®æ¨™** â†’ foreachBatch
4. **éœ€è¦è¤‡é›œæ‰¹æ¬¡é‚è¼¯** â†’ foreachBatch

```python
def my_batch_logic(batch_df, batch_id):
    # åœ¨é€™è£¡å¯ä»¥ä½¿ç”¨ä»»ä½•æ‰¹æ¬¡æ“ä½œ
    # rank, row_number, MERGE INTO, å¤šç›®æ¨™å¯«å…¥ç­‰
    pass

stream.writeStream.foreachBatch(my_batch_logic).start()
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Structured Streaming Unsupported Operations](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations)
- [foreachBatch Sink](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch)
- [Window Operations on Event Time](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#window-operations-on-event-time)

---

**[è¿”å›é¡Œç›®](#question-017)**
