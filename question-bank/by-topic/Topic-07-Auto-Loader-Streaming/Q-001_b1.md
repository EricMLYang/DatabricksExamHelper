# Question #001

---

## é¡Œç›®è³‡è¨Š

### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-001`

### ä¾†æº
**ä¾†æº:** Sample / Community

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹

### é¡Œå¹¹

A data engineering team at a supply chain company uses Lakeflow Declarative Pipelines to manage inventory data. The team maintains a streaming table, `inventory_updates`, with Change Data Feed (CDF) enabled. The table captures real-time changes to product inventory levels, with columns: `product_id`, `quantity`, and `update_timestamp`.

The team needs to incrementally propagate **all** inventory changes from the `inventory_updates` table to downstream layers.

Which implementation approach correctly satisfies this requirement?

### é¸é …

- **A.** Use `spark.readStream` to consume the `inventory_updates` table with `skipChangeCommits`, and propagate the newly added data incrementally to downstream tables.
- **B.** Use `spark.readStream` to consume the `inventory_updates` table directly, and propagate the new updates incrementally to downstream tables.
- **C.** Use `spark.readStream` to consume the `inventory_updates` table's CDF, and apply the changes into downstream tables using AUTO CDC APIs.
- **D.** Use `spark.read` to consume the `inventory_updates` table's CDF, and merge the changes into downstream tables using `MERGE INTO`.

---

## æ¨™ç±¤ç³»çµ±

### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Streaming`, `Delta-CDC`, `ETL-Patterns`

### Trap Tags (é™·é˜±é¡žåž‹æ¨™ç±¤)
**Traps:** `Command-Purpose`, `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Development

---

## ç­”æ¡ˆèˆ‡ä¾†æº

### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** C
- **ç¤¾ç¾¤å…±è­˜:** C

---

# é¡Œç›®è§£æž

---

## ðŸ“ è€ƒé»žè­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Delta Live Tables (DLT) / Change Data Feed (CDF)  
**é—œéµæ¦‚å¿µ:** Incremental Processing of Updates/Deletes, `APPLY CHANGES INTO` (Auto CDC)

**é¡Œç›®é—œéµå­—ï¼š**
- **Lakeflow Declarative Pipelines**: æš—ç¤ºä½¿ç”¨ DLT æˆ–é¡žä¼¼çš„è²æ˜Žå¼æ¡†æž¶ã€‚
- **Streaming Table**: ä¾†æºæ˜¯ä¸²æµè¡¨ã€‚
- **CDF Enabled**: ä¾†æºè¡¨å•Ÿç”¨äº† CDFï¼Œé€™æ˜¯è™•ç†è®Šæ›´çš„é—œéµç·šç´¢ã€‚
- **All inventory changes**: åŒ…å« Inserts, Updates, Deletesã€‚

---

## âœ… æ­£è§£èªªæ˜Ž

### ç‚ºä»€éº¼ C æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŽŸç†ï¼š**

1.  **CDF çš„å¿…è¦æ€§**:
    æ¨™æº–çš„ Structured Streaming (`spark.readStream.table("...")`) é è¨­å‡è¨­ä¾†æºæ˜¯ Append-Onlyã€‚å¦‚æžœä¾†æº Delta Table ç™¼ç”Ÿäº† `UPDATE` æˆ– `DELETE` æ“ä½œï¼Œæ¨™æº–ä¸²æµæœƒå ±éŒ¯ï¼ˆé™¤éžè¨­å®š `ignoreChanges` æˆ– `skipChangeCommits`ï¼Œä½†é€™äº›éƒ½æœƒå¿½ç•¥è®Šæ›´æˆ–å°Žè‡´é‡è¤‡è™•ç†ï¼Œä¸ç¬¦åˆç²¾ç¢ºå‚³æ’­è®Šæ›´çš„éœ€æ±‚ï¼‰ã€‚
    
    è¦å®Œæ•´ä¸”æ­£ç¢ºåœ°å‚³æ’­æ‰€æœ‰è®Šæ›´ï¼ˆå¢žåˆªæ”¹ï¼‰ï¼Œå¿…é ˆè®€å– **Change Data Feed (CDF)**ã€‚

2.  **Auto CDC APIs (DLT)**:
    åœ¨ DLT (Lakeflow Pipelines) ä¸­ï¼Œè™•ç† CDF çš„æ¨™æº–æ¨¡å¼æ˜¯ä½¿ç”¨ `APPLY CHANGES INTO` (Python API ç‚º `dlt.apply_changes`)ã€‚é€™è¢«ç¨±ç‚º "Auto CDC"ã€‚
    
    å®ƒæœƒè‡ªå‹•è™•ç†ï¼š
    - äº‚åºè³‡æ–™ (Out-of-order data)
    - åŽ»é‡ (Deduplication)
    - æ ¹æ“š CDF çš„æ“ä½œé¡žåž‹ (Insert/Update/Delete) æ›´æ–°ç›®æ¨™è¡¨ï¼ˆType 1 æˆ– Type 2 SCDï¼‰ã€‚

**ç¨‹å¼ç¢¼æ¦‚å¿µï¼š**

```python
# è®€å– CDF
changes_df = (spark.readStream
    .option("readChangeFeed", "true")  # âœ… é—œéµé¸é …
    .table("inventory_updates"))

# ä½¿ç”¨ DLT Auto CDC æ‡‰ç”¨è®Šæ›´ (æ¦‚å¿µç¢¼)
dlt.apply_changes(
    target = "downstream_target",
    source = changes_df,
    keys = ["product_id"],
    sequence_by = col("update_timestamp"),
    apply_as_deletes = expr("op = 'DELETE'"),
    ...
)
```

**C é¸é …å®Œå…¨ç¬¦åˆé€™å€‹æ¨¡å¼ï¼š**
- `spark.readStream` w/ CDF: æ­£ç¢ºç²å–è®Šæ›´æµã€‚
- `AUTO CDC APIs`: æ­£ç¢ºæ‡‰ç”¨è®Šæ›´åˆ°ä¸‹æ¸¸ã€‚

---

## âŒ éŒ¯èª¤é¸é …æŽ’é™¤

### A - `skipChangeCommits`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **è³‡æ–™éºå¤±**: `skipChangeCommits` é¸é …æœƒå‘Šè¨´ Spark Streaming **å¿½ç•¥** é‚£äº›æœƒç ´å£ž Append-only å‡è¨­çš„æäº¤ï¼ˆå¦‚ Updates/Deletesï¼‰ã€‚
- **å¾Œæžœ**: ä¸‹æ¸¸è¡¨æœƒéºæ¼æ‰€æœ‰çš„åº«å­˜æ›´æ–°èˆ‡åˆªé™¤æ“ä½œï¼Œå°Žè‡´æ•¸æ“šä¸ä¸€è‡´ã€‚

### B - Consume table directly

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **åŸ·è¡Œå¤±æ•—**: é è¨­æƒ…æ³ä¸‹ï¼Œå¦‚æžœåœ¨ Streaming Source ä¸ŠåŸ·è¡Œäº† Update/Deleteï¼ŒSpark Streaming æœƒæ‹‹å‡º `Detected a data update in the source table. This is currently not supported.` éŒ¯èª¤ä¸¦åœæ­¢ã€‚
- **å³ä½¿èƒ½è·‘**: å¦‚æžœä½¿ç”¨äº† `ignoreChanges`ï¼Œå®ƒæœƒé‡æ–°è™•ç†è¢«ä¿®æ”¹æª”æ¡ˆä¸­çš„æ‰€æœ‰è³‡æ–™ï¼ˆä¸åƒ…æ˜¯è®Šæ›´çš„éƒ¨åˆ†ï¼‰ï¼Œé€™æœƒå°Žè‡´ä¸‹æ¸¸å‡ºç¾é‡è¤‡è³‡æ–™ï¼Œä¸”ç„¡æ³•å€åˆ† Insert å’Œ Updateã€‚å®ƒä¸æ˜¯ "incrementally propagate updates" çš„æ­£ç¢ºæ–¹å¼ã€‚

### D - `spark.read` with `MERGE INTO`

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

- **Batch è™•ç†**: `spark.read` æ˜¯ Batch readï¼Œä¸æ˜¯ Streamingã€‚é€™æ„å‘³è‘—æ¯æ¬¡åŸ·è¡Œéƒ½æœƒè®€å–æ•´å€‹ CDF æ­·å²ï¼ˆæˆ–æ˜¯éœ€è¦æ‰‹å‹•ç®¡ç† Offset é€²è¡Œå¾®æ‰¹æ¬¡è®€å–ï¼‰ï¼Œä¸æ˜¯ DLT å®šç¾©çš„ "Streaming" è™•ç†æ–¹å¼ã€‚
- **ä¸ç¬¦åˆ Lakeflow/DLT æœ€ä½³å¯¦å‹™**: é›–ç„¶æ‰‹å‹•ç·¨å¯« `MERGE INTO` æ˜¯å¯èƒ½çš„ï¼ˆåœ¨ `foreachBatch` ä¸­ï¼‰ï¼Œä½†åœ¨ Lakeflow Declarative Pipelines ä¸­ï¼Œ`APPLY CHANGES INTO` (Auto CDC) æ˜¯å®˜æ–¹å°è£å¥½ã€æ›´é«˜æ•ˆä¸”è²æ˜Žå¼çš„åšæ³•ã€‚é¸é … C æ¯” D æ›´æº–ç¢ºæè¿°äº† DLT çš„æ¨™æº–æ¨¡å¼ã€‚

---

## ðŸ§  è¨˜æ†¶æ³•

### å£è¨£

**ã€ŒCDF æŠ“è®Šæ›´ï¼ŒAuto CDC ä¾†æ›´æ–°ã€**

- **CDF (Change Data Feed)**: è² è²¬æ•æ‰ Insert/Update/Deleteã€‚
- **Auto CDC (APPLY CHANGES)**: è² è²¬æŠŠé€™äº›è®Šæ›´ Merge é€²ç›®æ¨™è¡¨ã€‚

### DLT è™•ç†è®Šæ›´æµç¨‹

```mermaid
graph LR
    Source(Inventory Updates) -- CDF --> Stream(spark.readStream)
    Stream -- Apply Changes --> Target(Downstream Table)
```

---

## ðŸ“š å®˜æ–¹æ–‡ä»¶

- [Change Data Feed (CDF)](https://docs.databricks.com/delta/delta-change-data-feed.html)
- [Delta Live Tables - CDC](https://docs.databricks.com/delta-live-tables/cdc.html#process-change-data)

---

**[è¿”å›žé¡Œç›®](#question-001)**
