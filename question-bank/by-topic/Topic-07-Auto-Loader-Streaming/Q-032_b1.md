# Question #032

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-032`

### ä¾†æº
**ä¾†æº:** Sample / Batch 1

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
A data engineer wants to ingest input json data into a target Delta table. They want the data ingestion to happen incrementally in near real-time.

Which option correctly meets the specified requirement?

### é¸é …
- **A.**
```python
spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    .load(source_path)
.writeStream
    .trigger(availableNow=True)
    .start("target_table")
```

- **B.**
```python
spark.readStream
    .format("autoloader")
    .option("autoloader.format", "json")
    .load(source_path)
.writeStream
    .option("checkpointLocation", checkpointPath)
    .trigger(real-time=True)
    .start("target_table")
```

- **C.**
```python
spark.readStream
    .format("autoloader")
    .option("autoloader.format", "json")
    .load(source_path)
.writeStream
    .option("checkpointLocation", checkpointPath)
    .start("target_table")
```

- **D.**
```python
spark.readStream
    .format("cloudFiles")
    .option("cloudFiles.format", "json")
    .load(source_path)
.writeStream
    .option("checkpointLocation", checkpointPath)
    .start("target_table")
```

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Auto-Loader`, `Streaming`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Syntax-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Data Ingestion

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `D`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** D
- **ç¤¾ç¾¤å…±è­˜:** D

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Auto Loader èªæ³•èˆ‡ä¸²æµè¨­å®š

**é—œéµæ¦‚å¿µ:**
- Auto Loader çš„æ­£ç¢º format åç¨±
- checkpointLocation çš„å¿…è¦æ€§
- trigger æ¨¡å¼èˆ‡å³æ™‚è™•ç†

**é¡Œç›®é—œéµå­—ï¼š**
- **Auto Loader**: Databricks å¢é‡è¼‰å…¥å·¥å…·
- **incrementally**: å¢é‡è™•ç†
- **near real-time**: è¿‘å³æ™‚ï¼Œéœ€è¦é è¨­ trigger

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ D æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

é¸é … D ä½¿ç”¨äº†æ­£ç¢ºçš„ Auto Loader èªæ³•å’Œé…ç½®ï¼š

#### æ­£ç¢ºçš„ Auto Loader è¨­å®š

| è¨­å®šé …ç›® | æ­£ç¢ºå€¼ | èªªæ˜ |
|---------|--------|------|
| **format** | `"cloudFiles"` | Auto Loader çš„ format åç¨± |
| **option** | `"cloudFiles.format"` | æŒ‡å®šæª”æ¡ˆæ ¼å¼çš„é¸é …åç¨± |
| **checkpointLocation** | å¿…é ˆè¨­å®š | è¿½è¹¤è™•ç†é€²åº¦ |
| **trigger** | çœç•¥ï¼ˆä½¿ç”¨é è¨­ï¼‰ | é è¨­ 500ms å³ç‚ºè¿‘å³æ™‚ |

#### å®Œæ•´èªæ³•è§£æ

```python
# è®€å–ç«¯ï¼šä½¿ç”¨ Auto Loaderï¼ˆcloudFilesï¼‰
spark.readStream
    .format("cloudFiles")              # âœ… æ­£ç¢ºçš„ format åç¨±
    .option("cloudFiles.format", "json")  # âœ… æ­£ç¢ºçš„é¸é …å‰ç¶´
    .load(source_path)

# å¯«å…¥ç«¯
.writeStream
    .option("checkpointLocation", checkpointPath)  # âœ… å¿…è¦çš„ checkpoint
    .start("target_table")  # é è¨­ trigger = 500msï¼ˆè¿‘å³æ™‚ï¼‰
```

#### é è¨­ Trigger è¡Œç‚º

- ä¸æŒ‡å®š trigger æ™‚ï¼Œé è¨­ç‚º `trigger(processingTime="500ms")`
- æ¯ 500 æ¯«ç§’è™•ç†ä¸€å€‹ micro-batch
- é€™ç¬¦åˆã€Œnear real-timeã€çš„éœ€æ±‚

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … A
```python
.trigger(availableNow=True)  # ç¼ºå°‘ checkpointLocation
```

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **ç¼ºå°‘ checkpointLocation**ï¼š
   - ä¸²æµå¯«å…¥**å¿…é ˆ**æŒ‡å®š checkpointLocation
   - æ²’æœ‰ checkpoint ç„¡æ³•è¿½è¹¤é€²åº¦

2. **trigger(availableNow=True) ä¸æ˜¯è¿‘å³æ™‚**ï¼š
   - `availableNow=True` æ˜¯ä¸€æ¬¡æ€§è™•ç†æ‰€æœ‰å¯ç”¨è³‡æ–™
   - è™•ç†å®Œå¾Œä¸²æµå°±åœæ­¢
   - é€™æ˜¯ã€Œæ‰¹æ¬¡ã€è¡Œç‚ºï¼Œä¸æ˜¯ã€ŒæŒçºŒè¿‘å³æ™‚ã€

```python
# availableNow çš„è¡Œç‚º
# 1. è™•ç†æ‰€æœ‰ç›®å‰å¯ç”¨çš„è³‡æ–™
# 2. å®Œæˆå¾Œè‡ªå‹•åœæ­¢
# é©ç”¨å ´æ™¯ï¼šæ‰¹æ¬¡å¢é‡æ›´æ–°ï¼Œä¸æ˜¯å³æ™‚ä¸²æµ
```

---

### é¸é … B
```python
.format("autoloader")                    # âŒ éŒ¯èª¤çš„ format
.option("autoloader.format", "json")     # âŒ éŒ¯èª¤çš„é¸é …å‰ç¶´
.trigger(real-time=True)                 # âŒ ä¸å­˜åœ¨çš„ trigger é¸é …
```

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **format åç¨±éŒ¯èª¤**ï¼š
   - æ­£ç¢ºï¼š`"cloudFiles"`
   - éŒ¯èª¤ï¼š`"autoloader"`

2. **option å‰ç¶´éŒ¯èª¤**ï¼š
   - æ­£ç¢ºï¼š`"cloudFiles.format"`
   - éŒ¯èª¤ï¼š`"autoloader.format"`

3. **ä¸å­˜åœ¨çš„ trigger é¸é …**ï¼š
   - `trigger(real-time=True)` ä¸æ˜¯æœ‰æ•ˆçš„ Spark èªæ³•
   - æœ‰æ•ˆçš„ trigger é¸é …ï¼š
     - `trigger(processingTime="10 seconds")`
     - `trigger(once=True)`
     - `trigger(availableNow=True)`
     - `trigger(continuous="1 second")`

---

### é¸é … C
```python
.format("autoloader")                    # âŒ éŒ¯èª¤çš„ format
.option("autoloader.format", "json")     # âŒ éŒ¯èª¤çš„é¸é …å‰ç¶´
```

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **format å’Œ option éƒ½ä½¿ç”¨äº†éŒ¯èª¤çš„åç¨±**
2. é›–ç„¶æœ‰ checkpointLocationï¼Œä½†è®€å–ç«¯èªæ³•éŒ¯èª¤

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€ŒcloudFiles æ˜¯æ­£é“ï¼Œautoloader æ˜¯é™·é˜±ã€**
â†’ Auto Loader çš„ format åç¨±æ˜¯ `cloudFiles`ï¼Œä¸æ˜¯ `autoloader`

### Auto Loader å®Œæ•´èªæ³•

```python
# æ¨™æº– Auto Loader èªæ³•
df = (spark.readStream
    .format("cloudFiles")                    # å¿…é ˆæ˜¯ cloudFiles
    .option("cloudFiles.format", "json")     # é¸é …å¿…é ˆç”¨ cloudFiles. å‰ç¶´
    .option("cloudFiles.schemaLocation", "/schema/path")  # é¸ç”¨
    .option("cloudFiles.inferColumnTypes", "true")        # é¸ç”¨
    .load("/data/source/path")
)

# å¯«å…¥ Delta è¡¨
(df.writeStream
    .option("checkpointLocation", "/checkpoint/path")  # å¿…é ˆ
    .trigger(processingTime="1 minute")                # é¸ç”¨
    .toTable("target_table")  # æˆ– .start("/path/to/table")
)
```

### Trigger æ¨¡å¼æ¯”è¼ƒ

| Trigger é¡å‹ | èªæ³• | è¡Œç‚º | é©ç”¨å ´æ™¯ |
|-------------|------|------|---------|
| **é è¨­** | ä¸æŒ‡å®š | 500ms micro-batch | è¿‘å³æ™‚ |
| **Processing Time** | `trigger(processingTime="10s")` | å›ºå®šé–“éš” | ä¸€èˆ¬ä¸²æµ |
| **Once** | `trigger(once=True)` | è™•ç†ä¸€æ‰¹å¾Œåœæ­¢ | å·²æ£„ç”¨ |
| **Available Now** | `trigger(availableNow=True)` | è™•ç†å…¨éƒ¨å¾Œåœæ­¢ | æ‰¹æ¬¡å¢é‡ |
| **Continuous** | `trigger(continuous="1s")` | çœŸæ­£é€£çºŒ | è¶…ä½å»¶é² |

### å¸¸è¦‹ Auto Loader é¸é …

| é¸é … | èªªæ˜ | é è¨­å€¼ |
|------|------|--------|
| `cloudFiles.format` | ä¾†æºæª”æ¡ˆæ ¼å¼ | å¿…é ˆæŒ‡å®š |
| `cloudFiles.schemaLocation` | Schema æ¨æ–·å„²å­˜ä½ç½® | é¸ç”¨ |
| `cloudFiles.inferColumnTypes` | æ¨æ–·æ¬„ä½é¡å‹ | false |
| `cloudFiles.schemaHints` | Schema æç¤º | é¸ç”¨ |
| `cloudFiles.maxFilesPerTrigger` | æ¯æ‰¹æ¬¡æœ€å¤§æª”æ¡ˆæ•¸ | 1000 |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Auto Loader](https://docs.databricks.com/en/ingestion/auto-loader/index.html)
- [Auto Loader Options](https://docs.databricks.com/en/ingestion/auto-loader/options.html)
- [Streaming Triggers](https://docs.databricks.com/en/structured-streaming/triggers.html)

---

**[è¿”å›é¡Œç›®](#question-032)**
