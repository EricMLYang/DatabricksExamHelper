# Question #012

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-012`
### ä¾†æº
**ä¾†æº:** Community
### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
<!-- âš ï¸ ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è­¯ -->
A production environment has an S3 bucket receiving thousands of image files daily in different formats (.png, .jpg, .gif). A data engineer has been tasked with modifying the following streaming ingestion script to ensure only .png files are processed.

```python
df = spark.readStream.format("cloudFiles") \
          .option("cloudFiles.format", "binaryFile") \
          .option("_____________", "*.png") \
          .load("s3://shop/raw/invoices/")
```

Which option correctly fills in the blank to meet the specified requirement?

### é¸é …
<!-- âš ï¸ ä¿æŒè‹±æ–‡åŸæ–‡ï¼Œä¸è¦ç¿»è­¯ -->
- **A.** fileExtension
- **B.** cloudFiles.fileExtension
- **C.** pathGlobFilter
- **D.** cloudFiles.pathGlobFilter

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Auto-Loader`, `Spark-Streaming`
### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Syntax-Confusion`, `Parameter-Order`
### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Data Ingestion

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `C`
### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** C
- **ç¤¾ç¾¤å…±è­˜:** C

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥
**æ ¸å¿ƒæŠ€è¡“:** Auto Loader / File Filtering
**é—œéµæ¦‚å¿µ:** pathGlobFilter é¸é …ã€æª”æ¡ˆç¯©é¸
**é¡Œç›®é—œéµå­—ï¼š**
- **cloudFiles format**: Auto Loader
- **only .png files**: æª”æ¡ˆç¯©é¸
- **pathGlobFilter**: è·¯å¾‘ glob æ¨¡å¼ç¯©é¸

---

## âœ… æ­£è§£èªªæ˜
### ç‚ºä»€éº¼ C æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**`pathGlobFilter`** æ˜¯ Spark DataSource çš„æ¨™æº–é¸é …ï¼Œç”¨æ–¼æ ¹æ“š glob æ¨¡å¼ç¯©é¸æª”æ¡ˆã€‚

#### æ­£ç¢ºçš„èªæ³•
```python
df = spark.readStream.format("cloudFiles") \
          .option("cloudFiles.format", "binaryFile") \
          .option("pathGlobFilter", "*.png") \
          .load("s3://shop/raw/invoices/")
```

#### pathGlobFilter èªªæ˜
| é …ç›® | èªªæ˜ |
|------|------|
| **ç”¨é€”** | æ ¹æ“šæª”åæ¨¡å¼ç¯©é¸è¦è®€å–çš„æª”æ¡ˆ |
| **èªæ³•** | Glob æ¨¡å¼ï¼ˆå¦‚ `*.png`, `file_*.csv`ï¼‰ |
| **å±¤ç´š** | Spark DataSource é¸é …ï¼ˆé cloudFiles å°ˆå±¬ï¼‰ |

#### Glob æ¨¡å¼ç¯„ä¾‹
```python
# åªè®€å– .png æª”æ¡ˆ
.option("pathGlobFilter", "*.png")

# åªè®€å–ä»¥ data_ é–‹é ­çš„ CSV
.option("pathGlobFilter", "data_*.csv")

# åªè®€å–ç‰¹å®šæ—¥æœŸæ ¼å¼çš„æª”æ¡ˆ
.option("pathGlobFilter", "*_2024*.parquet")
```

#### é¸é …å‰ç¶´è¦å‰‡
```
Auto Loader é¸é …å‘½åè¦å‰‡ï¼š
â”œâ”€â”€ cloudFiles.* â†’ Auto Loader å°ˆå±¬é¸é …
â”‚   â”œâ”€â”€ cloudFiles.format
â”‚   â”œâ”€â”€ cloudFiles.schemaLocation
â”‚   â””â”€â”€ cloudFiles.maxFilesPerTrigger
â””â”€â”€ ç„¡å‰ç¶´ â†’ Spark é€šç”¨é¸é …
    â”œâ”€â”€ pathGlobFilter
    â”œâ”€â”€ recursiveFileLookup
    â””â”€â”€ modifiedBefore / modifiedAfter
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### A. fileExtension
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **é€™å€‹é¸é …ä¸å­˜åœ¨**
- ç¯©é¸æª”æ¡ˆè¦ç”¨ `pathGlobFilter`
- ä¸æ˜¯ Spark çš„æ¨™æº–é¸é …

### B. cloudFiles.fileExtension
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **é€™å€‹é¸é …ä¸å­˜åœ¨**
- cloudFiles æ²’æœ‰ fileExtension é¸é …
- è™›æ§‹çš„é¸é …åç¨±

### D. cloudFiles.pathGlobFilter
**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**
- **å‰ç¶´éŒ¯èª¤**
- `pathGlobFilter` æ˜¯ Spark é€šç”¨é¸é …ï¼Œä¸éœ€è¦ `cloudFiles.` å‰ç¶´
- åŠ äº†å‰ç¶´æœƒè®Šæˆç„¡æ•ˆé¸é …

```python
# æ­£ç¢º
.option("pathGlobFilter", "*.png")

# éŒ¯èª¤
.option("cloudFiles.pathGlobFilter", "*.png")  # ç„¡æ•ˆé¸é …
```

---

## ğŸ§  è¨˜æ†¶æ³•
### å£è¨£
**ã€ŒpathGlobFilter ç¯©æª”åï¼Œä¸åŠ  cloudFiles å‰ç¶´ã€**

### Auto Loader å¸¸ç”¨é¸é …
| é¸é … | èªªæ˜ | å‰ç¶´ |
|------|------|------|
| `cloudFiles.format` | æª”æ¡ˆæ ¼å¼ | cloudFiles |
| `cloudFiles.schemaLocation` | Schema å„²å­˜ä½ç½® | cloudFiles |
| `cloudFiles.maxFilesPerTrigger` | æ¯æ‰¹æ¬¡æœ€å¤§æª”æ¡ˆæ•¸ | cloudFiles |
| `pathGlobFilter` | æª”åç¯©é¸ | ç„¡ |
| `recursiveFileLookup` | éè¿´è®€å–å­ç›®éŒ„ | ç„¡ |

### å®Œæ•´ Auto Loader ç¯„ä¾‹
```python
df = spark.readStream.format("cloudFiles") \
    .option("cloudFiles.format", "json") \
    .option("cloudFiles.schemaLocation", "/schema") \
    .option("cloudFiles.maxFilesPerTrigger", 100) \
    .option("pathGlobFilter", "*.json") \
    .option("recursiveFileLookup", "true") \
    .load("s3://bucket/path/")
```

### è¨˜æ†¶æŠ€å·§
```
åˆ¤æ–·æ˜¯å¦éœ€è¦ cloudFiles å‰ç¶´ï¼š
â”œâ”€â”€ Auto Loader å°ˆå±¬åŠŸèƒ½ â†’ éœ€è¦ cloudFiles å‰ç¶´
â”‚   â””â”€â”€ format, schemaLocation, maxFilesPerTrigger
â””â”€â”€ Spark é€šç”¨åŠŸèƒ½ â†’ ä¸éœ€è¦å‰ç¶´
    â””â”€â”€ pathGlobFilter, recursiveFileLookup
```

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶
- [Auto Loader Options](https://docs.databricks.com/ingestion/auto-loader/options.html)
- [pathGlobFilter](https://spark.apache.org/docs/latest/sql-data-sources-generic-options.html#path-glob-filter)
- [Auto Loader File Detection](https://docs.databricks.com/ingestion/auto-loader/file-detection-modes.html)

---

**[è¿”å›é¡Œç›®](#question-012)**
