# Question 43

## Question
A data engineer defines the following function in their LDP pipeline:



```

- @dlt.table
- @dlt.expect_or_drop("quantity_within_range", "quantity BETWEEN 0 AND 1000")
- @dlt.expect_or_drop("recent_transaction", "transaction_date >= '2025-01-01'")
- @dlt.expect_or_drop("valid_transaction", "transaction_id IS NOT NULL'")
- def silver_sales():
-     return dlt.read_stream("bronze_sales")
```





Which of the following correctly describes the result of running this pipeline?

## Options
- A. Rows that violate the defined expectations are deleted from both tables. 
- B. Rows that violate the defined expectations are deleted from the bronze_sales table. 
- C. Rows that violate the defined expectations are filtered out, and only valid rows are written to silver_sales. (Correct)
- D. Rows that violate the defined expectations are streamed into the silver_sales table. 

## Explanation
The expect_or_drop function is a data quality enforcement rule in LDP (previously known as DLT):
- 

The expect part defines the quality constraint (e.g., "quantity BETWEEN 0 AND 1000").
- 

The or_drop part defines the action to take when the expectation is violated. "Drop" means that the violating row is discarded (filtered out) and will not be written to the target table (silver_sales).




In this example, only rows that successfully pass all three defined expectations (quantity_within_range, recent_transaction, and valid_transaction) will be included in the silver_sales table. Rows failing any of them are discarded.




Note: Databricks has recenlty open-sourced this solution, integrating it into the Apache Spark ecosystem under the name Spark Declarative Pipelines (SDP).
