# Question 20

## Question
A data engineer is building a Lakeflow Declarative Pipeline to process product sales data. The pipeline needs to enforce the following data quality rules:




`valid_products = {"valid_id": "products_id IS NOT NULL", "recent_sales": "date >= '2025-01-01", "quantity_within_range": "quantity BETWEEN 0 AND 1000"}`




Any invalid records should still be written to the target, while metrics about these violations are captured by the pipeline.




Which of the following configurations would satisfy these requirements?

## Options
- A. ```

- @dlt.table
- @dlt.expect_all(valid_products)
- def silver_sales():
-     return dlt.read_stream("bronze_sales")
``` (Correct)
- B. ```

- @dlt.table
- @dlt.expect_or_fail(valid_products)
- def silver_sales():
-     return dlt.read_stream("bronze_sales")
``` 
- C. ```

- @dlt.table
- @dlt.expect(valid_products)
- def silver_sales():
-     return dlt.read_stream("bronze_sales")
``` 
- D. ```

- @dlt.table
- @dlt.expect_or_drop(valid_products)
- def silver_sales():
-     return dlt.read_stream("bronze_sales")
``` 

## Explanation
`dlt.expect_all` enforces all the specified data quality rules, writes both valid and invalid records to the target table, and captures metrics about any rule violations.




`dlt.expect` would not fully meet the requirements because it applies expectations individually but doesn't automatically enforce all of them together as a group. Similarly, `dlt.expect_or_drop` removes individual invalid records, and `dlt.expect_or_fail` stops the pipeline on individual rule violations. You can group multiple expectations together and specify collective actions using the functions `dlt.expect_all_or_drop`, and `dlt.expect_all_or_fail`.




Note: Databricks has recenlty open-sourced this solution, integrating it into the Apache Spark ecosystem under the name Spark Declarative Pipelines (SDP).
