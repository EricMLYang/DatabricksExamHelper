# Question 35

## Question
Given the following query on the Delta table 'customers' on which Change Data Feed is enabled:



```

- spark.read
-         .option("readChangeFeed", "true")
-         .option("startingVersion", 0)
-         .table ("customers")
-         .filter (col("_change_type").isin(["update_postimage"]))
-     .write
-         .mode("append")
-         .table("customers_updates")
```





Which statement describes the result of this query each time it is executed ?

## Options
- A. The entire history of updated records will overwrite the target table at each execution. 
- B. Newly updated records will overwrite the target table. 
- C. The entire history of updated records will be appended to the target table at each execution, which leads to duplicate entries. (Correct)
- D. Newly updated records will be appended to the target table. 

## Explanation
Reading table's changes, captured by CDF, using spark.read means that you are reading them as a static source. So, each time you run the query, all table's changes (starting from the specified startingVersion) will be read.




The query in the question then appends the data to the target table at each execution since it's using the 'append' writing mode.







Reference:

https://docs.databricks.com/delta/delta-change-data-feed.html#read-changes-in-batch-queries




Study materials from our exam preparation course on Udemy:

Lecture

Hands-on
