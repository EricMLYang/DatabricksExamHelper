# Question 47

## Question
A data engineer is analyzing a Spark job via the Spark UI. They have the following summary metrics for 27 completed tasks in a particular stage







Which conclusion can the data engineer draw from the above statistics ?

## Options
- A. All task are operating over partitions with even amounts of data 
- B. All task are operating over empty or near empty partitions 
- C. Number of tasks are operating over near empty partitions (Correct)
- D. Number of tasks are operating over partitions with larger skewed amounts of data. 

## Explanation
Usually, if your computation was completely symmetric across tasks, you would see all of the statistics clustered tightly around the 50th percentile value.




Here, we see the distribution is reasonable, except that we have a bunch of "Min" values near zero. This suggests that we have almost empty partitions.
