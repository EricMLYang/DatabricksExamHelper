# Question 25

## Question
A data engineering team is running a complex analytical query on a large dataset, but they notice that the query execution results in a significant disk spill, which drastically slows down performance.




Which of the following approaches would NOT be an effective solution to minimize this issue?

## Options
- A. Reduce the size of Spark partitions 
- B. Increase memory size 
- C. Increase the number of CPU cores (Correct)
- D. Increase the number of shuffle partitions 

## Explanation
The issue described is disk spill during a Spark analytical query. Disk spill occurs when the data being processed does not fit into the memory allocated for a task, so Spark writes intermediate data to disk, which is much slower than in-memory processing.




The following option would effectively reduce disk spill:

- Reduce the size of Spark partitions
- 

Smaller partitions can help reduce memory pressure per task because each task handles less data that can fit in memory.

- Increase memory size (core-to-memory ratio)
- 

More memory allows Spark to keep more data in-memory and reduce spills.

- Increase the number of shuffle partitions
- 

Increasing shuffle partitions spreads the data across more tasks, reducing memory pressure per task.




However, more CPU cores allow more tasks to run in parallel, but it does not reduce the amount of memory each task requires. If memory per task is insufficient, disk spills will still happen. So, this does NOT directly address disk spill.
