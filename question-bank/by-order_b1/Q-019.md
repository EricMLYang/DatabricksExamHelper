# Question 19

## Question
The data engineering team has a large external Delta table where new changes are merged very frequently. They enabled Optimized writes and Auto Compaction on the table in order to automatically compact small data files to target files of size 128 MB. However, when they look at the table directory, they see that most data files are smaller than 128 MB.




Which of the following likely explains these smaller file sizes ?

## Options
- A. Auto compaction supports Auto Z-Ordering which is more expensive than just compaction 
- B. Optimized Writes and Auto Compaction have no effect on external tables. The table needs to be managed in order to store the information of file sizes in the Hive metastore. 
- C. Optimized Writes and Auto Compaction automatically generate smaller data files to reduce the duration of future MERGE operations. (Correct)
- D. Optimized Writes and Auto Compaction have no effect on large Delta tables. The table needs to be partitioned so Auto Compaction can be applied at partition level. 

## Explanation
Having many small files can help minimize rewrites during some operations like merges and deletes. For such operations, Databricks can automatically tune the file size of Delta tables. As a result, it can generate data files smaller than the default 128MB. This helps in reducing the duration of future MERGE operations.




Reference:

https://docs.databricks.com/delta/tune-file-size.html#autotune-file-size-based-on-workload




Study materials from our exam preparation course on Udemy:

Lecture
