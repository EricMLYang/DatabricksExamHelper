# Question 5

## Question
The data engineering team has a singleplex bronze table called 'orders_raw' where new orders data is appended every night. They created a new Silver table called 'orders_cleaned' in order to provide a more refined view of the orders data.




The team wants to create a batch processing pipeline to process all new records inserted in the orders_raw table and propagate them to the orders_cleaned table.




Which solution minimizes the compute costs to propagate this batch of data?

## Options
- A. Use Spark Structured Streaming's foreachBatch logic to process the new records from orders_raw using trigger(processingTime="24 hours") 
- B. Use batch overwrite logic to reprocess all records in orders_raw and overwrite the orders_cleaned table 
- C. Use time travel capabilities in Delta Lake to compare the latest version of orders_raw with one version prior, then write the difference to the orders_cleaned table. 
- D. Use Spark Structured Streaming to process the new records from orders_raw in batch mode using the trigger availableNow option (Correct)

## Explanation
`trigger(availableNow=True)` is more compute-efficient for one-time or scheduled batch runs, as it processes all available data once and stops, avoiding the overhead of keeping a cluster running. While, the `processingTime` option keeps the stream active continuously, so it's less efficient for nightly jobs.



There is also the `trigger(once=True)` option for incremental batch processing. However, this setting is now deprecated in the newer Databricks Runtime versions.

NOTE: You may still see this option in the current certification exam version. However, Databricks recommends you use `trigger(availableNow=True)` for all future incremental batch processing workloads.




Study materials from our exam preparation course on Udemy:
- 

Lecture
- 

Hands-on
- 

Lecture (Associate course)
