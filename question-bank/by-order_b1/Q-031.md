# Question 31

## Question
A data engineer is analyzing a Spark job via the Spark UI. They have the following summary metrics for 27 completed tasks in a particular stage







Which conclusion can the data engineer draw from the above statistics ?

## Options
- A. Number of tasks are operating over empty or near empty partitions 
- B. All task are operating over partitions with even amounts of data 
- C. All tasks are operating over partitions with larger skewed amounts of data. 
- D. Number of tasks are operating over partitions with larger skewed amounts of data. (Correct)

## Explanation
Usually, if your computation was completely symmetric across tasks, you would see all of the statistics clustered tightly around the 50th percentile value.




Here, the "Max" metrics task took 10x the time and read about 5x the data of the 75th-percentile task. This suggests a number of "straggler" tasks that operating over partitions with larger skewed amounts of data.
