# Question 1

## Question
A data engineering team at a supply chain company uses Lakeflow Declarative Pipelines to manage inventory data. The team maintains a streaming table, inventory_updates, with Change Data Feed (CDF) enabled. The table captures real-time changes to product inventory levels, with columns: product_id, quantity, and update_timestamp.




The team needs to incrementally propagate all inventory changes from the inventory_updates table to downstream layers.




Which implementation approach correctly satisfies this requirement?

## Options
- A. Use spark.readStream to consume the inventory_updates table with skipChangeCommits, and propagate the newly added data incrementally to downstream tables. 
- B. Use spark.readStream to consume the inventory_updates table directly, and propagate the new updates incrementally to downstream tables. 
- C. Use spark.readStream to consume the inventory_updates table's CDF, and apply the changes into downstream tables using AUTO CDC APIs. (Correct)
- D. Use spark.read to consume the inventory_updates table's CDF, and merge the changes into downstream tables using MERGE INTO. 

## Explanation
Because the inventory_updates table contains updates and deletes, it breaks the append-only requirement of standard streaming tables. This means that we cannot directly stream from the base table or just skip change commits. Instead, since Change Data Feed (CDF) is enabled, the correct approach is to use spark.readStream to consume all inventory changes - including inserts, updates, and deletes - from the CDF output and apply them downstream using AUTO CDC APIs (previously known as APPLY CHANGES APIs).




Remember, to read the change data feed from a table, you need to set the option readChangeFeed to true when configuring a stream read from the table, as shown in the following syntax example:



```
- (spark.readStream
-       .option("readChangeFeed", "true")
-       .table("inventory_updates")
- )
```





Note that using spark.read here is a completely incorrect approach as it performs batch processing, not incremental processing, which would require a full table refresh each time. In addition, MERGE INTO command is not supported in Lakeflow Declarative Pipelines, making this method unsuitable for incremental change propagation.
