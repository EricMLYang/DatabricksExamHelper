# Question #039

---

## é¡Œç›®è³‡è¨Š
### é¡Œç›®ç·¨è™Ÿ
**ID:** `Q-039`

### ä¾†æº
**ä¾†æº:** Sample / Batch 1

### é›£åº¦ç­‰ç´š
**é›£åº¦:** `L2-Intermediate`

---

## é¡Œç›®å…§å®¹
### é¡Œå¹¹
Given the following multi-task job:

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Task 1  â”‚
    â”‚(Notebookâ”‚
    â”‚   1)    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Task 2  â”‚     â”‚ Task 3  â”‚
    â”‚(Notebookâ”‚     â”‚(Notebookâ”‚
    â”‚   2)    â”‚     â”‚   3)    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Task 2 and Task 3 are both downstream of Task 1
Task 2 and Task 3 are parallel (no dependency between them)
```

If there is an error in the notebook 2 that is associated with Task 2, which statement describes the run result of this job?

### é¸é …
- **A.** Tasks 1 and 3 will succeed, while Task 2 will partially fail

- **B.** Task 1 will succeed. Task 2 will partially fail. Task 3 will be skipped

- **C.** Task 1 will succeed. Task 2 will completely fail. Task 3 will be skipped

- **D.** Tasks 1 and 3 will succeed, while Task 2 will completely fail

---

## æ¨™ç±¤ç³»çµ±
### Topic Tags (æŠ€è¡“ä¸»é¡Œæ¨™ç±¤)
**Topics:** `Workflows`, `Jobs`, `Task-Dependencies`

### Trap Tags (é™·é˜±é¡å‹æ¨™ç±¤)
**Traps:** `Concept-Confusion`

### Knowledge Domain (çŸ¥è­˜é ˜åŸŸ)
**Domain:** Data Engineering / Workflow Orchestration

---

## ç­”æ¡ˆèˆ‡ä¾†æº
### æ­£ç¢ºç­”æ¡ˆ
**æ­£è§£:** `A`

### ç­”æ¡ˆä¾†æº
- **ä¾†æºæ¨™è¨»ç­”æ¡ˆ:** A
- **ç¤¾ç¾¤å…±è­˜:** A

---

# é¡Œç›®è§£æ

---

## ğŸ“ è€ƒé»è­˜åˆ¥

**æ ¸å¿ƒæŠ€è¡“:** Databricks Workflows Multi-task Job å¤±æ•—è¡Œç‚º

**é—œéµæ¦‚å¿µ:**
- å¹³è¡Œä»»å‹™çš„åŸ·è¡Œè¡Œç‚º
- ä»»å‹™å¤±æ•—çš„å½±éŸ¿ç¯„åœ
- Partial failureï¼ˆéƒ¨åˆ†å¤±æ•—ï¼‰çš„æ¦‚å¿µ

**é¡Œç›®é—œéµå­—ï¼š**
- **multi-task job**: å¤šä»»å‹™ä½œæ¥­
- **parallel**: Task 2 å’Œ Task 3 æ˜¯å¹³è¡Œçš„
- **error in notebook 2**: Notebook 2 ä¸­æœ‰éŒ¯èª¤

---

## âœ… æ­£è§£èªªæ˜

### ç‚ºä»€éº¼ A æ˜¯æ­£ç¢ºç­”æ¡ˆï¼Ÿ

**æŠ€è¡“åŸç†ï¼š**

åœ¨ Databricks Workflows ä¸­ï¼š
1. **å¹³è¡Œä»»å‹™äº’ä¸å½±éŸ¿**ï¼šTask 2 å¤±æ•—ä¸æœƒå°è‡´ Task 3 è¢«è·³é
2. **å¤±æ•—æ˜¯éƒ¨åˆ†çš„**ï¼šNotebook ä¸­éŒ¯èª¤ä¹‹å‰çš„ç¨‹å¼ç¢¼æœƒæˆåŠŸåŸ·è¡Œ

#### ä»»å‹™åŸ·è¡Œçµæœåˆ†æ

| ä»»å‹™ | çµæœ | åŸå›  |
|------|------|------|
| **Task 1** | âœ… æˆåŠŸ | æ²’æœ‰éŒ¯èª¤ï¼Œæ­£å¸¸å®Œæˆ |
| **Task 2** | âš ï¸ éƒ¨åˆ†å¤±æ•— | éŒ¯èª¤å‰çš„ç¨‹å¼ç¢¼æˆåŠŸï¼ŒéŒ¯èª¤å¾Œçš„è¢«è·³é |
| **Task 3** | âœ… æˆåŠŸ | èˆ‡ Task 2 å¹³è¡Œï¼Œä¸å—å½±éŸ¿ |

#### éƒ¨åˆ†å¤±æ•—ï¼ˆPartial Failureï¼‰çš„æ¦‚å¿µ

```python
# Notebook 2 çš„åŸ·è¡Œæƒ…æ³
# Cell 1: åŸ·è¡ŒæˆåŠŸ âœ…
df = spark.read.table("source_table")

# Cell 2: åŸ·è¡ŒæˆåŠŸ âœ…
df_cleaned = df.filter("status = 'active'")

# Cell 3: ç™¼ç”ŸéŒ¯èª¤ âŒ
df_final = df_cleaned.withColumn("new_col", undefined_function())  # Error!

# Cell 4: è¢«è·³éï¼ˆå› ç‚ºå‰é¢å·²ç¶“å¤±æ•—ï¼‰
df_final.write.saveAsTable("target_table")
```

```
éƒ¨åˆ†å¤±æ•—ç¤ºæ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Notebook 2                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cell 1: âœ… åŸ·è¡ŒæˆåŠŸ        â”‚ â† å·²æäº¤
â”‚ Cell 2: âœ… åŸ·è¡ŒæˆåŠŸ        â”‚ â† å·²æäº¤
â”‚ Cell 3: âŒ ç™¼ç”ŸéŒ¯èª¤        â”‚ â† å¤±æ•—é»
â”‚ Cell 4: â­ï¸ è·³é           â”‚ â† æœªåŸ·è¡Œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ä»»å‹™ä¾è³´é—œä¿‚åœ–

```
        Task 1 âœ…
       /        \
      /          \
Task 2 âš ï¸      Task 3 âœ…
(éƒ¨åˆ†å¤±æ•—)     (å¹³è¡ŒåŸ·è¡Œï¼Œä¸å—å½±éŸ¿)
```

---

## âŒ éŒ¯èª¤é¸é …æ’é™¤

### é¸é … B
**Task 1 will succeed. Task 2 will partially fail. Task 3 will be skipped**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **Task 3 ä¸æœƒè¢«è·³é**ï¼š
   - Task 3 èˆ‡ Task 2 æ˜¯**å¹³è¡Œ**çš„ï¼Œæ²’æœ‰ä¾è³´é—œä¿‚
   - åªæœ‰ Task 2 çš„**ä¸‹æ¸¸ä»»å‹™**æ‰æœƒè¢«è·³é

2. **å¹³è¡Œä»»å‹™çš„è¡Œç‚º**ï¼š
   - å¹³è¡Œä»»å‹™äº’ç›¸ç¨ç«‹åŸ·è¡Œ
   - ä¸€å€‹å¤±æ•—ä¸å½±éŸ¿å¦ä¸€å€‹

---

### é¸é … C
**Task 1 will succeed. Task 2 will completely fail. Task 3 will be skipped**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **Task 2 æ˜¯ã€Œéƒ¨åˆ†å¤±æ•—ã€ï¼Œä¸æ˜¯ã€Œå®Œå…¨å¤±æ•—ã€**ï¼š
   - éŒ¯èª¤å‰çš„ç¨‹å¼ç¢¼å·²ç¶“æˆåŠŸåŸ·è¡Œ
   - Databricks çš„è¨­è¨ˆæ˜¯ partial failure

2. **Task 3 ä¸æœƒè¢«è·³é**ï¼ˆåŒé¸é … Bï¼‰

---

### é¸é … D
**Tasks 1 and 3 will succeed, while Task 2 will completely fail**

**ç‚ºä»€éº¼éŒ¯èª¤ï¼Ÿ**

1. **Task 2 æ˜¯ã€Œéƒ¨åˆ†å¤±æ•—ã€ï¼Œä¸æ˜¯ã€Œå®Œå…¨å¤±æ•—ã€**ï¼š
   - ã€Œcompletely failã€æ„å‘³è‘—æ‰€æœ‰æ“ä½œéƒ½å¤±æ•—
   - å¯¦éš›ä¸ŠéŒ¯èª¤å‰çš„æ“ä½œæ˜¯æˆåŠŸçš„

---

## ğŸ§  è¨˜æ†¶æ³•

### å£è¨£
**ã€Œå¹³è¡Œä»»å‹™å„è‡ªè·‘ï¼Œéƒ¨åˆ†å¤±æ•—ä¸å®Œæ•—ã€**
â†’ å¹³è¡Œä»»å‹™äº’ä¸å½±éŸ¿ï¼›Notebook å¤±æ•—æ˜¯éƒ¨åˆ†å¤±æ•—ï¼ˆéŒ¯èª¤å‰çš„ç¨‹å¼ç¢¼æœƒæˆåŠŸï¼‰

### Multi-task Job å¤±æ•—è¡Œç‚º

| æƒ…æ³ | çµæœ |
|------|------|
| **ä¸‹æ¸¸ä»»å‹™** | è¢«è·³é |
| **å¹³è¡Œä»»å‹™** | ç¹¼çºŒåŸ·è¡Œ |
| **å¤±æ•—ä»»å‹™æœ¬èº«** | éƒ¨åˆ†å¤±æ•—ï¼ˆéŒ¯èª¤å‰çš„ç¨‹å¼ç¢¼å·²åŸ·è¡Œï¼‰ |

### ä»»å‹™ä¾è³´é¡å‹

```
1. ç·šæ€§ä¾è³´ï¼š
   A â†’ B â†’ C
   å¦‚æœ B å¤±æ•—ï¼ŒC è¢«è·³é

2. å¹³è¡ŒåŸ·è¡Œï¼š
   A â†’ B
   A â†’ C
   å¦‚æœ B å¤±æ•—ï¼ŒC ä¸å—å½±éŸ¿

3. è¤‡åˆä¾è³´ï¼š
   A â†’ B â†’ D
   A â†’ C â†’ D
   å¦‚æœ B å¤±æ•—ï¼ŒC ç¹¼çºŒåŸ·è¡Œï¼ŒD è¢«è·³é
```

### ä¿®å¾©å¤±æ•—ä»»å‹™

```python
# ä½¿ç”¨ Repair Run åŠŸèƒ½
# åªé‡æ–°åŸ·è¡Œå¤±æ•—çš„ä»»å‹™å’Œå…¶ä¸‹æ¸¸ä»»å‹™

# UIï¼šJobs â†’ Run â†’ Repair Run
# APIï¼šPOST /api/2.1/jobs/runs/repair

# æ³¨æ„ï¼šéƒ¨åˆ†å¤±æ•—çš„ä»»å‹™æœƒå¾é ­é‡æ–°åŸ·è¡Œ
# éŒ¯èª¤å‰å·²æˆåŠŸçš„æ“ä½œæœƒé‡è¤‡åŸ·è¡Œ
```

### éƒ¨åˆ†å¤±æ•—çš„å½±éŸ¿

| æ“ä½œé¡å‹ | éƒ¨åˆ†å¤±æ•—çš„é¢¨éšª |
|---------|---------------|
| **è®€å–æ“ä½œ** | ä½é¢¨éšªï¼Œé‡è¤‡åŸ·è¡Œç„¡å‰¯ä½œç”¨ |
| **å¯«å…¥æ“ä½œï¼ˆoverwriteï¼‰** | ä½é¢¨éšªï¼Œè¦†è“‹ä¹‹å‰çš„çµæœ |
| **å¯«å…¥æ“ä½œï¼ˆappendï¼‰** | âš ï¸ é«˜é¢¨éšªï¼Œå¯èƒ½ç”¢ç”Ÿé‡è¤‡è³‡æ–™ |
| **å¤–éƒ¨ API å‘¼å«** | âš ï¸ éœ€è¦è€ƒæ…®å†ªç­‰æ€§ |

---

## ğŸ“š å®˜æ–¹æ–‡ä»¶

- [Multi-task Jobs](https://docs.databricks.com/en/workflows/jobs/create-run-jobs.html)
- [Repair Job Failures](https://docs.databricks.com/en/workflows/jobs/repair-job-failures.html)
- [Task Dependencies](https://docs.databricks.com/en/workflows/jobs/create-run-jobs.html#add-tasks)

---

**[è¿”å›é¡Œç›®](#question-039)**
