# Question 13

## Question
A data engineer is building a streaming pipeline using Databricks Autoloader to ingest JSON files from an S3 bucket into a target Delta table. The engineer wants the pipeline to automatically handle problematic files and store them separately so they can be inspected later. Specifically, the engineer wants to:



- 

Exclude files that are badly-formed JSON.
- 

Exclude files that do not match the expected schema.




Which of the following code blocks meets the specified requirement?

## Options
- A. ```
- df = (spark.readStream
-             .format("cloudFiles")
-             .option("cloudFiles.format", "json")
-             .option("badRecordsPath", "s3://project/quarantine")
-             .schema("id int, value double")
-             .load("s3://project/source/"))
``` (Correct)
- B. ```
- df = (spark.readStream
-             .format("cloudFiles")
-             .option("cloudFiles.format", "json")
-             .option("cloudFiles.schemaLocation", "s3://project/schema")
-             .option("pathGlobFilter", "*.json", "s3://project/quarantine")
-             .load("s3://project/source/"))
``` 
- C. ```
- df = (spark.readStream
-             .format("cloudFiles")
-             .option("cloudFiles.format", "json")
-             .schema("id int, value double")
-             .rescue("s3://project/quarantine")
-             .load("s3://project/source/"))
``` 
- D. ```
- df = (spark.readStream
-             .format("cloudFiles")
-             .option("cloudFiles.format", "json")
-             .option("cloudFiles.schemaEvolutionMode", "rescue", "s3://project/quarantine")
-             .schema("id int, value double")
-             .load("s3://project/source/"))
``` 

## Explanation
The `badRecordsPath` option is the standard configuration for handling bad records in Auto Loader:




1. Handling Badly-Formed Files (e.g., Syntax Errors)

The `badRecordsPath` option is a Spark standard for the JSON format (and other formats like CSV). When set, any file that cannot be parsed due to malformed syntax (e.g., non-JSON content, missing brackets, extra commas) is moved to the specified path, fulfilling the requirement for excluding badly-formed JSON.




2. Handling Schema Mismatches (e.g., Data Type Errors, Missing Fields)

When `badRecordsPath` is set, any record that results in an error during parsing (including schema mismatch errors like failed type casting) is automatically written to the specified quarantine location instead of being dropped or failing the stream.
