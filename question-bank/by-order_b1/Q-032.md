# Question 32

## Question
A data engineer wants to ingest input json data into a target Delta table. They want the data ingestion to happen incrementally in near real-time.




Which option correctly meets the specified requirement ?

## Options
- A. spark.readStream

           .format("cloudFiles")

           .option ("cloudFiles.format", "json")

           .load(source_path)

.writeStream

           .trigger(availableNow=True)

           .start("target_table") 
- B. spark.readStream

           .format("autoloader")

           .option ("autoloader.format", "json")

           .load(source_path)

.writeStream

           .option("checkpointLocation", checkpointPath)

           .trigger(real-time=True)

           .start("target_table") 
- C. spark.readStream

           .format("autoloader")

           .option ("autoloader.format", "json")

           .load(source_path)

.writeStream

           .option("checkpointLocation", checkpointPath)

           .start("target_table") 
- D. spark.readStream

           .format("cloudFiles")

           .option ("cloudFiles.format", "json")

           .load(source_path)

.writeStream

           .option("checkpointLocation", checkpointPath)

           .start("target_table") (Correct)

## Explanation
In order to ingest input json data into a target Delta table, we use Autoloader. Auto Loader is based on Spark Structured Streaming and provides a Structured Streaming source called 'cloudFiles'.

If you want the data ingestion to happen incrementally in near real-time, you can use the default trigger method which is trigger(processingTime="500ms"). This allows the processing of data in micro-batches at a fixed interval of half a second.




Reference:

https://docs.databricks.com/ingestion/auto-loader/index.html

https://docs.databricks.com/structured-streaming/triggers.html#what-is-the-default-trigger-interval




Study materials from our exam preparation course on Udemy:

Lecture (Associate course)

Hands-on (Associate course)

Lecture (Associate course)
