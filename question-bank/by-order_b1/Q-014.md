# Question 14

## Question
The data engineering team wants to build a pipeline that receives customers data as change data capture (CDC) feed from a source system. The CDC events logged at the source contain the data of the records along with metadata information. This metadata indicates whether the specified record was inserted, updated, or deleted. In addition to a timestamp column identified by the field update_time indicating the order in which the changes happened. Each record has a primary key identified by the field customer_id.




In the same batch, multiple changes for the same customer could be received with different update_time. The team wants to store only the most recent information for each customer in the target Delta Lake table.




Which of the following solutions meets these requirements?

## Options
- A. Use MERGE INTO to upsert the most recent entry for each customer_id into the table (Correct)
- B. Use dropDuplicates function to remove duplicates by customer_id, then merge the duplicate records into the table. 
- C. Use MERGE INTO with SEQUENCE BY clause on the update_time for ordering how operations should be applied 
- D. Enable Delta Lake's Change Data Feed (CDF) on the target table to automatically merge the received CDC feed 

## Explanation
`MERGE INTO` command allows you to upsert data from a source table, view, or DataFrame into a target Delta table. Delta Lake supports inserts, updates, and deletes in merge operations.




Note: The option to use SEQUENCE BY with MERGE INTO is incorrect because this clause only applies to AUTO CDC and APPLY CHANGES INTO, not to MERGE statements. Attempting to use SEQUENCE BY with MERGE INTO will result in a syntax error.




Reference:

https://docs.databricks.com/delta/merge.html







Study materials from our exam preparation course on Udemy:

Lecture

Hands-on
