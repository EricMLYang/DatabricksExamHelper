# Question 40

## Question
A data engineering team is building a LDP pipeline to clean and validate hotel reservations data. Some completed reservations have null check-in or check-out dates, which violates business rules.




To handle this, they implemented the following code:



```
- rules = {
-     "valid_check_in": "(check_in IS NOT NULL)",
-     "valid_check_out": "(check_out IS NOT NULL)",
- }
- quarantine_rules = "NOT({0})".format(" AND ".join(rules.values()))
-  
- @dlt.table(partition_cols=["is_quarantined"])
- @dlt.expect_all(rules)
- def silver_reservations():
- return (
-     spark.readStream.table("bronze_reservations")
-                      .withColumn("is_quarantined", expr(quarantine_rules))
- )
```





Which of the following correctly describes what this function does?

## Options
- A. This function streams all rows into the silver_reservations table, flags those with missing check-in or check-out values as quarantined, and partitions the table by the is_quarantined flag. (Correct)
- B. This function partitions the bronze_reservations table by the is_quarantined flag, streams valid partitions into the silver_reservations table, and drops the invalid partitions. 
- C. This function streams only rows with valid check-in and check-out values into the silver_reservations table, while writing invalid rows into a separate partition. 
- D. This function streams rows based on the quarantine_rules into two separate tables: silver_reservations for valid reservations, and is_quarantined for invalid reservations. 

## Explanation
This function streams all rows from the bronze_reservations table into the silver_reservations table, adds a new Boolean column called is_quarantined to flag records with missing check-in or check-out dates, and partitions the table by that flag.




The `dlt.expect_all(rules)` decorator applies data quality expectations but does not drop invalid rows; it simply records the validation results for monitoring purposes. As a result, both valid and invalid records are retained in the same table, making it easy to trace and fix data quality issues without losing information.




This design is a common pattern in Lakeflow Declarative Pipelines for managing data quality. Instead of discarding bad data outright, teams often quarantine it within the same dataset by tagging and partitioning. This allows for continuous ingestion and validation of streaming data while supporting later review or remediation of problematic records, ensuring both data reliability and auditability in production data pipelines.




Note: Databricks has recenlty open-sourced this solution, integrating it into the Apache Spark ecosystem under the name Spark Declarative Pipelines (SDP).
