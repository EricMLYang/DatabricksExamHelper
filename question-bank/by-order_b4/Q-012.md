# Question 12

## Question
The data engineering team has a large Delta Lake table named 'user_posts' which is partitioned over the 'year' column. The table is used as an input streaming source in a streaming job. The streaming query is displayed below with a blank:



```

- spark.readStream
-        .table("user_posts")
-        ________________
-        .groupBy("post_category", "post_date")
-        .agg(
-            count("psot_id").alias("posts_count"),
-            sum("likes").alias("total_likes"))
-     .writeStream
-        .option("checkpointLocation", "dbfs:/path/checkpoint")
-        .table("psots_stats")
```





They want to remove previous 2 years data from the table without breaking the append-only requirement of streaming sources.




Which option correctly fills in the blank to enable stream processing from the table after deleting the partitions?

## Options
- A. .window("year", "INTERVAL 2 YEARS") 
- B. .option("ignoreDeletes", True) (Correct)
- C. .withWatermark("year", "INTERVAL 2 YEARS") 
- D. .option("ignoreDeletes", "year") 

## Explanation
Partitioning on datetime columns can be leveraged when removing data older than a certain age from the table. For example, you can decide to delete previous years data. In this case, file deletion will be cleanly along partition boundaries.




However, if you are using this table as a streaming source, deleting data breaks the append-only requirement of streaming sources, which makes the table no more streamable. To avoid this, you can use the ignoreDeletes option when streaming from this table. This option enables streaming processing from Delta tables with partition deletes.

`option("ignoreDeletes", True)`




Study materials from our exam preparation course on Udemy:

Lecture

Hands-on
