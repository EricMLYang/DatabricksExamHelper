# Question 37

## Question
An IoT company processes live sensor readings from thousands of devices using a streaming pipeline. Occasionally, devices send corrupted or incomplete events that fail schema validation. The engineering team must ensure that production analytics dashboards, which rely on clean data, continue to update in real time. However, the corrupted records should still be captured for later investigation, using minimal computing resources.




What should the engineers do to meet these requirements?

## Options
- A. Filter out corrupted events in the main real-time stream and write only valid records to the production tables. Create a separate lightweight process that periodically reads and stores the corrupted messages for analysis. (Correct)
- B. Include all data, valid or not, in the main stream and use a flag to mark corrupted records. 
- C. Merge both valid and invalid data into the same Delta table and use downstream queries to apply data quality rules to exclude invalid entries from dashboards. 
- D. Add retry logic to the main stream so that it attempts to reprocess corrupted messages until they succeed. 

## Explanation
The engineers should filter out corrupted or incomplete events from the main real-time streaming pipeline and write only the valid records to the production analytics tables, ensuring that dashboards continue to update accurately and without delay. At the same time, they should implement a separate lightweight process that periodically collects and stores the corrupted messages for later investigation, such as debugging or auditing.




This design maintains the integrity and performance of the real-time analytics system by preventing invalid data from affecting dashboards, while still preserving all incoming data for offline analysis, and it does so efficiently without overloading computing resources or complicating the main pipeline.




Study materials from our exam preparation course on Udemy:
- 

Hands-on
