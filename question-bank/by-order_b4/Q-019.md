# Question 19

## Question
Given the following query:



```

- spark.table("stream_sink")
-         .filter("recent = true")
-         .dropDuplicates(["item_id", "item_timestamp"])
-     .write
-         .mode ("overwrite")
-         .table("stream_data_stage")
```





Which statement describes the result of executing this query ?

## Options
- A. An incremental job will overwrite the stream_sink table by those deduplicated records from stream_data_stage that have been added since the last time the job was run. 
- B. A batch job will overwrite the stream_data_stage table by deduplicated records calculated from all "recent" items in the stream_sink table (Correct)
- C. An incremental job will overwrite the stream_data_stage table by those deduplicated records from stream_sink that have been added since the last time the job was run. 
- D. A batch job will overwrite the stream_data_stage table by those deduplicated records from stream_sink that have been added since the last time the job was run. 

## Explanation
Reading a Delta table using spark.table() function means that you are reading it as a static source. So, each time you run the query, all records in the current version of the 'stream_sink' table will be read, filtered and deduplicated.




There is no difference between spark.table() and spark.read.table() function. Actually, spark.read.table() internally calls spark.table().




The query in the question then writes the data in mode "overwrite" to the 'stream_data_stage' table, which completely overwrites the table at each execution.




Study materials from our exam preparation course on Udemy:

Hands-on
