# Question 53

## Question
A data engineer has the following logic to handle duplicates in Spark Structured Streaming:



```
(spark.readStream
       .table("bronze")
       .filter("topic = 'orders'")
       .select(F.from_json(F.col("value").cast("string"), schema).alias("v"))
       .select("v.*")
       .withWatermark("order_timestamp", "30 seconds")
       .dropDuplicates(["order_id", "order_timestamp"]))
```





However, they notice that this logic is not sufficient to prevent duplicates for events that arrive later than the watermark threshold.




Which of the following code snippets can the data engineer include in a `foreachBatch` function to completely handle streaming duplicates?

## Options
- A. ```
MERGE INTO orders_silver a
USING microbatch b
ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp
WHEN NOT MATCHED THEN INSERT *
``` (Correct)
- B. ```
(spark.readStream
.table("microbatch")
.withWatermark("order_timestamp", "7 days")
.dropDuplicates(["order_id", "order_timestamp"]))
``` 
- C. ```
APPLY CHANGES INTO orders_silver a
FROM STREAM(microbatch)
KEYS (order_id, order_timestamp)
SEQUENCE BY order_timestamp
COLUMNS *
``` 
- D. ```
COPY INTO orders_silver
FROM microbatch
DISTINCT ALL
COPY_OPTIONS ('mergeSchema' = 'true');
``` 

## Explanation
In Spark Structured Streaming, dropDuplicates with a watermark only removes duplicates that arrive within the defined event-time threshold, for example, within 30 seconds of order_timestamp. However, any records arriving later than that threshold are considered "too late" and are not deduplicated by Spark's in-memory state. To ensure complete deduplication (including very late-arriving data), the foreachBatch sink can use an idempotent write pattern with Delta Lake's MERGE operation.




The MERGE INTO statement compares each micro-batch of incoming data (microbatch) with the target Delta table (orders_silver) based on unique keys (order_id and order_timestamp). It only inserts rows that do not already exist in the target table, preventing duplicates even across micro-batches or late arrivals. This combination of in-stream deduplication (for near-real-time performance) and MERGE-based deduplication (for completeness and correctness) provides an end-to-end reliable way to handle duplicates in streaming pipelines.




Study materials from our exam preparation course on Udemy:

Hands-on
