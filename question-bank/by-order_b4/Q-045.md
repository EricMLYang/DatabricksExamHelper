# Question 45

## Question
A data engineer is using the following Auto Loader stream to incrementally ingest large JSON files. These files cause long micro-batch processing times and occasional memory issues:



```
df = (spark.readStream
            .format("cloudFiles")
            .option("cloudFiles.format", "json")
            _________________________________
            .load("s3://project/source/"))
```





They want to process only a portion of the data per micro-batch, improving stability and keeping batch times predictable.




Which option correctly fills in the blank to process only 128 MB of data per micro-batch?

## Options
- A. .option('cloudFiles.maxBytesPerTrigger', '128mb') (Correct)
- B. .option('cloudFiles.maxDataPerTrigger', '128mb') 
- C. .option('triggerInterval', '128mb') 
- D. .option('batchSize', '128mb') 

## Explanation
In Auto Loader, `cloudFiles.maxBytesPerTrigger` controls the maximum amount of data to process in each micro-batch, allowing the stream to handle large files incrementally and keep batch processing times predictable.
