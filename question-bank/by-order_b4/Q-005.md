# Question 5

## Question
A junior data engineer creates a Databricks job with 15 notebook tasks, each performing the same data validation logic on 15 different tables. Each task depends on the completion of the previous one, making the workflow long and difficult to maintain.




What would be a more efficient and scalable solution for this use case?

## Options
- A. Use a foreach task to run the same validation notebook for each table in parallel, passing the table name as a parameter (Correct)
- B. Schedule 15 separate jobs instead of having multiple tasks in one job 
- C. Configure the 15 notebook tasks to run in parallel, each with a separate cluster configuration 
- D. Combine all table validations into one large notebook and loop through all tables sequentially 

## Explanation
A more efficient and scalable solution in this scenario is to use a For Each task. The For Each task allows you to run a nested task in a loop, passing different parameters to each iteration. In this case, the data engineer can pass each table name as a parameter, running the same validation notebook for all tables. This approach reduces maintenance overhead, and allows the validations to run concurrently to avoid sequential dependencies, making the workflow faster and easier to manage.
