# ğŸ¯ Databricks èªè­‰è€ƒè©¦ - ç°¡å–®ç‰ˆæ¨¡æ“¬æ¸¬è©¦ï¼ˆ10é¡Œï¼‰

---

## ğŸ“‹ æ¸¬è©¦èªªæ˜

- **é¡Œæ•¸ï¼š** 10 é¡Œ
- **é›£åº¦åˆ†å¸ƒï¼š** L1-Basic (70%) + L2-Intermediate (30%)
- **å»ºè­°ä½œç­”æ™‚é–“ï¼š** 15-20 åˆ†é˜
- **åŠæ ¼æ¨™æº–ï¼š** 7/10 é¡Œæ­£ç¢ºï¼ˆ70%ï¼‰

---

## âœï¸ ä½œç­”å€

è«‹åœ¨ä¸‹æ–¹è¨˜éŒ„ä½ çš„ç­”æ¡ˆï¼š

| é¡Œè™Ÿ | ä½ çš„ç­”æ¡ˆ | æ­£ç¢ºç­”æ¡ˆ | âœ“/âœ— |
|------|---------|---------|-----|
| Q1   |         | (æ‘ºç–Š)  |     |
| Q2   |         | (æ‘ºç–Š)  |     |
| Q3   |         | (æ‘ºç–Š)  |     |
| Q4   |         | (æ‘ºç–Š)  |     |
| Q5   |         | (æ‘ºç–Š)  |     |
| Q6   |         | (æ‘ºç–Š)  |     |
| Q7   |         | (æ‘ºç–Š)  |     |
| Q8   |         | (æ‘ºç–Š)  |     |
| Q9   |         | (æ‘ºç–Š)  |     |
| Q10  |         | (æ‘ºç–Š)  |     |

---

## ğŸ“ æ¸¬è©¦é¡Œç›®

### Q1 - Structured Streaming æ¦‚å¿µï¼ˆL1-Basicï¼‰

Which statement characterizes the general programming model used by Spark Structured Streaming?

**é¸é …ï¼š**
- **A.** Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.
- **B.** Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.
- **C.** Structured Streaming uses specialized hardware and I/O streams to achieve sub-second latency for data transfer.
- **D.** Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.
- **E.** Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.

---

### Q2 - Notebook æ¬Šé™ç®¡ç†ï¼ˆL1-Basicï¼‰

The DevOps team has configured a production workload as a collection of notebooks scheduled to run daily using the Jobs UI.

A new data engineering hire is onboarding to the team and has requested access to one of these notebooks to review the production logic.

**What are the maximum notebook permissions that can be granted to the user without allowing accidental changes to production code or data?**

**é¸é …ï¼š**
- **A.** Can Manage
- **B.** Can Edit
- **C.** No permissions
- **D.** Can Read
- **E.** Can Run

---

### Q3 - Table Metadata æª¢è¦–ï¼ˆL1-Basicï¼‰

The data governance team has instituted a requirement that all tables containing Personal Identifiable Information (PII) must be clearly annotated.

This includes adding column comments, table comments, and setting the custom table property "contains_pii" = true.

The following SQL DDL statement is executed to create a new table:

```sql
CREATE TABLE dev.pii_test
(id INT, name STRING COMMENT "PII")
COMMENT "Contains PII"
TBLPROPERTIES ('contains_pii' = 'True')
```

**Which command allows manual confirmation that these three requirements have been met?**

**é¸é …ï¼š**
- **A.** `DESCRIBE EXTENDED dev.pii_test`
- **B.** `DESCRIBE DETAIL dev.pii_test`
- **C.** `SHOW TBLPROPERTIES dev.pii_test`
- **D.** `DESCRIBE HISTORY dev.pii_test`
- **E.** `SHOW TABLES dev`

---

### Q4 - Jobs åŸ·è¡Œæ­·å²ä¿ç•™ï¼ˆL1-Basicï¼‰

What statement is true regarding the retention of job run history?

**é¸é …ï¼š**
- **A.** It is retained until you export or delete job run logs
- **B.** It is retained for 30 days, during which time you can deliver job run logs to DBFS or S3
- **C.** It is retained for 60 days, during which you can export notebook run results to HTML
- **D.** It is retained for 60 days, after which logs are archived
- **E.** It is retained for 90 days or until the run-id is re-used through custom run configuration

---

### Q5 - Python å¥—ä»¶æ ¼å¼ï¼ˆL1-Basicï¼‰

Which distribution does Databricks support for installing custom Python code packages?

**é¸é …ï¼š**
- **A.** sbt
- **B.** CRAN
- **C.** npm
- **D.** Wheels
- **E.** jars

---

### Q6 - Integration Testing å®šç¾©ï¼ˆL1-Basicï¼‰

Which statement describes integration testing?

**é¸é …ï¼š**
- **A.** Validates interactions between subsystems of your application
- **B.** Requires an automated testing framework
- **C.** Requires manual intervention
- **D.** Validates an application use-case
- **E.** Validates behavior of individual elements of your application

---

### Q7 - Notebook æ’ç¨‹å„ªåŒ–ï¼ˆL1-Basicï¼‰

A member of the data engineering team has submitted a short notebook that they wish to schedule as part of a larger data pipeline.

Assume that the commands provided below produce the logically correct results when run as presented.

```python
Cmd 1: rawDF = spark.table("raw_data")
Cmd 2: rawDF.printSchema()
Cmd 3: flattenedDF = rawDF.select("*", "values.*")
Cmd 4: finalDF = flattenedDF.drop("values")
Cmd 5: finalDF.explain()
Cmd 6: display(finalDF)
Cmd 7: finalDF.write.mode("append").saveAsTable("flat_data")
```

**Which command should be removed from the notebook before scheduling it as a job?**

**é¸é …ï¼š**
- **A.** Cmd 2
- **B.** Cmd 3
- **C.** Cmd 4
- **D.** Cmd 5
- **E.** Cmd 6

---

### Q8 - Audit Logs èˆ‡ REST APIï¼ˆL2-Intermediateï¼‰

A data engineer, User A, has promoted a new pipeline to production by using the REST API to programmatically create several jobs.

A DevOps engineer, User B, has configured an external orchestration tool to trigger job runs through the REST API.

Both users authorized the REST API calls using their personal access tokens.

**Which statement describes the contents of the workspace audit logs concerning these events?**

**é¸é …ï¼š**
- **A.** Because the REST API was used for job creation and triggering runs, a Service Principal will be automatically used to identify these events.
- **B.** Because User B last configured the jobs, their identity will be associated with both the job creation events and the job run events.
- **C.** Because these events are managed separately, User A will have their identity associated with the job creation events and User B will have their identity associated with the job run events.
- **D.** Because the REST API was used for job creation and triggering runs, user identity will not be captured in the audit logs.
- **E.** Because User A created the jobs, their identity will be associated with both the job creation events and the job run events.

---

### Q9 - REST API ç«¯é»é¸æ“‡ï¼ˆL2-Intermediateï¼‰

Which REST API call can be used to review the notebooks configured to run as tasks in a multi-task job?

**é¸é …ï¼š**
- **A.** `/jobs/runs/list`
- **B.** `/jobs/runs/get-output`
- **C.** `/jobs/runs/get`
- **D.** `/jobs/get`
- **E.** `/jobs/list`

---

### Q10 - Databricks Repos åˆ†æ”¯ç®¡ç†ï¼ˆL2-Intermediateï¼‰

A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.

**Which approach will allow this developer to review the current logic for this notebook?**

**é¸é …ï¼š**
- **A.** Use Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9
- **B.** Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch
- **C.** Use Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch
- **D.** Use Repos to push all changes and merge the latest version of main into dev-2.3.9
- **E.** Use Repos to checkout a new local branch and rebase it from dev-2.3.9

---

## ğŸ“Š ç­”æ¡ˆèˆ‡è§£æ

<details>
<summary><b>é»æ“Šå±•é–‹ç­”æ¡ˆèˆ‡ç°¡è¦è§£æ</b></summary>

### Q1 ç­”æ¡ˆï¼šD
**è§£æï¼š** Structured Streaming çš„æ ¸å¿ƒæ¦‚å¿µæ˜¯å°‡è³‡æ–™æµè¦–ç‚ºã€Œä¸æ–·è¿½åŠ æ–°åˆ—çš„ç„¡ç•Œè¡¨ï¼ˆunbounded tableï¼‰ã€ã€‚é€™æ˜¯ Spark Structured Streaming çš„åŸºç¤æ¦‚å¿µï¼Œä¹Ÿæ˜¯å®ƒèˆ‡å‚³çµ±ä¸²æµè™•ç†çš„ä¸»è¦å€åˆ¥ã€‚

**è¨˜æ†¶æ³•ï¼š** ä¸²æµ = ç„¡é™è¿½åŠ çš„è¡¨æ ¼

**åƒè€ƒï¼š** [Structured Streaming Programming Guide](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)

---

### Q2 ç­”æ¡ˆï¼šD
**è§£æï¼š** `Can Read` æ¬Šé™å…è¨±ä½¿ç”¨è€…æŸ¥çœ‹ notebook å…§å®¹ä½†ç„¡æ³•åŸ·è¡Œæˆ–ä¿®æ”¹ï¼Œæ˜¯æœ€å®‰å…¨çš„æª¢è¦–æ¬Šé™ã€‚`Can Run` æœƒå…è¨±åŸ·è¡Œï¼ˆå¯èƒ½å½±éŸ¿è³‡æ–™ï¼‰ï¼Œ`Can Edit` å…è¨±ä¿®æ”¹ï¼ˆå±éšªï¼‰ã€‚

**è¨˜æ†¶æ³•ï¼š** åªçœ‹ä¸å‹• = Can Read

**åƒè€ƒï¼š** [Notebook Access Control](https://docs.databricks.com/security/access-control/workspace-acl.html)

---

### Q3 ç­”æ¡ˆï¼šA
**è§£æï¼š** `DESCRIBE EXTENDED` æœƒé¡¯ç¤ºè¡¨çš„å®Œæ•´å…ƒè³‡æ–™ï¼ŒåŒ…æ‹¬ï¼šæ¬„ä½åŠå…¶è¨»è§£ã€è¡¨è¨»è§£ã€è¡¨å±¬æ€§ï¼ˆTBLPROPERTIESï¼‰ã€‚å…¶ä»–æŒ‡ä»¤åŠŸèƒ½ä¸åŒï¼š
- `DESCRIBE DETAIL`ï¼šDelta Lake ç‰¹å®šè³‡è¨Šï¼ˆæª”æ¡ˆä½ç½®ã€æ ¼å¼ç­‰ï¼‰
- `SHOW TBLPROPERTIES`ï¼šåªé¡¯ç¤ºè¡¨å±¬æ€§ï¼Œä¸å«æ¬„ä½è¨»è§£
- `DESCRIBE HISTORY`ï¼šé¡¯ç¤º Delta æ“ä½œæ­·å²

**è¨˜æ†¶æ³•ï¼š** EXTENDED = æ“´å……å®Œæ•´ç‰ˆï¼Œä»€éº¼éƒ½æœ‰

**åƒè€ƒï¼š** [DESCRIBE TABLE](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-aux-describe-table.html)

---

### Q4 ç­”æ¡ˆï¼šB æˆ– Cï¼ˆç¤¾ç¾¤æŠ•ç¥¨åˆ†æ­§ï¼‰
**è§£æï¼š** å®˜æ–¹æ–‡ä»¶æŒ‡å‡º job run history ä¿ç•™ **30 å¤©**ï¼Œä½†ç¤¾ç¾¤è¨±å¤šè€ƒç”Ÿå›å ±ç­”æ¡ˆæ˜¯ Cï¼ˆ60å¤©ï¼‰ã€‚é€™é¡Œåœ¨çœŸå¯¦è€ƒè©¦ä¸­æœ‰çˆ­è­°ï¼Œå»ºè­°è¨˜ä½å…©å€‹æ•¸å­—ã€‚

**è¨˜æ†¶æ³•ï¼š** 30å¤© æˆ– 60å¤©ï¼ˆè€ƒè©¦æ™‚é¸è¼ƒé•·çš„ï¼‰

**åƒè€ƒï¼š** [Job Run History](https://docs.databricks.com/workflows/jobs/jobs.html#view-job-run-history)

---

### Q5 ç­”æ¡ˆï¼šD
**è§£æï¼š** Databricks æ”¯æ´å®‰è£ **Wheels**ï¼ˆ.whl æª”æ¡ˆï¼‰ï¼Œé€™æ˜¯ Python å¥—ä»¶çš„æ¨™æº–ç™¼è¡Œæ ¼å¼ã€‚å…¶ä»–é¸é …ï¼š
- sbtï¼šScala å»ºç½®å·¥å…·
- CRANï¼šR èªè¨€å¥—ä»¶åº«
- npmï¼šNode.js å¥—ä»¶ç®¡ç†å™¨
- jarsï¼šJava/Scala å¥—ä»¶æ ¼å¼ï¼ˆä¸æ˜¯ Pythonï¼‰

**è¨˜æ†¶æ³•ï¼š** Python ç”¨ Wheelsï¼ˆè¼ªå­ï¼‰ï¼Œä¸æ˜¯ jarï¼ˆç½å­ï¼‰

**åƒè€ƒï¼š** [Libraries](https://docs.databricks.com/libraries/index.html)

---

### Q6 ç­”æ¡ˆï¼šA
**è§£æï¼š** Integration Testingï¼ˆæ•´åˆæ¸¬è©¦ï¼‰é©—è­‰**å­ç³»çµ±ä¹‹é–“çš„äº’å‹•**ã€‚å°æ¯”ï¼š
- Unit Testingï¼šæ¸¬è©¦å–®ä¸€å…ƒä»¶
- End-to-End Testingï¼šæ¸¬è©¦å®Œæ•´ä½¿ç”¨å ´æ™¯

**è¨˜æ†¶æ³•ï¼š** Integration = å­ç³»çµ±æ•´åˆäº’å‹•

**åƒè€ƒï¼š** [Testing Strategies](https://docs.databricks.com/dev-tools/testing.html)

---

### Q7 ç­”æ¡ˆï¼šE
**è§£æï¼š** `display()` æ˜¯ Databricks notebook çš„**äº’å‹•å¼é¡¯ç¤ºæŒ‡ä»¤**ï¼Œåªåœ¨ notebook UI ä¸­æœ‰æ•ˆï¼Œåœ¨æ’ç¨‹çš„ Job ä¸­æœƒå ±éŒ¯æˆ–ç„¡æ•ˆã€‚æ‡‰ç§»é™¤æ‰€æœ‰é™¤éŒ¯/é¡¯ç¤ºç”¨çš„æŒ‡ä»¤ï¼š
- `display()`ï¼šäº’å‹•å¼é¡¯ç¤º
- `printSchema()`ï¼šå¯ä¿ç•™ï¼ˆæœ‰åŠ©æ–¼é™¤éŒ¯æ—¥èªŒï¼‰
- `explain()`ï¼šå¯ä¿ç•™ï¼ˆæœ‰åŠ©æ–¼æ•ˆèƒ½é™¤éŒ¯ï¼‰

**è¨˜æ†¶æ³•ï¼š** display() = çµ¦äººçœ‹çš„ï¼ŒJob ä¸éœ€è¦

**åƒè€ƒï¼š** [Notebooks Best Practices](https://docs.databricks.com/notebooks/best-practices.html)

---

### Q8 ç­”æ¡ˆï¼šC
**è§£æï¼š** Audit logs æœƒè¨˜éŒ„**å¯¦éš›åŸ·è¡Œæ“ä½œçš„ä½¿ç”¨è€…èº«ä»½**ï¼ˆé€éå€‹äºº access tokenï¼‰ï¼š
- User A å»ºç«‹ jobs â†’ è¨˜éŒ„ User A
- User B è§¸ç™¼åŸ·è¡Œ â†’ è¨˜éŒ„ User B

**è¨˜æ†¶æ³•ï¼š** èª°çš„ token â†’ èª°çš„è¨˜éŒ„

**åƒè€ƒï¼š** [Audit Logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)

---

### Q9 ç­”æ¡ˆï¼šD
**è§£æï¼š** `/jobs/get` API æœƒå›å‚³ job çš„**å®Œæ•´è¨­å®š**ï¼ŒåŒ…æ‹¬æ‰€æœ‰ task é…ç½®ï¼ˆnotebook è·¯å¾‘ã€åƒæ•¸ç­‰ï¼‰ã€‚å°æ¯”ï¼š
- `/jobs/list`ï¼šåˆ—å‡ºæ‰€æœ‰ jobsï¼ˆç°¡ç•¥è³‡è¨Šï¼‰
- `/jobs/runs/list`ï¼šåˆ—å‡ºåŸ·è¡Œè¨˜éŒ„
- `/jobs/runs/get`ï¼šå–å¾—å–®æ¬¡åŸ·è¡Œçš„è©³ç´°è³‡è¨Š

**è¨˜æ†¶æ³•ï¼š** çœ‹è¨­å®š = /jobs/getï¼Œçœ‹åŸ·è¡Œ = /jobs/runs/*

**åƒè€ƒï¼š** [Jobs API](https://docs.databricks.com/api/workspace/jobs/get)

---

### Q10 ç­”æ¡ˆï¼šB
**è§£æï¼š** ç•¶é ç«¯åˆ†æ”¯å­˜åœ¨ä½†æœ¬åœ°çœ‹ä¸åˆ°æ™‚ï¼Œéœ€è¦ï¼š
1. **Pull from remote**ï¼ˆæ‹‰å–é ç«¯è®Šæ›´ï¼‰
2. **åˆ‡æ›åˆ°è©²åˆ†æ”¯**

é¸é … Aã€Cã€E éƒ½åŒ…å«ä¸å¿…è¦æˆ–éŒ¯èª¤çš„æ“ä½œï¼ˆpull requestã€auto-resolve conflictsã€rebaseï¼‰ã€‚

**è¨˜æ†¶æ³•ï¼š** çœ‹ä¸åˆ°åˆ†æ”¯ â†’ Pull + Checkout

**åƒè€ƒï¼š** [Databricks Repos](https://docs.databricks.com/repos/index.html)

</details>

---

## ğŸ¯ è©•åˆ†æ¨™æº–

- **9-10 é¡Œæ­£ç¢ºï¼š** å„ªç§€ï¼âœ¨ åŸºç¤çŸ¥è­˜ç´®å¯¦
- **7-8 é¡Œæ­£ç¢ºï¼š** è‰¯å¥½ï¼ğŸ‘ å·²é”åŠæ ¼æ¨™æº–
- **5-6 é¡Œæ­£ç¢ºï¼š** éœ€åŠ å¼·ï¼Œå»ºè­°è¤‡ç¿’éŒ¯é¡Œä¸»é¡Œ
- **0-4 é¡Œæ­£ç¢ºï¼š** éœ€è¦ç³»çµ±æ€§å­¸ç¿’ï¼Œå»ºè­°å¾åŸºç¤æ¦‚å¿µé–‹å§‹

---

## ğŸ“š è¤‡ç¿’å»ºè­°

æ ¹æ“šä½ çš„éŒ¯é¡Œï¼Œå»ºè­°è¤‡ç¿’ä»¥ä¸‹ä¸»é¡Œï¼š
- **Q1 éŒ¯èª¤ï¼š** è¤‡ç¿’ [Structured Streaming æ¦‚å¿µ](../docs/core-knowledge/0-åŸºç¤çŸ¥è­˜/delta-lake-cheatsheet.md)
- **Q2-Q4 éŒ¯èª¤ï¼š** è¤‡ç¿’ [Jobs & Workflows](../docs/exam-map/part-2-ç¶­é‹èˆ‡è‡ªå‹•åŒ–.md)
- **Q5-Q7 éŒ¯èª¤ï¼š** è¤‡ç¿’ [Development Practices](../docs/exam-map/part-1-é–‹ç™¼èˆ‡è½‰æ›.md)
- **Q8-Q10 éŒ¯èª¤ï¼š** è¤‡ç¿’ [REST API & Repos](../docs/exam-map/part-2-ç¶­é‹èˆ‡è‡ªå‹•åŒ–.md)

---

## âœ… å®Œæˆæ¸¬è©¦å¾Œ

1. è¨˜éŒ„ä½ çš„åˆ†æ•¸å’ŒéŒ¯é¡Œ
2. æŸ¥çœ‹è©³ç´°è§£æç†è§£æ¦‚å¿µ
3. å°‡éŒ¯é¡Œä¸»é¡ŒåŠ å…¥è¤‡ç¿’æ¸…å–®
4. å»ºè­° 2-3 å¤©å¾Œé‡åšéŒ¯é¡Œ

**ç¥è€ƒè©¦é †åˆ©ï¼ğŸš€**
