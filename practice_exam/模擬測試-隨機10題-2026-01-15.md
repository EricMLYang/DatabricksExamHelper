# ğŸ¯ Databricks èªè­‰æ¨¡æ“¬è€ƒ - 10 é¡Œéš¨æ©Ÿæ¸¬è©¦

**ç”Ÿæˆæ—¥æœŸï¼š** 2026-01-15  
**é¡Œç›®æ•¸é‡ï¼š** 10 é¡Œ  
**é›£åº¦åˆ†å¸ƒï¼š** L1(3é¡Œ) | L2(5é¡Œ) | L3(2é¡Œ)  
**å»ºè­°æ™‚é–“ï¼š** 20 åˆ†é˜

---

## ğŸ“ ç­”é¡Œèªªæ˜

1. åœ¨æ¯é¡Œä¸‹æ–¹çš„ `**ä½ çš„ç­”æ¡ˆï¼š**` è™•å¡«å¯«ç­”æ¡ˆï¼ˆA/B/C/D/Eï¼‰
2. å®Œæˆå¾Œå±•é–‹ã€Œç­”æ¡ˆèˆ‡è§£æã€å€å¡Šæ ¸å°
3. å»ºè­°è¨ˆæ™‚ä½œç­”ï¼Œæ¨¡æ“¬çœŸå¯¦è€ƒè©¦ç’°å¢ƒ

---

## ç¬¬ 1 é¡Œ - Q-023 [L1-Basic]

### é¡Œå¹¹

Which statement characterizes the general programming model used by Spark Structured Streaming?

### é¸é …

- **A.** Structured Streaming leverages the parallel processing of GPUs to achieve highly parallel data throughput.
- **B.** Structured Streaming is implemented as a messaging bus and is derived from Apache Kafka.
- **C.** Structured Streaming uses specialized hardware and I/O streams to achieve sub-second latency for data transfer.
- **D.** Structured Streaming models new data arriving in a data stream as new rows appended to an unbounded table.
- **E.** Structured Streaming relies on a distributed network of nodes that hold incremental state values for cached stages.

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 2 é¡Œ - Q-047 [L1-Basic]

### é¡Œå¹¹

What statement is true regarding the retention of job run history?

### é¸é …

- **A.** It is retained until you export or delete job run logs
- **B.** It is retained for 30 days, during which time you can deliver job run logs to DBFS or S3
- **C.** It is retained for 60 days, during which you can export notebook run results to HTML
- **D.** It is retained for 60 days, after which logs are archived
- **E.** It is retained for 90 days or until the run-id is re-used through custom run configuration

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 3 é¡Œ - Q-056 [L1-Basic]

### é¡Œå¹¹

Which statement describes integration testing?

### é¸é …

- **A.** Validates interactions between subsystems of your application
- **B.** Requires an automated testing framework
- **C.** Requires manual intervention
- **D.** Validates an application use-case
- **E.** Validates behavior of individual elements of your application

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 4 é¡Œ - Q-005 [L2-Intermediate]

### é¡Œå¹¹

A junior developer complains that the code in their notebook isn't producing the correct results in the development environment. A shared screenshot reveals that while they're using a notebook versioned with Databricks Repos, they're using a personal branch that contains old logic. The desired branch named dev-2.3.9 is not available from the branch selection dropdown.

Which approach will allow this developer to review the current logic for this notebook?

### é¸é …

- **A.** Use Repos to make a pull request use the Databricks REST API to update the current branch to dev-2.3.9
- **B.** Use Repos to pull changes from the remote Git repository and select the dev-2.3.9 branch
- **C.** Use Repos to checkout the dev-2.3.9 branch and auto-resolve conflicts with the current branch
- **D.** Delete the current Databricks Repo and clone a new copy
- **E.** Navigate to the desired repo in Git and import the notebook to Workspace

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 5 é¡Œ - Q-012 [L2-Intermediate]

### é¡Œå¹¹

A junior data engineer has configured a workload that posts the following JSON to the Databricks REST API endpoint 2.0/jobs/create.

```json
{
  "name": "Ingest new data",
  "existing_cluster_id": "6015-954420-peace720",
  "notebook_task": {
    "notebook_path": "/Prod/ingest.py"
  }
}
```

Which statement describes the reason why executing this workload multiple times will cause undesired behaviour?

### é¸é …

- **A.** Each execution will create a new job with the name "Ingest new data".
- **B.** The "Ingest new data" job will continuously fail at the beginning of each execution.
- **C.** No notebook exists at the location /Prod/ingest.py.
- **D.** Production Databricks cluster cannot be controlled using the REST API.
- **E.** If the cluster is terminated, no new cluster will be created.

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 6 é¡Œ - Q-029 [L2-Intermediate]

### é¡Œå¹¹

A new data engineer notices that a critical field was omitted from an application that writes its Kafka source to Delta Lake.

This happened even though the critical field was in the Kafka source.

That field was further missing from data written to dependent, long-term storage.

The retention threshold on the Kafka service is seven days. The pipeline has been in production for three months.

Which describes how Delta Lake can help to avoid data loss of this nature in the future?

### é¸é …

- **A.** The Delta Lake transaction log maintains records of each write transaction which includes both the schema and partitioning for all records written to the table.
- **B.** Delta Lake maintains metric tables for all records committed, allowing users to check if schemas are consistent across versions.
- **C.** Delta Lake has special integration with Kafka source that ensure no data loss if fields are missing.
- **D.** Delta Lake automatically archives all previous table records when a write is committed.
- **E.** Time travel allows reviewing a snapshot of a table from any point in the retention period of the table data files.

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 7 é¡Œ - Q-034 [L2-Intermediate]

### é¡Œå¹¹

The data architect has mandated that all tables in the Lakehouse should be configured as external Delta Lake tables.

Which approach will ensure that this requirement is met?

### é¸é …

- **A.** Whenever a database is being created, make sure that the LOCATION keyword is used
- **B.** When configuring an external data warehouse for all table storage, leverage Databricks for all ELT.
- **C.** Whenever a table is being created, make sure that the LOCATION keyword is used.
- **D.** External tables are the default for databases created with the LOCATION keyword
- **E.** Auto Loader and COPY INTO will default to external tables for all data sources.

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 8 é¡Œ - Q-068 [L2-Intermediate]

### é¡Œå¹¹

Assuming that the Databricks CLI has been installed and configured correctly, which Databricks CLI command can be used to upload a custom Python Wheel to object storage mounted with the DBFS for use with a production job?

### é¸é …

- **A.** `configure`
- **B.** `fs`
- **C.** `jobs`
- **D.** `libraries`
- **E.** `workspace`

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 9 é¡Œ - Q-015 [L3-Advanced]

### é¡Œå¹¹

A table in the Lakehouse named customer_churn_params is used in churn prediction by the machine learning team.

The table contains information about customers derived from a number of upstream sources.

Currently, the data engineering team populates this table nightly by overwriting the table with the current valid values derived from upstream data sources.

The churn prediction model used by the ML team is fairly stable in production.

The team is only interested in making predictions on records that have changed in the past 24 hours.

Which approach can the data engineering team use to reduce compute costs while writing to the customer_churn_params table and still allow the team to identify which records have been updated?

### é¸é …

- **A.** Add a field to capture the write_time and use this to identify records that have been updated
- **B.** Only process changed records upstream and use Delta Lake's insert only mode to append these records; add a field to capture the write_timestamp
- **C.** Add a field to capture write_timestamp and use Delta Lake's merge to update changed records; partition the table by write_timestamp
- **D.** Develop a change data capture system to compare values from the previous day and append all changed records
- **E.** Use Delta Lake Change Data Feed to identify records that have been updated in the past 24 hours

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

## ç¬¬ 10 é¡Œ - Q-041 [L1-Basic] â†’ [ä¿®æ­£ç‚º L2]

### é¡Œå¹¹

The DevOps team has configured a production workload as a collection of notebooks scheduled to run daily using the Jobs UI.

A new data engineering hire is onboarding to the team and has requested access to one of these notebooks to review the production logic.

What are the maximum notebook permissions that can be granted to the user without allowing accidental changes to production code or data?

### é¸é …

- **A.** Can Manage
- **B.** Can Run
- **C.** Can Edit
- **D.** Can Read
- **E.** Can View

**ä½ çš„ç­”æ¡ˆï¼š** ________

---

---

## âœ… ç­”æ¡ˆèˆ‡è§£æ

<details>
<summary><strong>é»æ“Šå±•é–‹æŸ¥çœ‹ç­”æ¡ˆ</strong></summary>

### ç¬¬ 1 é¡Œ ç­”æ¡ˆï¼š**D**
**è€ƒé»ï¼š** Streaming / æ ¸å¿ƒæ¦‚å¿µ  
**è§£æï¼š** Structured Streaming å°‡ä¸²æµè³‡æ–™æ¨¡å‹åŒ–ç‚ºã€Œç„¡ç•Œè¡¨æ ¼ã€ï¼ˆunbounded tableï¼‰ï¼Œæ–°è³‡æ–™å°±åƒæ–°çš„è¡Œè¢«è¿½åŠ åˆ°è¡¨æ ¼ä¸­ã€‚é€™æ˜¯ Spark Structured Streaming çš„æ ¸å¿ƒè¨­è¨ˆç†å¿µã€‚

---

### ç¬¬ 2 é¡Œ ç­”æ¡ˆï¼š**C**
**è€ƒé»ï¼š** Jobs / ç¶­é‹ç®¡ç†  
**è§£æï¼š** Databricks çš„ Job Run History ä¿ç•™ **60 å¤©**ï¼ŒæœŸé–“å¯ä»¥åŒ¯å‡º notebook åŸ·è¡Œçµæœç‚º HTMLã€‚é€™æ˜¯è€ƒè©¦ä¸­å¸¸è¦‹çš„è¨˜æ†¶é¡Œã€‚

---

### ç¬¬ 3 é¡Œ ç­”æ¡ˆï¼š**A**
**è€ƒé»ï¼š** Testing / è»Ÿé«”å·¥ç¨‹  
**è§£æï¼š** Integration Testingï¼ˆæ•´åˆæ¸¬è©¦ï¼‰é©—è­‰çš„æ˜¯**å­ç³»çµ±ä¹‹é–“çš„äº’å‹•**ï¼Œè€Œéå–®ä¸€å…ƒä»¶æˆ–å®Œæ•´ä½¿ç”¨æ¡ˆä¾‹ã€‚

---

### ç¬¬ 4 é¡Œ ç­”æ¡ˆï¼š**B**
**è€ƒé»ï¼š** Repos / Git æ•´åˆ  
**è§£æï¼š** ç•¶åˆ†æ”¯ä¸åœ¨ä¸‹æ‹‰é¸å–®ä¸­æ™‚ï¼Œéœ€è¦å…ˆ **pull changes from remote**ï¼Œç„¶å¾Œæ‰èƒ½åˆ‡æ›åˆ°è©²åˆ†æ”¯ã€‚é€™æ˜¯ Git æ“ä½œçš„æ¨™æº–æµç¨‹ã€‚

---

### ç¬¬ 5 é¡Œ ç­”æ¡ˆï¼š**A**
**è€ƒé»ï¼š** Jobs / REST API  
**è§£æï¼š** ä½¿ç”¨ `2.0/jobs/create` æ¯æ¬¡éƒ½æœƒå‰µå»º**æ–°çš„ job**ï¼Œå³ä½¿åç¨±ç›¸åŒã€‚æ­£ç¢ºåšæ³•æ˜¯ä½¿ç”¨ `2.1/jobs/create`ï¼ˆæœ‰ idempotency tokenï¼‰æˆ–å…ˆå»ºç«‹å¾Œä½¿ç”¨ `run-now`ã€‚

---

### ç¬¬ 6 é¡Œ ç­”æ¡ˆï¼š**E**
**è€ƒé»ï¼š** Delta Lake / Time Travel  
**è§£æï¼š** **Time Travel** å…è¨±æŸ¥çœ‹ä¿ç•™æœŸå…§ä»»ä½•æ™‚é–“é»çš„è³‡æ–™å¿«ç…§ï¼Œå¯ä»¥ç”¨ä¾†æ‰¾å›éºå¤±çš„æ¬„ä½è³‡æ–™ï¼ˆå¦‚æœåœ¨ä¿ç•™æœŸå…§ï¼‰ã€‚

---

### ç¬¬ 7 é¡Œ ç­”æ¡ˆï¼š**C**
**è€ƒé»ï¼š** Delta Lake / External Tables  
**è§£æï¼š** è¦å‰µå»º **External Table**ï¼Œå¿…é ˆåœ¨ `CREATE TABLE` æ™‚ä½¿ç”¨ `LOCATION` é—œéµå­—æŒ‡å®šå¤–éƒ¨å­˜å„²ä½ç½®ã€‚é¸é … Aï¼ˆdatabase levelï¼‰ä¸æœƒå½±éŸ¿ table çš„ managed/external å±¬æ€§ã€‚

---

### ç¬¬ 8 é¡Œ ç­”æ¡ˆï¼š**B**
**è€ƒé»ï¼š** Databricks CLI / æª”æ¡ˆæ“ä½œ  
**è§£æï¼š** `databricks fs` å‘½ä»¤ç”¨æ–¼æ“ä½œ DBFS ä¸Šçš„æª”æ¡ˆï¼ŒåŒ…æ‹¬ä¸Šå‚³ Python Wheel å¥—ä»¶ã€‚

---

### ç¬¬ 9 é¡Œ ç­”æ¡ˆï¼š**B**
**è€ƒé»ï¼š** Delta Lake / CDC / æ•ˆèƒ½å„ªåŒ–  
**è§£æï¼š** æœ€ä½³æ–¹æ¡ˆæ˜¯**åªè™•ç†è®Šæ›´è¨˜éŒ„ + insert-only mode + åŠ ä¸Š write_timestamp**ã€‚é€™æ¨£æ—¢ç¯€çœé‹ç®—ï¼ˆä¸è™•ç†æœªè®Šæ›´è³‡æ–™ï¼‰ï¼Œåˆèƒ½è­˜åˆ¥æ–°è¨˜éŒ„ã€‚é¸é … Eï¼ˆChange Data Feedï¼‰æ˜¯è¿½è¹¤è®Šæ›´ï¼Œä½†ç„¡æ³•ã€Œæ¸›å°‘å¯«å…¥æ™‚é‹ç®—ã€ã€‚

---

### ç¬¬ 10 é¡Œ ç­”æ¡ˆï¼š**D** (Can Read)
**è€ƒé»ï¼š** Security / Notebook Permissions  
**è§£æï¼š** **Can Read** å…è¨±æŸ¥çœ‹ç¨‹å¼ç¢¼ä½†ç„¡æ³•åŸ·è¡Œæˆ–ä¿®æ”¹ï¼Œæ˜¯å¯©é–±ç”Ÿç”¢é‚è¼¯çš„æœ€å®‰å…¨æ¬Šé™ã€‚Can Run æœƒå…è¨±åŸ·è¡Œï¼ˆå¯èƒ½å½±éŸ¿è³‡æ–™ï¼‰ï¼ŒCan Edit/Can Manage å‰‡å¯ä¿®æ”¹ç¨‹å¼ç¢¼ã€‚

---

</details>

---

## ğŸ“Š è©•åˆ†æ¨™æº–

| ç­”å°é¡Œæ•¸ | åˆ†æ•¸ | è©•åƒ¹ |
|---------|------|------|
| 9-10 é¡Œ | 90-100 | å„ªç§€ï¼å¯ä»¥æº–å‚™è€ƒè©¦ |
| 7-8 é¡Œ  | 70-89  | è‰¯å¥½ï¼Œéƒ¨åˆ†çŸ¥è­˜éœ€åŠ å¼· |
| 5-6 é¡Œ  | 50-69  | åŠæ ¼é‚Šç·£ï¼Œéœ€é‡å°æ€§è¤‡ç¿’ |
| 0-4 é¡Œ  | 0-49   | éœ€ç³»çµ±æ€§è¤‡ç¿’åŸºç¤çŸ¥è­˜ |

---

## ğŸ¯ ä¸‹ä¸€æ­¥å»ºè­°

1. **çµ±è¨ˆéŒ¯é¡Œä¸»é¡Œ** - è¨˜éŒ„ä½ ç­”éŒ¯çš„é¡Œç›®å±¬æ–¼å“ªäº›è€ƒé»
2. **é‡å°æ€§è¤‡ç¿’** - å‰å¾€ `docs/core-knowledge/` å’Œ `question-bank/by-topic/` è¤‡ç¿’ç›¸é—œä¸»é¡Œ
3. **éŒ¯é¡Œé‡åš** - éš”å¤©å†æ¬¡ä½œç­”éŒ¯é¡Œï¼Œç¢ºä¿ç†è§£
4. **å¢åŠ é›£åº¦** - è‹¥æ­¤ä»½æ¸¬é©—å¾—åˆ† > 80%ï¼Œå¯å˜—è©¦æ›´å¤š L3 é€²éšé¡Œ

---

**ğŸ“Œ æç¤ºï¼š** è‹¥éœ€è¦é‡å°ç‰¹å®šä¸»é¡Œï¼ˆå¦‚ Delta Lakeã€Streamingã€Unity Catalogï¼‰çš„æ¨¡æ“¬è€ƒï¼Œè«‹å‘Šè¨´æˆ‘ï¼
